[![](https://img.shields.io/badge/Awesome_Continual_Learning-yellow)](https://github.com/wutong8023/Awesome_Continual_Learning.git) [![](https://img.shields.io/badge/Awesome_Few_Shot_learning-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning.git) [![](https://img.shields.io/badge/Awesome_Information_Extraction-blue)](https://github.com/wutong8023/Awesome_Information_Extraction.git) [![](https://img.shields.io/badge/Awesome_Ideas-orange)](https://github.com/wutong8023/Awesome_Ideas.git)

# Few-shot Learning Literature 
This repository is maintained by [Tongtong Wu](http://wutong8023.site). Please don't hesitate to send me an email to collaborate or fix some entries (wutong8023 AT gmail.com). 
The automation script of this repo is powered by [Auto-Bibfile](https://github.com/wutong8023/Auto-Bibfile.git).

You can directly use our bibtex.bib in overleaf with this [link](https://www.overleaf.com/read/rgscdxhxbwhp).

This page categorizes the literature by the **Published Venue**.

## Outline 
- [![](https://img.shields.io/badge/Hyperlink-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/venue/README.md#hyperlink)
- [![](https://img.shields.io/badge/ACL-36-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/venue/README.md#acl)
- [![](https://img.shields.io/badge/EMNLP-39-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/venue/README.md#emnlp)
- [![](https://img.shields.io/badge/NAACL-23-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/venue/README.md#naacl)
- [![](https://img.shields.io/badge/COLING-1-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/venue/README.md#coling)
- [![](https://img.shields.io/badge/EACL-1-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/venue/README.md#eacl)
- [![](https://img.shields.io/badge/ICML-36-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/venue/README.md#icml)
- [![](https://img.shields.io/badge/ICLR-55-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/venue/README.md#iclr)
- [![](https://img.shields.io/badge/NeurIPS-57-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/venue/README.md#neurips)
- [![](https://img.shields.io/badge/AAAI-3-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/venue/README.md#aaai)
- [![](https://img.shields.io/badge/IJCAI-6-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/venue/README.md#ijcai)
- [![](https://img.shields.io/badge/SIGIR-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/venue/README.md#sigir)
- [![](https://img.shields.io/badge/KDD-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/venue/README.md#kdd)
- [![](https://img.shields.io/badge/CVPR-19-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/venue/README.md#cvpr)
- [![](https://img.shields.io/badge/ICCV-3-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/venue/README.md#iccv)
- [![](https://img.shields.io/badge/TACL-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/venue/README.md#tacl)
- [![](https://img.shields.io/badge/ACM_Comput._Surv.-1-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/venue/README.md#acm-comput.-surv.)
- [![](https://img.shields.io/badge/arXiv-28-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/venue/README.md#arxiv)
## Hyperlink 
- [[Overview]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/README.md) -- [Homepage](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/README.md)
- [[NLP]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4nlp/./)  [[CV]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4cv/./) -- [Summary](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/./)
- [[NLP]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4nlp/application)  [[CV]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4cv/application) -- [Application](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/application)
- [[NLP]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4nlp/approach)  [[CV]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4cv/approach) -- [Approach](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/approach)
- [[NLP]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4nlp/author)  [[CV]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4cv/author) -- [Author](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author)
- [[NLP]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4nlp/backbone_model)  [[CV]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4cv/backbone_model) -- [Backbone Model](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/backbone_model)
- [[NLP]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4nlp/contribution)  [[CV]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4cv/contribution) -- [Contribution](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/contribution)
- [[NLP]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4nlp/dataset)  [[CV]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4cv/dataset) -- [Dataset](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/dataset)
- [[NLP]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4nlp/metrics)  [[CV]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4cv/metrics) -- [Metrics](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/metrics)
- [[NLP]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4nlp/research_question)  [[CV]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4cv/research_question) -- [Research Questions](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/research_question)
- [[NLP]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4nlp/setting)  [[CV]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4cv/setting) -- [Setting](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/setting)
- [[NLP]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4nlp/supervision)  [[CV]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4cv/supervision) -- [ Learning Paradigm](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/supervision)
- [[NLP]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4nlp/time)  [[CV]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4cv/time) -- [Published Time](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/time)
- [[NLP]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4nlp/venue)  [[CV]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4cv/venue) -- [Published Venue](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/venue)

## ACL

- [![](https://img.shields.io/badge/ACL-2022-green)](https://aclanthology.org/2022.acl-long.254)<a href="https://scholar.google.com.hk/scholar?q=Prompt-free+and+Efficient+Few-shot+Learning+with+Language+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Prompt-free and Efficient Few-shot Learning with Language Models**](https://aclanthology.org/2022.acl-long.254) , <br> by *Karimi Mahabadi, Rabeeh  and
Zettlemoyer, Luke  and
Henderson, James  and
Mathias, Lambert  and
Saeidi, Marzieh  and
Stoyanov, Veselin  and
Yazdani, Majid* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L855-L869) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```karimi-mahabadi-etal-2022-prompt```
- [![](https://img.shields.io/badge/ACL-2022-green)](https://aclanthology.org/2022.acl-long.439)<a href="https://scholar.google.com.hk/scholar?q=CONTaiNER:+Few-Shot+Named+Entity+Recognition+via+Contrastive+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**CONTaiNER: Few-Shot Named Entity Recognition via Contrastive Learning**](https://aclanthology.org/2022.acl-long.439) , <br> by *Das, Sarkar Snigdha Sarathi  and
Katiyar, Arzoo  and
Passonneau, Rebecca  and
Zhang, Rui* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L871-L882) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```das-etal-2022-container```
- [![](https://img.shields.io/badge/ACL-2022-green)](https://aclanthology.org/2022.acl-long.43)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Class-Incremental+Learning+for+Named+Entity+Recognition"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Class-Incremental Learning for Named Entity Recognition**](https://aclanthology.org/2022.acl-long.43) , <br> by *Wang, Rui  and
Yu, Tong  and
Zhao, Handong  and
Kim, Sungchul  and
Mitra, Subrata  and
Zhang, Ruiyi  and
Henao, Ricardo* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L884-L898) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```wang-etal-2022-shot```
- [![](https://img.shields.io/badge/ACL-2022-green)](https://aclanthology.org/2022.acl-long.198)<a href="https://scholar.google.com.hk/scholar?q=Continual+Few-shot+Relation+Learning+via+Embedding+Space+Regularization+and+Data+Augmentation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Continual Few-shot Relation Learning via Embedding Space Regularization and Data Augmentation**](https://aclanthology.org/2022.acl-long.198) , <br> by *Qin, Chengwei  and
Joty, Shafiq* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L900-L909) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```qin-joty-2022-continual```
- [![](https://img.shields.io/badge/ACL-2022-green)](https://aclanthology.org/2022.acl-long.197)<a href="https://scholar.google.com.hk/scholar?q=A+Good+Prompt+Is+Worth+Millions+of+Parameters:+Low-resource+Prompt-based+Learning+for+Vision-Language+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models**](https://aclanthology.org/2022.acl-long.197) , <br> by *Jin, Woojeong  and
Cheng, Yu  and
Shen, Yelong  and
Chen, Weizhu  and
Ren, Xiang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L911-L923) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```jin-etal-2022-good```
- [![](https://img.shields.io/badge/ACL-2022-green)](https://aclanthology.org/2022.acl-long.521)<a href="https://scholar.google.com.hk/scholar?q=Memorisation+versus+Generalisation+in+Pre-trained+Language+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Memorisation versus Generalisation in Pre-trained Language Models**](https://aclanthology.org/2022.acl-long.521) , <br> by *T{\"a}nzer, Michael  and
Ruder, Sebastian  and
Rei, Marek* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L925-L935) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```tanzer-etal-2022-memorisation```
- [![](https://img.shields.io/badge/ACL-2022-green)](https://aclanthology.org/2022.acl-long.592)<a href="https://scholar.google.com.hk/scholar?q=FlipDA:+Effective+and+Robust+Data+Augmentation+for+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**FlipDA: Effective and Robust Data Augmentation for Few-Shot Learning**](https://aclanthology.org/2022.acl-long.592) , <br> by *Zhou, Jing  and
Zheng, Yanan  and
Tang, Jie  and
Jian, Li  and
Yang, Zhilin* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L937-L949) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhou-etal-2022-flipda```
- [![](https://img.shields.io/badge/ACL-2022-green)](https://aclanthology.org/2022.acl-long.483)<a href="https://scholar.google.com.hk/scholar?q=Prototypical+Verbalizer+for+Prompt-based+Few-shot+Tuning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Prototypical Verbalizer for Prompt-based Few-shot Tuning**](https://aclanthology.org/2022.acl-long.483) , <br> by *Cui, Ganqu  and
Hu, Shengding  and
Ding, Ning  and
Huang, Longtao  and
Liu, Zhiyuan* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L951-L963) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```cui-etal-2022-prototypical```
- [![](https://img.shields.io/badge/ACL-2022-green)](https://aclanthology.org/2022.acl-long.481)<a href="https://scholar.google.com.hk/scholar?q=A+Rationale-Centric+Framework+for+Human-in-the-loop+Machine+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**A Rationale-Centric Framework for Human-in-the-loop Machine Learning**](https://aclanthology.org/2022.acl-long.481) , <br> by *Lu, Jinghui  and
Yang, Linyi  and
Namee, Brian  and
Zhang, Yue* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L965-L976) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```lu-etal-2022-rationale```
- [![](https://img.shields.io/badge/ACL-2022-green)](https://aclanthology.org/2022.acl-long.584)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Learning+with+Siamese+Networks+and+Label+Tuning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Learning with Siamese Networks and Label Tuning**](https://aclanthology.org/2022.acl-long.584) , <br> by *M{\"u}ller, Thomas  and
P{\'e}rez-Torr{\'o}, Guillermo  and
Franco-Salvador, Marc* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L978-L988) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```muller-etal-2022-shot```
- [![](https://img.shields.io/badge/ACL-2022-green)](https://aclanthology.org/2022.acl-long.576)<a href="https://scholar.google.com.hk/scholar?q=PPT:+Pre-trained+Prompt+Tuning+for+Few-shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**PPT: Pre-trained Prompt Tuning for Few-shot Learning**](https://aclanthology.org/2022.acl-long.576) , <br> by *Gu, Yuxian  and
Han, Xu  and
Liu, Zhiyuan  and
Huang, Minlie* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L990-L1001) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```gu-etal-2022-ppt```
- [![](https://img.shields.io/badge/ACL-2022-green)](https://aclanthology.org/2022.acl-short.36)<a href="https://scholar.google.com.hk/scholar?q=Exploiting+Language+Model+Prompts+Using+Similarity+Measures:+A+Case+Study+on+the+Word-in-Context+Task"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Exploiting Language Model Prompts Using Similarity Measures: A Case Study on the Word-in-Context Task**](https://aclanthology.org/2022.acl-short.36) , <br> by *Tabasi, Mohsen  and
Rezaee, Kiamehr  and
Pilehvar, Mohammad Taher* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1003-L1012) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```tabasi-etal-2022-exploiting```
- [![](https://img.shields.io/badge/ACL_Findings-2021-green)](https://doi.org/10.18653/v1/2021.findings-acl.214)<a href="https://scholar.google.com.hk/scholar?q=Adaptive+Knowledge-Enhanced+Bayesian+Meta-Learning+for+Few-shot+Event+Detection"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Adaptive Knowledge-Enhanced Bayesian Meta-Learning for Few-shot Event
Detection**](https://doi.org/10.18653/v1/2021.findings-acl.214) , <br> by *Shirong Shen and
Tongtong Wu and
Guilin Qi and
Yuan{-}Fang Li and
Gholamreza Haffari and
Sheng Bi* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1098-L1111) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ShenWQLHB21```
- [![](https://img.shields.io/badge/ACL-2021-green)](https://aclanthology.org/2021.acl-long.447)<a href="https://scholar.google.com.hk/scholar?q=A+Closer+Look+at+Few-Shot+Crosslingual+Transfer:+The+Choice+of+Shots+Matters"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**A Closer Look at Few-Shot Crosslingual Transfer: The Choice of Shots Matters**](https://aclanthology.org/2021.acl-long.447) , <br> by *Zhao, Mengjie  and
Zhu, Yi  and
Shareghi, Ehsan  and
Vuli{\'c}, Ivan  and
Reichart, Roi  and
Korhonen, Anna  and
Sch{\"u}tze, Hinrich* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1377-L1390) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhao-etal-2021-closer```
- [![](https://img.shields.io/badge/ACL-2021-green)](https://aclanthology.org/2021.acl-long.239)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Question+Answering+by+Pretraining+Span+Selection"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Question Answering by Pretraining Span Selection**](https://aclanthology.org/2021.acl-long.239) , <br> by *Ram, Ori  and
Kirstain, Yuval  and
Berant, Jonathan  and
Globerson, Amir  and
Levy, Omer* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1392-L1403) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ram-etal-2021-shot```
- [![](https://img.shields.io/badge/ACL-2021-green)](https://aclanthology.org/2021.acl-long.248)<a href="https://scholar.google.com.hk/scholar?q=Few-NERD:+A+Few-shot+Named+Entity+Recognition+Dataset"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-NERD: A Few-shot Named Entity Recognition Dataset**](https://aclanthology.org/2021.acl-long.248) , <br> by *Ding, Ning  and
Xu, Guangwei  and
Chen, Yulin  and
Wang, Xiaobin  and
Han, Xu  and
Xie, Pengjun  and
Zheng, Haitao  and
Liu, Zhiyuan* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1405-L1419) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ding-etal-2021-nerd```
- [![](https://img.shields.io/badge/ACL-2021-green)](https://aclanthology.org/2021.acl-long.295)<a href="https://scholar.google.com.hk/scholar?q=Making+Pre-trained+Language+Models+Better+Few-shot+Learners"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Making Pre-trained Language Models Better Few-shot Learners**](https://aclanthology.org/2021.acl-long.295) , <br> by *Gao, Tianyu  and
Fisch, Adam  and
Chen, Danqi* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1421-L1430) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```gao-etal-2021-making```
- [![](https://img.shields.io/badge/ACL-2021-green)](https://aclanthology.org/2021.acl-short.105)<a href="https://scholar.google.com.hk/scholar?q=Distinct+Label+Representations+for+Few-Shot+Text+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Distinct Label Representations for Few-Shot Text Classification**](https://aclanthology.org/2021.acl-short.105) , <br> by *Ohashi, Sora  and
Takayama, Junya  and
Kajiwara, Tomoyuki  and
Arase, Yuki* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1432-L1442) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ohashi-etal-2021-distinct```
- [![](https://img.shields.io/badge/ACL-2021-green)](https://aclanthology.org/2021.acl-long.95)<a href="https://scholar.google.com.hk/scholar?q=AugNLG:+Few-shot+Natural+Language+Generation+using+Self-trained+Data+Augmentation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**AugNLG: Few-shot Natural Language Generation using Self-trained Data Augmentation**](https://aclanthology.org/2021.acl-long.95) , <br> by *Xu, Xinnuo  and
Wang, Guoyin  and
Kim, Young-Bum  and
Lee, Sungjin* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1444-L1454) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```xu-etal-2021-augnlg```
- [![](https://img.shields.io/badge/ACL-2021-green)](https://aclanthology.org/2021.acl-long.495)<a href="https://scholar.google.com.hk/scholar?q=Multi-Label+Few-Shot+Learning+for+Aspect+Category+Detection"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Multi-Label Few-Shot Learning for Aspect Category Detection**](https://aclanthology.org/2021.acl-long.495) , <br> by *Hu, Mengting  and
Zhao, Shiwan  and
Guo, Honglei  and
Xue, Chao  and
Gao, Hang  and
Gao, Tiegang  and
Cheng, Renhong  and
Su, Zhong* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1456-L1470) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```hu-etal-2021-multi-label```
- [![](https://img.shields.io/badge/ACL-2021-green)](https://aclanthology.org/2021.acl-long.382)<a href="https://scholar.google.com.hk/scholar?q=Lexicon+Learning+for+Few+Shot+Sequence+Modeling"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Lexicon Learning for Few Shot Sequence Modeling**](https://aclanthology.org/2021.acl-long.382) , <br> by *Akyurek, Ekin  and
Andreas, Jacob* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1472-L1480) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```akyurek-andreas-2021-lexicon```
- [![](https://img.shields.io/badge/ACL-2021-green)](https://aclanthology.org/2021.acl-short.124)<a href="https://scholar.google.com.hk/scholar?q=Entity+Concept-enhanced+Few-shot+Relation+Extraction"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Entity Concept-enhanced Few-shot Relation Extraction**](https://aclanthology.org/2021.acl-short.124) , <br> by *Yang, Shan  and
Zhang, Yongfei  and
Niu, Guanglin  and
Zhao, Qinghua  and
Pu, Shiliang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1482-L1493) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```yang-etal-2021-entity```
- [![](https://img.shields.io/badge/ACL-2021-green)](https://aclanthology.org/2021.acl-long.487)<a href="https://scholar.google.com.hk/scholar?q=Learning+from+Miscellaneous+Other-Class+Words+for+Few-shot+Named+Entity+Recognition"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning from Miscellaneous Other-Class Words for Few-shot Named Entity Recognition**](https://aclanthology.org/2021.acl-long.487) , <br> by *Tong, Meihan  and
Wang, Shuai  and
Xu, Bin  and
Cao, Yixin  and
Liu, Minghui  and
Hou, Lei  and
Li, Juanzi* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1495-L1508) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```tong-etal-2021-learning```
- [![](https://img.shields.io/badge/ACL-2021-green)](https://aclanthology.org/2021.acl-short.2)<a href="https://scholar.google.com.hk/scholar?q=On+Training+Instance+Selection+for+Few-Shot+Neural+Text+Generation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**On Training Instance Selection for Few-Shot Neural Text Generation**](https://aclanthology.org/2021.acl-short.2) , <br> by *Chang, Ernie  and
Shen, Xiaoyu  and
Yeh, Hui-Syuan  and
Demberg, Vera* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1510-L1520) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```chang-etal-2021-training```
- [![](https://img.shields.io/badge/ACL-2020-green)](https://www.aclweb.org/anthology/2020.acl-main.11)<a href="https://scholar.google.com.hk/scholar?q=Span-ConveRT:+Few-shot+Span+Extraction+for+Dialog+with+Pretrained+Conversational+Representations"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Span-ConveRT: Few-shot Span Extraction for Dialog with Pretrained Conversational Representations**](https://www.aclweb.org/anthology/2020.acl-main.11) , <br> by *Coope, Samuel  and
Farghly, Tyler  and
Gerz, Daniela  and
Vuli{\'c}, Ivan  and
Henderson, Matthew* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3036-L3047) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```coope-etal-2020-span```
- [![](https://img.shields.io/badge/ACL-2020-green)](https://www.aclweb.org/anthology/2020.acl-main.18)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+NLG+with+Pre-Trained+Language+Model"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot NLG with Pre-Trained Language Model**](https://www.aclweb.org/anthology/2020.acl-main.18) , <br> by *Chen, Zhiyu  and
Eavani, Harini  and
Chen, Wenhu  and
Liu, Yinyin  and
Wang, William Yang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3049-L3060) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```chen-etal-2020-shot```
- [![](https://img.shields.io/badge/ACL-2020-green)](https://www.aclweb.org/anthology/2020.acl-main.102)<a href="https://scholar.google.com.hk/scholar?q=Dynamic+Memory+Induction+Networks+for+Few-Shot+Text+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Dynamic Memory Induction Networks for Few-Shot Text Classification**](https://www.aclweb.org/anthology/2020.acl-main.102) , <br> by *Geng, Ruiying  and
Li, Binhua  and
Li, Yongbin  and
Sun, Jian  and
Zhu, Xiaodan* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3062-L3073) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```geng-etal-2020-dynamic```
- [![](https://img.shields.io/badge/ACL-2020-green)](https://www.aclweb.org/anthology/2020.acl-main.128)<a href="https://scholar.google.com.hk/scholar?q=Few-shot+Slot+Tagging+with+Collapsed+Dependency+Transfer+and+Label-enhanced+Task-adaptive+Projection+Network"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-shot Slot Tagging with Collapsed Dependency Transfer and Label-enhanced Task-adaptive Projection Network**](https://www.aclweb.org/anthology/2020.acl-main.128) , <br> by *Hou, Yutai  and
Che, Wanxiang  and
Lai, Yongkui  and
Zhou, Zhihan  and
Liu, Yijia  and
Liu, Han  and
Liu, Ting* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3075-L3088) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```hou-etal-2020-shot```
- [![](https://img.shields.io/badge/ACL-2020-green)](https://www.aclweb.org/anthology/2020.acl-main.436)<a href="https://scholar.google.com.hk/scholar?q=Shaping+Visual+Representations+with+Language+for+Few-Shot+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Shaping Visual Representations with Language for Few-Shot Classification**](https://www.aclweb.org/anthology/2020.acl-main.436) , <br> by *Mu, Jesse  and
Liang, Percy  and
Goodman, Noah* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3090-L3099) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```mu-etal-2020-shaping```
- [![](https://img.shields.io/badge/ACL-2020-green)](https://www.aclweb.org/anthology/2020.acl-main.517)<a href="https://scholar.google.com.hk/scholar?q=Learning+to+Customize+Model+Structures+for+Few-shot+Dialogue+Generation+Tasks"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning to Customize Model Structures for Few-shot Dialogue Generation Tasks**](https://www.aclweb.org/anthology/2020.acl-main.517) , <br> by *Song, Yiping  and
Liu, Zequn  and
Bi, Wei  and
Yan, Rui  and
Zhang, Ming* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3101-L3112) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```song-etal-2020-learning```
- [![](https://img.shields.io/badge/ACL-2020-green)](https://www.aclweb.org/anthology/2020.acl-main.654)<a href="https://scholar.google.com.hk/scholar?q=Multi-source+Meta+Transfer+for+Low+Resource+Multiple-Choice+Question+Answering"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Multi-source Meta Transfer for Low Resource Multiple-Choice Question Answering**](https://www.aclweb.org/anthology/2020.acl-main.654) , <br> by *Yan, Ming  and
Zhang, Hao  and
Jin, Di  and
Zhou, Joey Tianyi* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3114-L3124) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```yan-etal-2020-multi-source```
- [![](https://img.shields.io/badge/ACL-2020-green)](https://www.aclweb.org/anthology/2020.acl-main.437)<a href="https://scholar.google.com.hk/scholar?q=Discrete+Latent+Variable+Representations+for+Low-Resource+Text+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Discrete Latent Variable Representations for Low-Resource Text Classification**](https://www.aclweb.org/anthology/2020.acl-main.437) , <br> by *Jin, Shuning  and
Wiseman, Sam  and
Stratos, Karl  and
Livescu, Karen* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3126-L3136) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```jin-etal-2020-discrete```
- [![](https://img.shields.io/badge/ACL-2020-green)](https://www.aclweb.org/anthology/2020.acl-main.523)<a href="https://scholar.google.com.hk/scholar?q=Improving+Low-Resource+Named+Entity+Recognition+using+Joint+Sentence+and+Token+Labeling"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Improving Low-Resource Named Entity Recognition using Joint Sentence and Token Labeling**](https://www.aclweb.org/anthology/2020.acl-main.523) , <br> by *Kruengkrai, Canasai  and
Nguyen, Thien Hai  and
Aljunied, Sharifah Mahani  and
Bing, Lidong* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3138-L3148) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```kruengkrai-etal-2020-improving```
- [![](https://img.shields.io/badge/ACL-2020-green)](https://www.aclweb.org/anthology/2020.acl-main.722)<a href="https://scholar.google.com.hk/scholar?q=Soft+Gazetteers+for+Low-Resource+Named+Entity+Recognition"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Soft Gazetteers for Low-Resource Named Entity Recognition**](https://www.aclweb.org/anthology/2020.acl-main.722) , <br> by *Rijhwani, Shruti  and
Zhou, Shuyan  and
Neubig, Graham  and
Carbonell, Jaime* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3150-L3160) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```rijhwani-etal-2020-soft```
- [![](https://img.shields.io/badge/ACL-2019-green)](https://doi.org/10.18653/v1/p19-1279)<a href="https://scholar.google.com.hk/scholar?q=Matching+the+Blanks:+Distributional+Similarity+for+Relation+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Matching the Blanks: Distributional Similarity for Relation Learning**](https://doi.org/10.18653/v1/p19-1279) , <br> by *Livio Baldini Soares and
Nicholas FitzGerald and
Jeffrey Ling and
Tom Kwiatkowski* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1227-L1237) <br>```MTB
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```SoaresFLK19```
- [![](https://img.shields.io/badge/ACL-2019-green)](https://doi.org/10.18653/v1/p19-1277)<a href="https://scholar.google.com.hk/scholar?q=Multi-Level+Matching+and+Aggregation+Network+for+Few-Shot+Relation+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Multi-Level Matching and Aggregation Network for Few-Shot Relation
Classification**](https://doi.org/10.18653/v1/p19-1277) , <br> by *Zhi{-}Xiu Ye and
Zhen{-}Hua Ling* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1288-L1297) <br>```MLMAN
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```YeL19```
## EMNLP

- [![](https://img.shields.io/badge/EMNLP-2021-green)](https://aclanthology.org/2021.emnlp-main.212)<a href="https://scholar.google.com.hk/scholar?q=MapRE:+An+Effective+Semantic+Mapping+Approach+for+Low-resource+Relation+Extraction"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**MapRE: An Effective Semantic Mapping Approach for Low-resource Relation Extraction**](https://aclanthology.org/2021.emnlp-main.212) , <br> by *Dong, Manqing  and
Pan, Chunguang  and
Luo, Zhipeng* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L166-L176) <br>```proposing a label-aware method for low-resource relation extraction
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```dong-etal-2021-mapre```
- [![](https://img.shields.io/badge/EMNLP-2021-green)](https://aclanthology.org/2021.emnlp-main.144)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Intent+Detection+via+Contrastive+Pre-Training+and+Fine-Tuning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Intent Detection via Contrastive Pre-Training and Fine-Tuning**](https://aclanthology.org/2021.emnlp-main.144) , <br> by *Zhang, Jianguo  and
Bui, Trung  and
Yoon, Seunghyun  and
Chen, Xiang  and
Liu, Zhiwei  and
Xia, Congying  and
Tran, Quan Hung  and
Chang, Walter  and
Yu, Philip* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L181-L198) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhang-etal-2021-shot```
- [![](https://img.shields.io/badge/EMNLP-2021-green)](https://aclanthology.org/2021.emnlp-main.142)<a href="https://scholar.google.com.hk/scholar?q=Self-training+Improves+Pre-training+for+Few-shot+Learning+in+Task-oriented+Dialog+Systems"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Self-training Improves Pre-training for Few-shot Learning in Task-oriented Dialog Systems**](https://aclanthology.org/2021.emnlp-main.142) , <br> by *Mi, Fei  and
Zhou, Wanhao  and
Kong, Lingjing  and
Cai, Fengyu  and
Huang, Minlie  and
Faltings, Boi* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L200-L214) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```mi-etal-2021-self```
- [![](https://img.shields.io/badge/EMNLP-2021-green)](https://aclanthology.org/2021.emnlp-main.131)<a href="https://scholar.google.com.hk/scholar?q=Nearest+Neighbour+Few-Shot+Learning+for+Cross-lingual+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Nearest Neighbour Few-Shot Learning for Cross-lingual Classification**](https://aclanthology.org/2021.emnlp-main.131) , <br> by *Bari, M Saiful  and
Haider, Batool  and
Mansour, Saab* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L216-L227) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```bari-etal-2021-nearest```
- [![](https://img.shields.io/badge/EMNLP-2021-green)](https://aclanthology.org/2021.emnlp-main.221)<a href="https://scholar.google.com.hk/scholar?q=TransPrompt:+Towards+an+Automatic+Transferable+Prompting+Framework+for+Few-shot+Text+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**TransPrompt: Towards an Automatic Transferable Prompting Framework for Few-shot Text Classification**](https://aclanthology.org/2021.emnlp-main.221) , <br> by *Wang, Chengyu  and
Wang, Jianing  and
Qiu, Minghui  and
Huang, Jun  and
Gao, Ming* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L229-L242) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```wang-etal-2021-transprompt```
- [![](https://img.shields.io/badge/EMNLP-2021-green)](https://aclanthology.org/2021.emnlp-main.433)<a href="https://scholar.google.com.hk/scholar?q=Towards+Realistic+Few-Shot+Relation+Extraction"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Towards Realistic Few-Shot Relation Extraction**](https://aclanthology.org/2021.emnlp-main.433) , <br> by *Brody, Sam  and
Wu, Sichao  and
Benton, Adrian* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L244-L255) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```brody-etal-2021-towards```
- [![](https://img.shields.io/badge/EMNLP-2021-green)](https://aclanthology.org/2021.emnlp-main.204)<a href="https://scholar.google.com.hk/scholar?q=Exploring+Task+Difficulty+for+Few-Shot+Relation+Extraction"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Exploring Task Difficulty for Few-Shot Relation Extraction**](https://aclanthology.org/2021.emnlp-main.204) , <br> by *Han, Jiale  and
Cheng, Bo  and
Lu, Wei* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L257-L268) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```han-etal-2021-exploring```
- [![](https://img.shields.io/badge/EMNLP-2021-green)](https://aclanthology.org/2021.emnlp-main.427)<a href="https://scholar.google.com.hk/scholar?q=Learning+Prototype+Representations+Across+Few-Shot+Tasks+for+Event+Detection"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning Prototype Representations Across Few-Shot Tasks for Event Detection**](https://aclanthology.org/2021.emnlp-main.427) , <br> by *Lai, Viet Dac  and
Dernoncourt, Franck  and
Nguyen, Thien Huu* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L270-L281) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```lai-etal-2021-learning```
- [![](https://img.shields.io/badge/EMNLP-2021-green)](https://aclanthology.org/2021.emnlp-main.734)<a href="https://scholar.google.com.hk/scholar?q=Language+Models+are+Few-Shot+Butlers"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Language Models are Few-Shot Butlers**](https://aclanthology.org/2021.emnlp-main.734) , <br> by *Micheli, Vincent  and
Fleuret, Francois* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L283-L293) <br>```proposing to use RL and few-shot supervised learning for text generation.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```micheli-fleuret-2021-language```
- [![](https://img.shields.io/badge/EMNLP-2021-green)](https://aclanthology.org/2021.emnlp-main.637)<a href="https://scholar.google.com.hk/scholar?q=Honey+or+Poison?+Solving+the+Trigger+Curse+in+Few-shot+Event+Detection+via+Causal+Intervention"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Honey or Poison? Solving the Trigger Curse in Few-shot Event Detection via Causal Intervention**](https://aclanthology.org/2021.emnlp-main.637) , <br> by *Chen, Jiawei  and
Lin, Hongyu  and
Han, Xianpei  and
Sun, Le* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L297-L309) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```chen-etal-2021-honey```
- [![](https://img.shields.io/badge/EMNLP-2021-green)](https://aclanthology.org/2021.emnlp-main.572)<a href="https://scholar.google.com.hk/scholar?q=CrossFit:+A+Few-shot+Learning+Challenge+for+Cross-task+Generalization+in+NLP"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP**](https://aclanthology.org/2021.emnlp-main.572) , <br> by *Ye, Qinyuan  and
Lin, Bill Yuchen  and
Ren, Xiang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L311-L322) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ye-etal-2021-crossfit```
- [![](https://img.shields.io/badge/EMNLP-2021-green)](https://aclanthology.org/2021.emnlp-main.608)<a href="https://scholar.google.com.hk/scholar?q=Constrained+Language+Models+Yield+Few-Shot+Semantic+Parsers"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Constrained Language Models Yield Few-Shot Semantic Parsers**](https://aclanthology.org/2021.emnlp-main.608) , <br> by *Shin, Richard  and
Lin, Christopher  and
Thomson, Sam  and
Chen, Charles  and
Roy, Subhro  and
Platanios, Emmanouil Antonios  and
Pauls, Adam  and
Klein, Dan  and
Eisner, Jason  and
Van Durme, Benjamin* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L324-L342) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```shin-etal-2021-constrained```
- [![](https://img.shields.io/badge/EMNLP-2021-green)](https://aclanthology.org/2021.emnlp-main.407)<a href="https://scholar.google.com.hk/scholar?q=Improving+and+Simplifying+Pattern+Exploiting+Training"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Improving and Simplifying Pattern Exploiting Training**](https://aclanthology.org/2021.emnlp-main.407) , <br> by *Tam, Derek  and
R. Menon, Rakesh  and
Bansal, Mohit  and
Srivastava, Shashank  and
Raffel, Colin* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L344-L357) <br>```proposing ADAPET which promisingly improves the data efficiency of PET. ADAPET does not leverage unlabelled data for training, and introduces label-conditioned loss for the denser supervision.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```tam-etal-2021-improving```
- [![](https://img.shields.io/badge/EMNLP-2021-green)](https://aclanthology.org/2021.emnlp-main.836)<a href="https://scholar.google.com.hk/scholar?q=Self-training+with+Few-shot+Rationalization"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Self-training with Few-shot Rationalization**](https://aclanthology.org/2021.emnlp-main.836) , <br> by *Bhat, Meghana Moorthy  and
Sordoni, Alessandro  and
Mukherjee, Subhabrata* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L360-L371) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```bhat-etal-2021-self```
- [![](https://img.shields.io/badge/EMNLP-2021-green)](https://aclanthology.org/2021.emnlp-main.92)<a href="https://scholar.google.com.hk/scholar?q=Label+Verbalization+and+Entailment+for+Effective+Zero+and+Few-Shot+Relation+Extraction"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Label Verbalization and Entailment for Effective Zero and Few-Shot Relation Extraction**](https://aclanthology.org/2021.emnlp-main.92) , <br> by *Sainz, Oscar  and
Lopez de Lacalle, Oier  and
Labaka, Gorka  and
Barrena, Ander  and
Agirre, Eneko* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L373-L386) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```sainz-etal-2021-label```
- [![](https://img.shields.io/badge/EMNLP-2021-green)](https://aclanthology.org/2021.emnlp-main.460)<a href="https://scholar.google.com.hk/scholar?q=Continual+Few-Shot+Learning+for+Text+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Continual Few-Shot Learning for Text Classification**](https://aclanthology.org/2021.emnlp-main.460) , <br> by *Pasunuru, Ramakanth  and
Stoyanov, Veselin  and
Bansal, Mohit* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L388-L399) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pasunuru-etal-2021-continual```
- [![](https://img.shields.io/badge/EMNLP-2021-green)](https://aclanthology.org/2021.emnlp-main.813)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Named+Entity+Recognition:+An+Empirical+Baseline+Study"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Named Entity Recognition: An Empirical Baseline Study**](https://aclanthology.org/2021.emnlp-main.813) , <br> by *Huang, Jiaxin  and
Li, Chunyuan  and
Subudhi, Krishan  and
Jose, Damien  and
Balakrishnan, Shobana  and
Chen, Weizhu  and
Peng, Baolin  and
Gao, Jianfeng  and
Han, Jiawei* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L401-L418) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```huang-etal-2021-shot```
- [![](https://img.shields.io/badge/EMNLP-2021-green)](https://aclanthology.org/2021.emnlp-main.462)<a href="https://scholar.google.com.hk/scholar?q=STraTA:+Self-Training+with+Task+Augmentation+for+Better+Few-shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**STraTA: Self-Training with Task Augmentation for Better Few-shot Learning**](https://aclanthology.org/2021.emnlp-main.462) , <br> by *Vu, Tu  and
Luong, Minh-Thang  and
Le, Quoc  and
Simon, Grady  and
Iyyer, Mohit* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L420-L433) <br>```pretrained language model-based self-training and data agumentation for few-shot learning.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```vu-etal-2021-strata```
- [![](https://img.shields.io/badge/EMNLP-2021-green)](https://aclanthology.org/2021.emnlp-main.491)<a href="https://scholar.google.com.hk/scholar?q=FewshotQA:+A+simple+framework+for+few-shot+learning+of+question+answering+tasks+using+pre-trained+text-to-text+models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**FewshotQA: A simple framework for few-shot learning of question answering tasks using pre-trained text-to-text models**](https://aclanthology.org/2021.emnlp-main.491) , <br> by *Chada, Rakesh  and
Natarajan, Pradeep* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L436-L446) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```chada-natarajan-2021-fewshotqa```
- [![](https://img.shields.io/badge/EMNLP-2021-green)](https://aclanthology.org/2021.emnlp-main.713)<a href="https://scholar.google.com.hk/scholar?q=Avoiding+Inference+Heuristics+in+Few-shot+Prompt-based+Finetuning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Avoiding Inference Heuristics in Few-shot Prompt-based Finetuning**](https://aclanthology.org/2021.emnlp-main.713) , <br> by *Utama, Prasetya  and
Moosavi, Nafise Sadat  and
Sanh, Victor  and
Gurevych, Iryna* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L448-L460) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```utama-etal-2021-avoiding```
- [![](https://img.shields.io/badge/EMNLP-2021-green)](https://aclanthology.org/2021.emnlp-main.718)<a href="https://scholar.google.com.hk/scholar?q=Revisiting+Self-training+for+Few-shot+Learning+of+Language+Model"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Revisiting Self-training for Few-shot Learning of Language Model**](https://aclanthology.org/2021.emnlp-main.718) , <br> by *Chen, Yiming  and
Zhang, Yan  and
Zhang, Chen  and
Lee, Grandee  and
Cheng, Ran  and
Li, Haizhou* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L462-L476) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```chen-etal-2021-revisiting```
- [![](https://img.shields.io/badge/EMNLP-2021-green)](https://aclanthology.org/2021.emnlp-main.509)<a href="https://scholar.google.com.hk/scholar?q=Open+Aspect+Target+Sentiment+Classification+with+Natural+Language+Prompts"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Open Aspect Target Sentiment Classification with Natural Language Prompts**](https://aclanthology.org/2021.emnlp-main.509) , <br> by *Seoh, Ronald  and
Birle, Ian  and
Tak, Mrinal  and
Chang, Haw-Shiuan  and
Pinette, Brian  and
Hough, Alfred* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L478-L492) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```seoh-etal-2021-open```
- [![](https://img.shields.io/badge/EMNLP-2021-green)](https://aclanthology.org/2021.emnlp-main.301)<a href="https://scholar.google.com.hk/scholar?q=FiD-Ex:+Improving+Sequence-to-Sequence+Models+for+Extractive+Rationale+Generation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**FiD-Ex: Improving Sequence-to-Sequence Models for Extractive Rationale Generation**](https://aclanthology.org/2021.emnlp-main.301) , <br> by *Lakhotia, Kushal  and
Paranjape, Bhargavi  and
Ghoshal, Asish  and
Yih, Scott  and
Mehdad, Yashar  and
Iyer, Srini* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L494-L508) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```lakhotia-etal-2021-fid```
- [![](https://img.shields.io/badge/EMNLP-2021-green)](https://aclanthology.org/2021.emnlp-main.549)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Emotion+Recognition+in+Conversation+with+Sequential+Prototypical+Networks"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Emotion Recognition in Conversation with Sequential Prototypical Networks**](https://aclanthology.org/2021.emnlp-main.549) , <br> by *Guibon, Ga{\"e}l  and
Labeau, Matthieu  and
Flamein, H{\'e}l{\`e}ne  and
Lefeuvre, Luce  and
Clavel, Chlo{\'e}* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L511-L524) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```guibon-etal-2021-shot```
- [![](https://img.shields.io/badge/EMNLP-2020-green)](https://www.aclweb.org/anthology/2020.emnlp-main.346)<a href="https://scholar.google.com.hk/scholar?q=AutoPrompt:+Eliciting+Knowledge+from+Language+Models+with+Automatically+Generated+Prompts"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts**](https://www.aclweb.org/anthology/2020.emnlp-main.346) , <br> by *Shin, Taylor  and
Razeghi, Yasaman  and
Logan IV, Robert L.  and
Wallace, Eric  and
Singh, Sameer* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3023-L3034) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```shin-etal-2020-autoprompt```
- [![](https://img.shields.io/badge/EMNLP-2020-green)](https://www.aclweb.org/anthology/2020.emnlp-main.38)<a href="https://scholar.google.com.hk/scholar?q=Self-Supervised+Meta-Learning+for+Few-Shot+Natural+Language+Classification+Tasks"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Self-Supervised Meta-Learning for Few-Shot Natural Language Classification Tasks**](https://www.aclweb.org/anthology/2020.emnlp-main.38) , <br> by *Bansal, Trapit  and
Jha, Rishikesh  and
Munkhdalai, Tsendsuren  and
McCallum, Andrew* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3162-L3172) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```bansal-etal-2020-self```
- [![](https://img.shields.io/badge/EMNLP-2020-green)](https://www.aclweb.org/anthology/2020.emnlp-main.131)<a href="https://scholar.google.com.hk/scholar?q=Adaptive+Attentional+Network+for+Few-Shot+Knowledge+Graph+Completion"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Adaptive Attentional Network for Few-Shot Knowledge Graph Completion**](https://www.aclweb.org/anthology/2020.emnlp-main.131) , <br> by *Sheng, Jiawei  and
Guo, Shu  and
Chen, Zhenyu  and
Yue, Juwei  and
Wang, Lihong  and
Liu, Tingwen  and
Xu, Hongbo* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3174-L3187) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```sheng-etal-2020-adaptive```
- [![](https://img.shields.io/badge/EMNLP-2020-green)](https://www.aclweb.org/anthology/2020.emnlp-main.235)<a href="https://scholar.google.com.hk/scholar?q=Multi-label+Few/Zero-shot+Learning+with+Knowledge+Aggregated+from+Multiple+Label+Graphs"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Multi-label Few/Zero-shot Learning with Knowledge Aggregated from Multiple Label Graphs**](https://www.aclweb.org/anthology/2020.emnlp-main.235) , <br> by *Lu, Jueqing  and
Du, Lan  and
Liu, Ming  and
Dipnall, Joanna* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3189-L3199) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```lu-etal-2020-multi```
- [![](https://img.shields.io/badge/EMNLP-2020-green)](https://www.aclweb.org/anthology/2020.emnlp-main.375)<a href="https://scholar.google.com.hk/scholar?q=Structural+Supervision+Improves+Few-Shot+Learning+and+Syntactic+Generalization+in+Neural+Language+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Structural Supervision Improves Few-Shot Learning and Syntactic Generalization in Neural Language Models**](https://www.aclweb.org/anthology/2020.emnlp-main.375) , <br> by *Wilcox, Ethan  and
Qian, Peng  and
Futrell, Richard  and
Kohita, Ryosuke  and
Levy, Roger  and
Ballesteros, Miguel* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3201-L3213) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```wilcox-etal-2020-structural```
- [![](https://img.shields.io/badge/EMNLP-2020-green)](https://www.aclweb.org/anthology/2020.emnlp-main.411)<a href="https://scholar.google.com.hk/scholar?q=Discriminative+Nearest+Neighbor+Few-Shot+Intent+Detection+by+Transferring+Natural+Language+Inference"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Discriminative Nearest Neighbor Few-Shot Intent Detection by Transferring Natural Language Inference**](https://www.aclweb.org/anthology/2020.emnlp-main.411) , <br> by *Zhang, Jianguo  and
Hashimoto, Kazuma  and
Liu, Wenhao  and
Wu, Chien-Sheng  and
Wan, Yao  and
Yu, Philip  and
Socher, Richard  and
Xiong, Caiming* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3215-L3229) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhang-etal-2020-discriminative```
- [![](https://img.shields.io/badge/EMNLP-2020-green)](https://www.aclweb.org/anthology/2020.emnlp-main.469)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Complex+Knowledge+Base+Question+Answering+via+Meta+Reinforcement+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Complex Knowledge Base Question Answering via Meta Reinforcement Learning**](https://www.aclweb.org/anthology/2020.emnlp-main.469) , <br> by *Hua, Yuncheng  and
Li, Yuan-Fang  and
Haffari, Gholamreza  and
Qi, Guilin  and
Wu, Tongtong* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3231-L3242) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```hua-etal-2020-shot```
- [![](https://img.shields.io/badge/EMNLP-2020-green)](https://www.aclweb.org/anthology/2020.emnlp-main.516)<a href="https://scholar.google.com.hk/scholar?q=Simple+and+Effective+Few-Shot+Named+Entity+Recognition+with+Structured+Nearest+Neighbor+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Simple and Effective Few-Shot Named Entity Recognition with Structured Nearest Neighbor Learning**](https://www.aclweb.org/anthology/2020.emnlp-main.516) , <br> by *Yang, Yi  and
Katiyar, Arzoo* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3244-L3252) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```yang-katiyar-2020-simple```
- [![](https://img.shields.io/badge/EMNLP-2020-green)](https://www.aclweb.org/anthology/2020.emnlp-main.607)<a href="https://scholar.google.com.hk/scholar?q=An+Empirical+Study+on+Large-Scale+Multi-Label+Text+Classification+Including+Few+and+Zero-Shot+Labels"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**An Empirical Study on Large-Scale Multi-Label Text Classification Including Few and Zero-Shot Labels**](https://www.aclweb.org/anthology/2020.emnlp-main.607) , <br> by *Chalkidis, Ilias  and
Fergadiotis, Manos  and
Kotitsas, Sotiris  and
Malakasiotis, Prodromos  and
Aletras, Nikolaos  and
Androutsopoulos, Ion* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3254-L3266) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```chalkidis-etal-2020-empirical```
- [![](https://img.shields.io/badge/EMNLP-2020-green)](https://www.aclweb.org/anthology/2020.emnlp-main.660)<a href="https://scholar.google.com.hk/scholar?q=Universal+Natural+Language+Processing+with+Limited+Annotations:+Try+Few-shot+Textual+Entailment+as+a+Start"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Universal Natural Language Processing with Limited Annotations: Try Few-shot Textual Entailment as a Start**](https://www.aclweb.org/anthology/2020.emnlp-main.660) , <br> by *Yin, Wenpeng  and
Rajani, Nazneen Fatema  and
Radev, Dragomir  and
Socher, Richard  and
Xiong, Caiming* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3268-L3279) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```yin-etal-2020-universal```
- [![](https://img.shields.io/badge/EMNLP_Findings-2020-green)](https://www.aclweb.org/anthology/2020.findings-emnlp.17)<a href="https://scholar.google.com.hk/scholar?q=Few-shot+Natural+Language+Generation+for+Task-Oriented+Dialog"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-shot Natural Language Generation for Task-Oriented Dialog**](https://www.aclweb.org/anthology/2020.findings-emnlp.17) , <br> by *Peng, Baolin  and
Zhu, Chenguang  and
Li, Chunyuan  and
Li, Xiujun  and
Li, Jinchao  and
Zeng, Michael  and
Gao, Jianfeng* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3281-L3294) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```peng-etal-2020-shot```
- [![](https://img.shields.io/badge/EMNLP_Findings-2020-green)](https://www.aclweb.org/anthology/2020.findings-emnlp.108)<a href="https://scholar.google.com.hk/scholar?q=Dynamic+Semantic+Matching+and+Aggregation+Network+for+Few-shot+Intent+Detection"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Dynamic Semantic Matching and Aggregation Network for Few-shot Intent Detection**](https://www.aclweb.org/anthology/2020.findings-emnlp.108) , <br> by *Nguyen, Hoang  and
Zhang, Chenwei  and
Xia, Congying  and
Yu, Philip* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3296-L3306) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```nguyen-etal-2020-dynamic```
- [![](https://img.shields.io/badge/EMNLP_Findings-2020-green)](https://www.aclweb.org/anthology/2020.findings-emnlp.303)<a href="https://scholar.google.com.hk/scholar?q=Composed+Variational+Natural+Language+Generation+for+Few-shot+Intents"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Composed Variational Natural Language Generation for Few-shot Intents**](https://www.aclweb.org/anthology/2020.findings-emnlp.303) , <br> by *Xia, Congying  and
Xiong, Caiming  and
Yu, Philip  and
Socher, Richard* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3308-L3318) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```xia-etal-2020-composed```
- [![](https://img.shields.io/badge/EMNLP-2019-green)](https://doi.org/10.18653/v1/D19-1649)<a href="https://scholar.google.com.hk/scholar?q=FewRel+2.0:+Towards+More+Challenging+Few-Shot+Relation+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**FewRel 2.0: Towards More Challenging Few-Shot Relation Classification**](https://doi.org/10.18653/v1/D19-1649) , <br> by *Tianyu Gao and
Xu Han and
Hao Zhu and
Zhiyuan Liu and
Peng Li and
Maosong Sun and
Jie Zhou* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1272-L1285) <br>```Fewrel 2.0 dataset
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```GaoHZLLSZ19```
- [![](https://img.shields.io/badge/EMNLP-2018-green)](https://doi.org/10.18653/v1/d18-1514)<a href="https://scholar.google.com.hk/scholar?q=FewRel:+A+Large-Scale+Supervised+Few-shot+Relation+Classification+Dataset+with+State-of-the-Art+Evaluation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**FewRel: A Large-Scale Supervised Few-shot Relation Classification
Dataset with State-of-the-Art Evaluation**](https://doi.org/10.18653/v1/d18-1514) , <br> by *Xu Han and
Hao Zhu and
Pengfei Yu and
Ziyun Wang and
Yuan Yao and
Zhiyuan Liu and
Maosong Sun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1255-L1269) <br>```FewRel dataset
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```HanZYWYLS18```
## NAACL

- [![](https://img.shields.io/badge/NAACL-2022-green)](https://aclanthology.org/2022.naacl-main.7)<a href="https://scholar.google.com.hk/scholar?q=LEA:+Meta+Knowledge-Driven+Self-Attentive+Document+Embedding+for+Few-Shot+Text+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**LEA: Meta Knowledge-Driven Self-Attentive Document Embedding for Few-Shot Text Classification**](https://aclanthology.org/2022.naacl-main.7) , <br> by *Hong, S. K.  and
Jang, Tae Young* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1-L9) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```hong-jang-2022-lea```
- [![](https://img.shields.io/badge/NAACL-2022-green)](https://aclanthology.org/2022.naacl-main.98)<a href="https://scholar.google.com.hk/scholar?q=On+the+Economics+of+Multilingual+Few-shot+Learning:+Modeling+the+Cost-Performance+Trade-offs+of+Machine+Translated+and+Manual+Data"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**On the Economics of Multilingual Few-shot Learning: Modeling the Cost-Performance Trade-offs of Machine Translated and Manual Data**](https://aclanthology.org/2022.naacl-main.98) , <br> by *Ahuja, Kabir  and
Choudhury, Monojit  and
Dandapat, Sandipan* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L11-L20) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ahuja-etal-2022-economics```
- [![](https://img.shields.io/badge/NAACL-2022-green)](https://aclanthology.org/2022.naacl-main.39)<a href="https://scholar.google.com.hk/scholar?q=Fine-tuning+Pre-trained+Language+Models+for+Few-shot+Intent+Detection:+Supervised+Pre-training+and+Isotropization"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Fine-tuning Pre-trained Language Models for Few-shot Intent Detection: Supervised Pre-training and Isotropization**](https://aclanthology.org/2022.naacl-main.39) , <br> by *Zhang, Haode  and
Liang, Haowen  and
Zhang, Yuwei  and
Zhan, Li-Ming  and
Wu, Xiao-Ming  and
Lu, Xiaolei  and
Lam, Albert* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L22-L35) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhang-etal-2022-fine```
- [![](https://img.shields.io/badge/NAACL-2022-green)](https://aclanthology.org/2022.naacl-main.260)<a href="https://scholar.google.com.hk/scholar?q=Improving+In-Context+Few-Shot+Learning+via+Self-Supervised+Training"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Improving In-Context Few-Shot Learning via Self-Supervised Training**](https://aclanthology.org/2022.naacl-main.260) , <br> by *Chen, Mingda  and
Du, Jingfei  and
Pasunuru, Ramakanth  and
Mihaylov, Todor  and
Iyer, Srini  and
Stoyanov, Veselin  and
Kozareva, Zornitsa* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L37-L50) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```chen-etal-2022-improving```
- [![](https://img.shields.io/badge/NAACL-2022-green)](https://aclanthology.org/2022.naacl-main.369)<a href="https://scholar.google.com.hk/scholar?q=An+Enhanced+Span-based+Decomposition+Method+for+Few-Shot+Sequence+Labeling"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**An Enhanced Span-based Decomposition Method for Few-Shot Sequence Labeling**](https://aclanthology.org/2022.naacl-main.369) , <br> by *Wang, Peiyi  and
Xu, Runxin  and
Liu, Tianyu  and
Zhou, Qingyu  and
Cao, Yunbo  and
Chang, Baobao  and
Sui, Zhifang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L53-L66) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```wang-etal-2022-enhanced```
- [![](https://img.shields.io/badge/NAACL-2022-green)](https://aclanthology.org/2022.naacl-main.141)<a href="https://scholar.google.com.hk/scholar?q=MGIMN:+Multi-Grained+Interactive+Matching+Network+for+Few-shot+Text+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**MGIMN: Multi-Grained Interactive Matching Network for Few-shot Text Classification**](https://aclanthology.org/2022.naacl-main.141) , <br> by *Zhang, Jianhai  and
Maimaiti, Mieradilijiang  and
Xing, Gao  and
Zheng, Yuanhang  and
Zhang, Ji* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L69-L80) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhang-etal-2022-mgimn```
- [![](https://img.shields.io/badge/NAACL-2022-green)](https://aclanthology.org/2022.naacl-main.47)<a href="https://scholar.google.com.hk/scholar?q=Reframing+Human-AI+Collaboration+for+Generating+Free-Text+Explanations"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Reframing Human-AI Collaboration for Generating Free-Text Explanations**](https://aclanthology.org/2022.naacl-main.47) , <br> by *Wiegreffe, Sarah  and
Hessel, Jack  and
Swayamdipta, Swabha  and
Riedl, Mark  and
Choi, Yejin* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L82-L93) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```wiegreffe-etal-2022-reframing```
- [![](https://img.shields.io/badge/NAACL-2022-green)](https://aclanthology.org/2022.naacl-main.421)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Document-Level+Relation+Extraction"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Document-Level Relation Extraction**](https://aclanthology.org/2022.naacl-main.421) , <br> by *Popovic, Nicholas  and
F{\"a}rber, Michael* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L95-L103) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```popovic-farber-2022-shot```
- [![](https://img.shields.io/badge/NAACL-2022-green)](https://aclanthology.org/2022.naacl-main.420)<a href="https://scholar.google.com.hk/scholar?q=Template-free+Prompt+Tuning+for+Few-shot+NER"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Template-free Prompt Tuning for Few-shot NER**](https://aclanthology.org/2022.naacl-main.420) , <br> by *Ma, Ruotian  and
Zhou, Xin  and
Gui, Tao  and
Tan, Yiding  and
Li, Linyang  and
Zhang, Qi  and
Huang, Xuanjing* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L105-L118) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ma-etal-2022-template```
- [![](https://img.shields.io/badge/NAACL-2022-green)](https://aclanthology.org/2022.naacl-main.201)<a href="https://scholar.google.com.hk/scholar?q=MetaICL:+Learning+to+Learn+In+Context"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**MetaICL: Learning to Learn In Context**](https://aclanthology.org/2022.naacl-main.201) , <br> by *Min, Sewon  and
Lewis, Mike  and
Zettlemoyer, Luke  and
Hajishirzi, Hannaneh* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L120-L130) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```min-etal-2022-metaicl```
- [![](https://img.shields.io/badge/NAACL-2022-green)](https://aclanthology.org/2022.naacl-main.408)<a href="https://scholar.google.com.hk/scholar?q=Contrastive+Learning+for+Prompt-based+Few-shot+Language+Learners"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Contrastive Learning for Prompt-based Few-shot Language Learners**](https://aclanthology.org/2022.naacl-main.408) , <br> by *Jian, Yiren  and
Gao, Chongyang  and
Vosoughi, Soroush* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L132-L141) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```jian-etal-2022-contrastive```
- [![](https://img.shields.io/badge/NAACL-2022-green)](https://aclanthology.org/2022.naacl-main.404)<a href="https://scholar.google.com.hk/scholar?q=Embedding+Hallucination+for+Few-shot+Language+Fine-tuning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Embedding Hallucination for Few-shot Language Fine-tuning**](https://aclanthology.org/2022.naacl-main.404) , <br> by *Jian, Yiren  and
Gao, Chongyang  and
Vosoughi, Soroush* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L143-L152) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```jian-etal-2022-embedding```
- [![](https://img.shields.io/badge/NAACL-2022-green)](https://aclanthology.org/2022.naacl-main.401)<a href="https://scholar.google.com.hk/scholar?q=Automatic+Multi-Label+Prompting:+Simple+and+Interpretable+Few-Shot+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Automatic Multi-Label Prompting: Simple and Interpretable Few-Shot Classification**](https://aclanthology.org/2022.naacl-main.401) , <br> by *Wang, Han  and
Xu, Canwen  and
McAuley, Julian* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L154-L163) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```wang-etal-2022-automatic```
- [![](https://img.shields.io/badge/NAACL_HLT-2021-green)](https://www.aclweb.org/anthology/2021.naacl-main.88)<a href="https://scholar.google.com.hk/scholar?q=DReCa:+A+General+Task+Augmentation+Strategy+for+Few-Shot+Natural+Language+Inference"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**DReCa: A General Task Augmentation Strategy for Few-Shot Natural Language Inference**](https://www.aclweb.org/anthology/2021.naacl-main.88) , <br> by *Murty, Shikhar  and
Hashimoto, Tatsunori B.  and
Manning, Christopher* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2655-L2664) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```murty-etal-2021-dreca```
- [![](https://img.shields.io/badge/NAACL_HLT-2021-green)](https://www.aclweb.org/anthology/2021.naacl-main.410)<a href="https://scholar.google.com.hk/scholar?q=Learning+How+to+Ask:+Querying+LMs+with+Mixtures+of+Soft+Prompts"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning How to Ask: Querying LMs with Mixtures of Soft Prompts**](https://www.aclweb.org/anthology/2021.naacl-main.410) , <br> by *Qin, Guanghui  and
Eisner, Jason* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2969-L2977) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```qin-eisner-2021-learning```
- [![](https://img.shields.io/badge/NAACL_HLT-2021-green)](https://www.aclweb.org/anthology/2021.naacl-main.398)<a href="https://scholar.google.com.hk/scholar?q=Factual+Probing+Is+[MASK]:+Learning+vs.+Learning+to+Recall"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Factual Probing Is [MASK]: Learning vs. Learning to Recall**](https://www.aclweb.org/anthology/2021.naacl-main.398) , <br> by *Zhong, Zexuan  and
Friedman, Dan  and
Chen, Danqi* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2989-L2998) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhong-etal-2021-factual```
- [![](https://img.shields.io/badge/NAACL_HLT-2021-green)](https://www.aclweb.org/anthology/2021.naacl-main.185)<a href="https://scholar.google.com.hk/scholar?q=It's+Not+Just+Size+That+Matters:+Small+Language+Models+Are+Also+Few-Shot+Learners"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners**](https://www.aclweb.org/anthology/2021.naacl-main.185) , <br> by *Schick, Timo and
Sch{\"u}tze, Hinrich* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3000-L3008) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```schick-schutze-2021-just```
- [![](https://img.shields.io/badge/NAACL_HLT-2021-green)](https://www.aclweb.org/anthology/2021.naacl-main.59)<a href="https://scholar.google.com.hk/scholar?q=Few-shot+Intent+Classification+and+Slot+Filling+with+Retrieved+Examples"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-shot Intent Classification and Slot Filling with Retrieved Examples**](https://www.aclweb.org/anthology/2021.naacl-main.59) , <br> by *Yu, Dian  and
He, Luheng  and
Zhang, Yuan  and
Du, Xinya  and
Pasupat, Panupong  and
Li, Qi* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3320-L3332) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```yu-etal-2021-shot```
- [![](https://img.shields.io/badge/NAACL_HLT-2021-green)](https://www.aclweb.org/anthology/2021.naacl-main.106)<a href="https://scholar.google.com.hk/scholar?q=Incremental+Few-shot+Text+Classification+with+Multi-round+New+Classes:+Formulation,+Dataset+and+System"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Incremental Few-shot Text Classification with Multi-round New Classes: Formulation, Dataset and System**](https://www.aclweb.org/anthology/2021.naacl-main.106) , <br> by *Xia, Congying  and
Yin, Wenpeng  and
Feng, Yihao  and
Yu, Philip* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3335-L3345) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```xia-etal-2021-incremental```
- [![](https://img.shields.io/badge/NAACL_HLT-2021-green)](https://www.aclweb.org/anthology/2021.naacl-main.158)<a href="https://scholar.google.com.hk/scholar?q=Towards+Few-shot+Fact-Checking+via+Perplexity"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Towards Few-shot Fact-Checking via Perplexity**](https://www.aclweb.org/anthology/2021.naacl-main.158) , <br> by *Lee, Nayeon  and
Bang, Yejin  and
Madotto, Andrea  and
Fung, Pascale* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3347-L3357) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```lee-etal-2021-towards```
- [![](https://img.shields.io/badge/NAACL_HLT-2021-green)](https://www.aclweb.org/anthology/2021.naacl-main.261)<a href="https://scholar.google.com.hk/scholar?q=Knowledge+Guided+Metric+Learning+for+Few-Shot+Text+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Knowledge Guided Metric Learning for Few-Shot Text Classification**](https://www.aclweb.org/anthology/2021.naacl-main.261) , <br> by *Sui, Dianbo  and
Chen, Yubo  and
Mao, Binjie  and
Qiu, Delai  and
Liu, Kang  and
Zhao, Jun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3360-L3375) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```sui-etal-2021-knowledge```
- [![](https://img.shields.io/badge/NAACL_HLT-2021-green)](https://www.aclweb.org/anthology/2021.naacl-main.264)<a href="https://scholar.google.com.hk/scholar?q=ConVEx:+Data-Efficient+and+Few-Shot+Slot+Labeling"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**ConVEx: Data-Efficient and Few-Shot Slot Labeling**](https://www.aclweb.org/anthology/2021.naacl-main.264) , <br> by *Henderson, Matthew  and
Vuli{\'c}, Ivan* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3377-L3385) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```henderson-vulic-2021-convex```
- [![](https://img.shields.io/badge/NAACL_HLT-2021-green)](https://www.aclweb.org/anthology/2021.naacl-main.434)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Text+Classification+with+Triplet+Networks,+Data+Augmentation,+and+Curriculum+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Text Classification with Triplet Networks, Data Augmentation, and Curriculum Learning**](https://www.aclweb.org/anthology/2021.naacl-main.434) , <br> by *Wei, Jason  and
Huang, Chengyu  and
Vosoughi, Soroush  and
Cheng, Yu  and
Xu, Shiqi* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3387-L3398) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```wei-etal-2021-shot```
## COLING

- [![](https://img.shields.io/badge/COLING-2020-green)](https://doi.org/10.18653/v1/2020.coling-main.563)<a href="https://scholar.google.com.hk/scholar?q=Bridging+Text+and+Knowledge+with+Multi-Prototype+Embedding+for+Few-Shot+Relational+Triple+Extraction"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Bridging Text and Knowledge with Multi-Prototype Embedding for Few-Shot
Relational Triple Extraction**](https://doi.org/10.18653/v1/2020.coling-main.563) , <br> by *Haiyang Yu and
Ningyu Zhang and
Shumin Deng and
Hongbin Ye and
Wei Zhang and
Huajun Chen* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1240-L1253) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```YuZDYZC20```
## EACL

- [![](https://img.shields.io/badge/EACL-2021-green)](https://www.aclweb.org/anthology/2021.eacl-main.20)<a href="https://scholar.google.com.hk/scholar?q=Exploiting+Cloze-Questions+for+Few-Shot+Text+Classification+and+Natural+Language+Inference"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference**](https://www.aclweb.org/anthology/2021.eacl-main.20) , <br> by *Schick, Timo and
Sch{\"u}tze, Hinrich* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2979-L2987) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```schick-schutze-2021-exploiting```
## ICML

- [![](https://img.shields.io/badge/ICML-2021-green)](http://proceedings.mlr.press/v139/lee21d.html)<a href="https://scholar.google.com.hk/scholar?q=Unsupervised+Embedding+Adaptation+via+Early-Stage+Feature+Reconstruction+for+Few-Shot+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Unsupervised Embedding Adaptation via Early-Stage Feature Reconstruction for Few-Shot Classification**](http://proceedings.mlr.press/v139/lee21d.html) , <br> by *Lee, Dong Hoon and Chung, Sae-Young* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1350-L1357) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v139-lee21d```
- [![](https://img.shields.io/badge/ICML-2021-green)](http://proceedings.mlr.press/v139/shin21a.html)<a href="https://scholar.google.com.hk/scholar?q=Large-Scale+Meta-Learning+with+Continual+Trajectory+Shifting"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Large-Scale Meta-Learning with Continual Trajectory Shifting**](http://proceedings.mlr.press/v139/shin21a.html) , <br> by *Shin, Jaewoong, Lee, Hae Beom, Gong, Boqing and Hwang, Sung Ju* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1523-L1530) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v139-shin21a```
- [![](https://img.shields.io/badge/ICML-2021-green)](http://proceedings.mlr.press/v139/zhu21d.html)<a href="https://scholar.google.com.hk/scholar?q=Few-shot+Language+Coordination+by+Modeling+Theory+of+Mind"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-shot Language Coordination by Modeling Theory of Mind**](http://proceedings.mlr.press/v139/zhu21d.html) , <br> by *Zhu, Hao, Neubig, Graham and Bisk, Yonatan* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1533-L1540) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v139-zhu21d```
- [![](https://img.shields.io/badge/ICML-2021-green)](http://proceedings.mlr.press/v139/zhao21c.html)<a href="https://scholar.google.com.hk/scholar?q=Calibrate+Before+Use:+Improving+Few-shot+Performance+of+Language+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Calibrate Before Use: Improving Few-shot Performance of Language Models**](http://proceedings.mlr.press/v139/zhao21c.html) , <br> by *Zhao, Zihao, Wallace, Eric, Feng, Shi, Klein, Dan and Singh, Sameer* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1544-L1551) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v139-zhao21c```
- [![](https://img.shields.io/badge/ICML-2021-green)](http://proceedings.mlr.press/v139/zhao21d.html)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Neural+Architecture+Search"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Neural Architecture Search**](http://proceedings.mlr.press/v139/zhao21d.html) , <br> by *Zhao, Yiyang, Wang, Linnan, Tian, Yuandong, Fonseca, Rodrigo and Guo, Tian* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1554-L1561) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v139-zhao21d```
- [![](https://img.shields.io/badge/ICML-2021-green)](http://proceedings.mlr.press/v139/triantafillou21a.html)<a href="https://scholar.google.com.hk/scholar?q=Learning+a+Universal+Template+for+Few-shot+Dataset+Generalization"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning a Universal Template for Few-shot Dataset Generalization**](http://proceedings.mlr.press/v139/triantafillou21a.html) , <br> by *Triantafillou, Eleni, Larochelle, Hugo, Zemel, Richard and Dumoulin, Vincent* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1564-L1571) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v139-triantafillou21a```
- [![](https://img.shields.io/badge/ICML-2021-green)](http://proceedings.mlr.press/v139/cui21a.html)<a href="https://scholar.google.com.hk/scholar?q=Parameterless+Transductive+Feature+Re-representation+for+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Parameterless Transductive Feature Re-representation for Few-Shot Learning**](http://proceedings.mlr.press/v139/cui21a.html) , <br> by *Cui, Wentao and Guo, Yuhong* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1574-L1581) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v139-cui21a```
- [![](https://img.shields.io/badge/ICML-2021-green)](http://proceedings.mlr.press/v139/bai21a.html)<a href="https://scholar.google.com.hk/scholar?q=How+Important+is+the+Train-Validation+Split+in+Meta-Learning?"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**How Important is the Train-Validation Split in Meta-Learning?**](http://proceedings.mlr.press/v139/bai21a.html) , <br> by *Bai, Yu, Chen, Minshuo, Zhou, Pan, Zhao, Tuo, Lee, Jason, Kakade, Sham, Wang, Huan and Xiong, Caiming* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1584-L1591) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v139-bai21a```
- [![](https://img.shields.io/badge/ICML-2021-green)](http://proceedings.mlr.press/v139/fisch21a.html)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Conformal+Prediction+with+Auxiliary+Tasks"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Conformal Prediction with Auxiliary Tasks**](http://proceedings.mlr.press/v139/fisch21a.html) , <br> by *Fisch, Adam, Schuster, Tal, Jaakkola, Tommi and Barzilay, Dr.Regina* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1594-L1601) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v139-fisch21a```
- [![](https://img.shields.io/badge/ICML-2021-green)](http://proceedings.mlr.press/v139/konobeev21a.html)<a href="https://scholar.google.com.hk/scholar?q=A+Distribution-dependent+Analysis+of+Meta+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**A Distribution-dependent Analysis of Meta Learning**](http://proceedings.mlr.press/v139/konobeev21a.html) , <br> by *Konobeev, Mikhail, Kuzborskij, Ilja and Szepesvari, Csaba* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1604-L1611) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v139-konobeev21a```
- [![](https://img.shields.io/badge/ICML-2021-green)](http://proceedings.mlr.press/v139/ni21a.html)<a href="https://scholar.google.com.hk/scholar?q=Data+Augmentation+for+Meta-Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Data Augmentation for Meta-Learning**](http://proceedings.mlr.press/v139/ni21a.html) , <br> by *Ni, Renkun, Goldblum, Micah, Sharaf, Amr, Kong, Kezhi and Goldstein, Tom* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1614-L1621) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v139-ni21a```
- [![](https://img.shields.io/badge/ICML-2021-green)](http://proceedings.mlr.press/v139/wang21ad.html)<a href="https://scholar.google.com.hk/scholar?q=Bridging+Multi-Task+Learning+and+Meta-Learning:+Towards+Efficient+Training+and+Effective+Adaptation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Bridging Multi-Task Learning and Meta-Learning: Towards Efficient Training and Effective Adaptation**](http://proceedings.mlr.press/v139/wang21ad.html) , <br> by *Wang, Haoxiang, Zhao, Han and Li, Bo* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1624-L1631) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v139-wang21ad```
- [![](https://img.shields.io/badge/ICML-2021-green)](http://proceedings.mlr.press/v139/vedantam21a.html)<a href="https://scholar.google.com.hk/scholar?q=CURI:+A+Benchmark+for+Productive+Concept+Learning+Under+Uncertainty"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**CURI: A Benchmark for Productive Concept Learning Under Uncertainty**](http://proceedings.mlr.press/v139/vedantam21a.html) , <br> by *Vedantam, Ramakrishna, Szlam, Arthur, Nickel, Maximillian, Morcos, Ari and Lake, Brenden M* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1634-L1641) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v139-vedantam21a```
- [![](https://img.shields.io/badge/ICML-2021-green)](http://proceedings.mlr.press/v139/saunshi21a.html)<a href="https://scholar.google.com.hk/scholar?q=A+Representation+Learning+Perspective+on+the+Importance+of+Train-Validation+Splitting+in+Meta-Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**A Representation Learning Perspective on the Importance of Train-Validation Splitting in Meta-Learning**](http://proceedings.mlr.press/v139/saunshi21a.html) , <br> by *Saunshi, Nikunj, Gupta, Arushi and Hu, Wei* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1644-L1651) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v139-saunshi21a```
- [![](https://img.shields.io/badge/ICML-2021-green)](http://proceedings.mlr.press/v139/acar21b.html)<a href="https://scholar.google.com.hk/scholar?q=Memory+Efficient+Online+Meta+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Memory Efficient Online Meta Learning**](http://proceedings.mlr.press/v139/acar21b.html) , <br> by *Acar, Durmus Alp Emre, Zhu, Ruizhao and Saligrama, Venkatesh* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1654-L1661) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v139-acar21b```
- [![](https://img.shields.io/badge/ICML-2021-green)](http://proceedings.mlr.press/v139/yap21a.html)<a href="https://scholar.google.com.hk/scholar?q=Addressing+Catastrophic+Forgetting+in+Few-Shot+Problems"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Addressing Catastrophic Forgetting in Few-Shot Problems**](http://proceedings.mlr.press/v139/yap21a.html) , <br> by *Yap, Pauching, Ritter, Hippolyt and Barber, David* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1663-L1670) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v139-yap21a```
- [![](https://img.shields.io/badge/ICML-2021-green)](http://proceedings.mlr.press/v139/achituve21a.html)<a href="https://scholar.google.com.hk/scholar?q=GP-Tree:+A+Gaussian+Process+Classifier+for+Few-Shot+Incremental+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**GP-Tree: A Gaussian Process Classifier for Few-Shot Incremental Learning**](http://proceedings.mlr.press/v139/achituve21a.html) , <br> by *Achituve, Idan, Navon, Aviv, Yemini, Yochai, Chechik, Gal and Fetaya, Ethan* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1672-L1679) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v139-achituve21a```
- [![](https://img.shields.io/badge/ICML-2020-green)](http://proceedings.mlr.press/v119/bronskill20a.html)<a href="https://scholar.google.com.hk/scholar?q=TaskNorm:+Rethinking+Batch+Normalization+for+Meta-Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**TaskNorm: Rethinking Batch Normalization for Meta-Learning**](http://proceedings.mlr.press/v119/bronskill20a.html) , <br> by *Bronskill, John, Gordon, Jonathan, Requeima, James, Nowozin, Sebastian and Turner, Richard* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2033-L2040) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v119-bronskill20a```
- [![](https://img.shields.io/badge/ICML-2020-green)](http://proceedings.mlr.press/v119/goldblum20a.html)<a href="https://scholar.google.com.hk/scholar?q=Unraveling+Meta-Learning:+Understanding+Feature+Representations+for+Few-Shot+Tasks"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Unraveling Meta-Learning: Understanding Feature Representations for Few-Shot Tasks**](http://proceedings.mlr.press/v119/goldblum20a.html) , <br> by *Goldblum, Micah, Reich, Steven, Fowl, Liam, Ni, Renkun, Cherepanova, Valeriia and Goldstein, Tom* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2043-L2050) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v119-goldblum20a```
- [![](https://img.shields.io/badge/ICML-2020-green)](http://proceedings.mlr.press/v119/iakovleva20a.html)<a href="https://scholar.google.com.hk/scholar?q=Meta-Learning+with+Shared+Amortized+Variational+Inference"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta-Learning with Shared Amortized Variational Inference**](http://proceedings.mlr.press/v119/iakovleva20a.html) , <br> by *Iakovleva, Ekaterina, Verbeek, Jakob and Alahari, Karteek* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2053-L2060) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v119-iakovleva20a```
- [![](https://img.shields.io/badge/ICML-2020-green)](http://proceedings.mlr.press/v119/park20b.html)<a href="https://scholar.google.com.hk/scholar?q=Meta+Variance+Transfer:+Learning+to+Augment+from+the+Others"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta Variance Transfer: Learning to Augment from the Others**](http://proceedings.mlr.press/v119/park20b.html) , <br> by *Park, Seong-Jin, Han, Seungju, Baek, Ji-Won, Kim, Insoo, Song, Juhwan, Lee, Hae Beom, Han, Jae-Joon and Hwang, Sung Ju* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2063-L2070) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v119-park20b```
- [![](https://img.shields.io/badge/ICML-2020-green)](http://proceedings.mlr.press/v119/qu20a.html)<a href="https://scholar.google.com.hk/scholar?q=Few-shot+Relation+Extraction+via+Bayesian+Meta-learning+on+Relation+Graphs"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-shot Relation Extraction via Bayesian Meta-learning on Relation Graphs**](http://proceedings.mlr.press/v119/qu20a.html) , <br> by *Qu, Meng, Gao, Tianyu, Xhonneux, Louis-Pascal and Tang, Jian* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2073-L2080) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v119-qu20a```
- [![](https://img.shields.io/badge/ICML-2020-green)](http://proceedings.mlr.press/v119/teshima20a.html)<a href="https://scholar.google.com.hk/scholar?q=Few-shot+Domain+Adaptation+by+Causal+Mechanism+Transfer"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-shot Domain Adaptation by Causal Mechanism Transfer**](http://proceedings.mlr.press/v119/teshima20a.html) , <br> by *Teshima, Takeshi, Sato, Issei and Sugiyama, Masashi* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2083-L2090) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v119-teshima20a```
- [![](https://img.shields.io/badge/ICML-2020-green)](http://proceedings.mlr.press/v119/wang20j.html)<a href="https://scholar.google.com.hk/scholar?q=Frustratingly+Simple+Few-Shot+Object+Detection"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Frustratingly Simple Few-Shot Object Detection**](http://proceedings.mlr.press/v119/wang20j.html) , <br> by *Wang, Xin, Huang, Thomas, Gonzalez, Joseph, Darrell, Trevor and Yu, Fisher* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2092-L2099) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v119-wang20j```
- [![](https://img.shields.io/badge/ICML-2020-green)](http://proceedings.mlr.press/v119/yoon20b.html)<a href="https://scholar.google.com.hk/scholar?q=XtarNet:+Learning+to+Extract+Task-Adaptive+Representation+for+Incremental+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**XtarNet: Learning to Extract Task-Adaptive Representation for Incremental Few-Shot Learning**](http://proceedings.mlr.press/v119/yoon20b.html) , <br> by *Yoon, Sung Whan, Kim, Do-Yeon, Seo, Jun and Moon, Jaekyun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2102-L2109) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v119-yoon20b```
- [![](https://img.shields.io/badge/ICML-2019-green)]( http://proceedings.mlr.press/v97/allen19b.html )<a href="https://scholar.google.com.hk/scholar?q=Infinite+Mixture+Prototypes+for+Few-shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Infinite Mixture Prototypes for Few-shot Learning**]( http://proceedings.mlr.press/v97/allen19b.html ) , <br> by *Allen, Kelsey, Shelhamer, Evan, Shin, Hanul and Tenenbaum, Joshua* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2112-L2119) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v97-allen19b```
- [![](https://img.shields.io/badge/ICML-2019-green)](http://proceedings.mlr.press/v97/li19c.html)<a href="https://scholar.google.com.hk/scholar?q=LGM-Net:+Learning+to+Generate+Matching+Networks+for+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**LGM-Net: Learning to Generate Matching Networks for Few-Shot Learning**](http://proceedings.mlr.press/v97/li19c.html) , <br> by *Li, Huaiyu, Dong, Weiming, Mei, Xing, Ma, Chongyang, Huang, Feiyue and Hu, Bao-Gang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2122-L2129) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v97-li19c```
- [![](https://img.shields.io/badge/ICML-2019-green)](http://proceedings.mlr.press/v97/yoon19a.html)<a href="https://scholar.google.com.hk/scholar?q=TapNet:+Neural+Network+Augmented+with+Task-Adaptive+Projection+for+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**TapNet: Neural Network Augmented with Task-Adaptive Projection for Few-Shot Learning**](http://proceedings.mlr.press/v97/yoon19a.html) , <br> by *Yoon, Sung Whan, Seo, Jun and Moon, Jaekyun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2131-L2138) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v97-yoon19a```
- [![](https://img.shields.io/badge/ICML-2019-green)](
http://proceedings.mlr.press/v97/yao19b.html)<a href="https://scholar.google.com.hk/scholar?q=Hierarchically+Structured+Meta-learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Hierarchically Structured Meta-learning**](
http://proceedings.mlr.press/v97/yao19b.html) , <br> by *Yao, Huaxiu, Wei, Ying, Huang, Junzhou and Li, Zhenhui* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2140-L2149) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v97-yao19b```
- [![](https://img.shields.io/badge/ICML-2019-green)](
http://proceedings.mlr.press/v97/zintgraf19a.html)<a href="https://scholar.google.com.hk/scholar?q=Fast+Context+Adaptation+via+Meta-Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Fast Context Adaptation via Meta-Learning**](
http://proceedings.mlr.press/v97/zintgraf19a.html) , <br> by *Zintgraf, Luisa, Shiarli, Kyriacos, Kurin, Vitaly, Hofmann, Katja and Whiteson, Shimon* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2151-L2160) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v97-zintgraf19a```
- [![](https://img.shields.io/badge/ICML-2018-green)](http://proceedings.mlr.press/v80/zhao18c.html)<a href="https://scholar.google.com.hk/scholar?q=MSplit+LBI:+Realizing+Feature+Selection+and+Dense+Estimation+Simultaneously+in+Few-shot+and+Zero-shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**MSplit LBI: Realizing Feature Selection and Dense Estimation Simultaneously in Few-shot and Zero-shot Learning**](http://proceedings.mlr.press/v80/zhao18c.html) , <br> by *Zhao, Bo, Sun, Xinwei, Fu, Yanwei, Yao, Yuan and Wang, Yizhou* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2163-L2170) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v80-zhao18c```
- [![](https://img.shields.io/badge/ICML-2018-green)](http://proceedings.mlr.press/v80/amit18a.html)<a href="https://scholar.google.com.hk/scholar?q=Meta-Learning+by+Adjusting+Priors+Based+on+Extended+PAC-Bayes+Theory"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta-Learning by Adjusting Priors Based on Extended PAC-Bayes Theory**](http://proceedings.mlr.press/v80/amit18a.html) , <br> by *Amit, Ron and Meir, Ron* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2173-L2180) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v80-amit18a```
- [![](https://img.shields.io/badge/ICML-2018-green)](http://proceedings.mlr.press/v80/franceschi18a.html)<a href="https://scholar.google.com.hk/scholar?q=Bilevel+Programming+for+Hyperparameter+Optimization+and+Meta-Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Bilevel Programming for Hyperparameter Optimization and Meta-Learning**](http://proceedings.mlr.press/v80/franceschi18a.html) , <br> by *Franceschi, Luca, Frasconi, Paolo, Salzo, Saverio, Grazzi, Riccardo and Pontil, Massimiliano* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2183-L2190) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v80-franceschi18a```
- [![](https://img.shields.io/badge/ICML-2018-green)](http://proceedings.mlr.press/v80/lee18a.html)<a href="https://scholar.google.com.hk/scholar?q=Gradient-Based+Meta-Learning+with+Learned+Layerwise+Metric+and+Subspace"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace**](http://proceedings.mlr.press/v80/lee18a.html) , <br> by *Lee, Yoonho and Choi, Seungjin* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2193-L2200) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v80-lee18a```
- [![](https://img.shields.io/badge/ICML-2017-green)](http://proceedings.mlr.press/v70/finn17a.html)<a href="https://scholar.google.com.hk/scholar?q=Model-Agnostic+Meta-Learning+for+Fast+Adaptation+of+Deep+Networks"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks**](http://proceedings.mlr.press/v70/finn17a.html) , <br> by *Chelsea Finn, Pieter Abbeel and Sergey Levine* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2203-L2210) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v70-finn17a```
- [![](https://img.shields.io/badge/ICML-2017-green)](http://proceedings.mlr.press/v70/munkhdalai17a.html)<a href="https://scholar.google.com.hk/scholar?q=Meta+Networks"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta Networks**](http://proceedings.mlr.press/v70/munkhdalai17a.html) , <br> by *Tsendsuren Munkhdalai and Hong Yu* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2212-L2219) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v70-munkhdalai17a```
## ICLR

- [![](https://img.shields.io/badge/ICLR-2022-green)](https://openreview.net/forum?id=ek9a0qIafW)<a href="https://scholar.google.com.hk/scholar?q=Differentiable+Prompt+Makes+Pre-trained+Language+Models+Better+Few-shot+Learners"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners**](https://openreview.net/forum?id=ek9a0qIafW) , <br> by *Ningyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang and Huajun Chen* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L529-L535) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhang2022differentiable```
- [![](https://img.shields.io/badge/ICLR-2022-green)](https://openreview.net/forum?id=V3C8p78sDa)<a href="https://scholar.google.com.hk/scholar?q=Exploring+the+Limits+of+Large+Scale+Pre-training"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Exploring the Limits of Large Scale Pre-training**](https://openreview.net/forum?id=V3C8p78sDa) , <br> by *Samira Abnar, Mostafa Dehghani, Behnam Neyshabur and Hanie Sedghi* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L538-L544) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abnar2022exploring```
- [![](https://img.shields.io/badge/ICLR-2022-green)](https://openreview.net/forum?id=boJy41J-tnQ)<a href="https://scholar.google.com.hk/scholar?q=Subspace+Regularizers+for+Few-Shot+Class+Incremental+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Subspace Regularizers for Few-Shot Class Incremental Learning**](https://openreview.net/forum?id=boJy41J-tnQ) , <br> by *Afra Feyza Aky{\"u}rek, Ekin Aky{\"u}rek, Derry Wijaya and Jacob Andreas* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L547-L553) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```akyurek2022subspace```
- [![](https://img.shields.io/badge/ICLR-2022-green)](https://openreview.net/forum?id=u2GZOiUTbt)<a href="https://scholar.google.com.hk/scholar?q=Task+Affinity+with+Maximum+Bipartite+Matching+in+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Task Affinity with Maximum Bipartite Matching in Few-Shot Learning**](https://openreview.net/forum?id=u2GZOiUTbt) , <br> by *Cat Phuoc Le, Juncheng Dong, Mohammadreza Soltani and Vahid Tarokh* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L556-L562) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```le2022task```
- [![](https://img.shields.io/badge/ICLR-2022-green)](https://openreview.net/forum?id=DNRADop4ksB)<a href="https://scholar.google.com.hk/scholar?q=On+the+Importance+of+Firth+Bias+Reduction+in+Few-Shot+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**On the Importance of Firth Bias Reduction in Few-Shot Classification**](https://openreview.net/forum?id=DNRADop4ksB) , <br> by *Saba Ghaffari, Ehsan Saleh, David Forsyth and Yu-Xiong Wang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L565-L571) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ghaffari2022on```
- [![](https://img.shields.io/badge/ICLR-2022-green)](https://openreview.net/forum?id=H-iABMvzIc)<a href="https://scholar.google.com.hk/scholar?q=Switch+to+Generalize:+Domain-Switch+Learning+for+Cross-Domain+Few-Shot+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Switch to Generalize: Domain-Switch Learning for Cross-Domain Few-Shot Classification**](https://openreview.net/forum?id=H-iABMvzIc) , <br> by *Zhengdong Hu, Yifan Sun and Yi Yang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L574-L580) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```hu2022switch```
- [![](https://img.shields.io/badge/ICLR-2022-green)](https://openreview.net/forum?id=HCRVf71PMF)<a href="https://scholar.google.com.hk/scholar?q=LFPT5:+A+Unified+Framework+for+Lifelong+Few-shot+Language+Learning+Based+on+Prompt+Tuning+of+T5"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**LFPT5: A Unified Framework for Lifelong Few-shot Language Learning Based on Prompt Tuning of T5**](https://openreview.net/forum?id=HCRVf71PMF) , <br> by *Chengwei Qin and Shafiq Joty* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L583-L589) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```qin2022lfpt```
- [![](https://img.shields.io/badge/ICLR-2022-green)](https://openreview.net/forum?id=xKZ4K0lTj_)<a href="https://scholar.google.com.hk/scholar?q=Hierarchical+Few-Shot+Imitation+with+Skill+Transition+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Hierarchical Few-Shot Imitation with Skill Transition Models**](https://openreview.net/forum?id=xKZ4K0lTj_) , <br> by *Kourosh Hakhamaneshi, Ruihan Zhao, Albert Zhan, Pieter Abbeel and Michael Laskin* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L592-L598) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```hakhamaneshi2022hierarchical```
- [![](https://img.shields.io/badge/ICLR-2022-green)](https://openreview.net/forum?id=zRJu6mU2BaE)<a href="https://scholar.google.com.hk/scholar?q=ConFeSS:+A+Framework+for+Single+Source+Cross-Domain+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**ConFeSS: A Framework for Single Source Cross-Domain Few-Shot Learning**](https://openreview.net/forum?id=zRJu6mU2BaE) , <br> by *Debasmit Das, Sungrack Yun and Fatih Porikli* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L601-L607) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```das2022confess```
- [![](https://img.shields.io/badge/ICLR-2022-green)](https://openreview.net/forum?id=i3RI65sR7N)<a href="https://scholar.google.com.hk/scholar?q=Hierarchical+Variational+Memory+for+Few-shot+Learning+Across+Domains"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Hierarchical Variational Memory for Few-shot Learning Across Domains**](https://openreview.net/forum?id=i3RI65sR7N) , <br> by *Yingjun Du, Xiantong Zhen, Ling Shao and Cees G. M. Snoek* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L610-L616) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```du2022hierarchical```
- [![](https://img.shields.io/badge/ICLR-2022-green)](https://openreview.net/forum?id=p3DKPQ7uaAi)<a href="https://scholar.google.com.hk/scholar?q=Temporal+Alignment+Prediction+for+Supervised+Representation+Learning+and+Few-Shot+Sequence+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Temporal Alignment Prediction for Supervised Representation Learning and Few-Shot Sequence Classification**](https://openreview.net/forum?id=p3DKPQ7uaAi) , <br> by *Bing Su and Ji-Rong Wen* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L619-L625) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```su2022temporal```
- [![](https://img.shields.io/badge/ICLR-2022-green)](https://openreview.net/forum?id=_jMtny3sMKU)<a href="https://scholar.google.com.hk/scholar?q=Generalizing+Few-Shot+NAS+with+Gradient+Matching"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Generalizing Few-Shot NAS with Gradient Matching**](https://openreview.net/forum?id=_jMtny3sMKU) , <br> by *Shoukang Hu, Ruochen Wang, Lanqing HONG, Zhenguo Li, Cho-Jui Hsieh and Jiashi Feng* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L628-L634) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```hu2022generalizing```
- [![](https://img.shields.io/badge/ICLR-2022-green)](https://openreview.net/forum?id=6kCiVaoQdx9)<a href="https://scholar.google.com.hk/scholar?q=Few-shot+Learning+via+Dirichlet+Tessellation+Ensemble"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-shot Learning via Dirichlet Tessellation Ensemble**](https://openreview.net/forum?id=6kCiVaoQdx9) , <br> by *Chunwei Ma, Ziyun Huang, Mingchen Gao and Jinhui Xu* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L637-L643) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ma2022fewshot```
- [![](https://img.shields.io/badge/ICLR-2022-green)](https://openreview.net/forum?id=49h_IkpJtaE)<a href="https://scholar.google.com.hk/scholar?q=How+to+Train+Your+MAML+to+Excel+in+Few-Shot+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**How to Train Your MAML to Excel in Few-Shot Classification**](https://openreview.net/forum?id=49h_IkpJtaE) , <br> by *Han-Jia Ye and Wei-Lun Chao* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L646-L652) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ye2022how```
- [![](https://img.shields.io/badge/ICLR-2021-green)](https://openreview.net/forum?id=JWOiYxMG92s)<a href="https://scholar.google.com.hk/scholar?q=Free+Lunch+for+Few-shot+Learning:+Distribution+Calibration"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Free Lunch for Few-shot Learning: Distribution Calibration**](https://openreview.net/forum?id=JWOiYxMG92s) , <br> by *Shuo Yang, Lu Liu and Min Xu* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1890-L1896) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```yang2021free```
- [![](https://img.shields.io/badge/ICLR-2021-green)](https://openreview.net/forum?id=O3Y56aqpChA)<a href="https://scholar.google.com.hk/scholar?q=Self-training+For+Few-shot+Transfer+Across+Extreme+Task+Differences"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Self-training For Few-shot Transfer Across Extreme Task Differences**](https://openreview.net/forum?id=O3Y56aqpChA) , <br> by *Cheng Perng Phoo and Bharath Hariharan* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1899-L1905) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```phoo2021selftraining```
- [![](https://img.shields.io/badge/ICLR-2021-green)](https://openreview.net/forum?id=oZIvHV04XgC)<a href="https://scholar.google.com.hk/scholar?q=Wandering+within+a+world:+Online+contextualized+few-shot+learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Wandering within a world: Online contextualized few-shot learning**](https://openreview.net/forum?id=oZIvHV04XgC) , <br> by *Mengye Ren, Michael Louis Iuzzolino, Michael Curtis Mozer and Richard Zemel* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1908-L1914) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ren2021wandering```
- [![](https://img.shields.io/badge/ICLR-2021-green)](https://openreview.net/forum?id=pW2Q2xLwIMD)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Learning+via+Learning+the+Representation,+Provably"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Learning via Learning the Representation, Provably**](https://openreview.net/forum?id=pW2Q2xLwIMD) , <br> by *Simon Shaolei Du, Wei Hu, Sham M. Kakade, Jason D. Lee and Qi Lei* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1917-L1923) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```du2021fewshot```
- [![](https://img.shields.io/badge/ICLR-2021-green)](https://openreview.net/forum?id=04cII6MumYV)<a href="https://scholar.google.com.hk/scholar?q=A+Universal+Representation+Transformer+Layer+for+Few-Shot+Image+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**A Universal Representation Transformer Layer for Few-Shot Image Classification**](https://openreview.net/forum?id=04cII6MumYV) , <br> by *Lu Liu, William L. Hamilton, Guodong Long, Jing Jiang and Hugo Larochelle* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1926-L1932) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```liu2021a```
- [![](https://img.shields.io/badge/ICLR-2021-green)](https://openreview.net/forum?id=cO1IH43yUF)<a href="https://scholar.google.com.hk/scholar?q=Revisiting+Few-sample+\BERT\+Fine-tuning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Revisiting Few-sample \BERT\ Fine-tuning**](https://openreview.net/forum?id=cO1IH43yUF) , <br> by *Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q Weinberger and Yoav Artzi* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1935-L1941) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhang2021revisiting```
- [![](https://img.shields.io/badge/ICLR-2021-green)](https://openreview.net/forum?id=eJIJF3-LoZO)<a href="https://scholar.google.com.hk/scholar?q=Concept+Learners+for+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Concept Learners for Few-Shot Learning**](https://openreview.net/forum?id=eJIJF3-LoZO) , <br> by *Kaidi Cao, Maria Brbic and Jure Leskovec* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1944-L1950) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```cao2021concept```
- [![](https://img.shields.io/badge/ICLR-2021-green)](https://openreview.net/forum?id=de11dbHzAMF)<a href="https://scholar.google.com.hk/scholar?q=Conditionally+Adaptive+Multi-Task+Learning:+Improving+Transfer+Learning+in+\NLP\+Using+Fewer+Parameters+\&+Less+Data"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Conditionally Adaptive Multi-Task Learning: Improving Transfer Learning in \NLP\ Using Fewer Parameters \& Less Data**](https://openreview.net/forum?id=de11dbHzAMF) , <br> by *Jonathan Pilault, Amine El hattami and Christopher Pal* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1953-L1959) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pilault2021conditionally```
- [![](https://img.shields.io/badge/ICLR-2021-green)](https://openreview.net/forum?id=3SV-ZePhnZM)<a href="https://scholar.google.com.hk/scholar?q=Incremental+few-shot+learning+via+vector+quantization+in+deep+embedded+space"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Incremental few-shot learning via vector quantization in deep embedded space**](https://openreview.net/forum?id=3SV-ZePhnZM) , <br> by *Kuilin Chen and Chi-Guhn Lee* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1962-L1968) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```chen2021incremental```
- [![](https://img.shields.io/badge/ICLR-2021-green)](https://openreview.net/forum?id=qkLMTphG5-h)<a href="https://scholar.google.com.hk/scholar?q=Repurposing+Pretrained+Models+for+Robust+Out-of-domain+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Repurposing Pretrained Models for Robust Out-of-domain Few-Shot Learning**](https://openreview.net/forum?id=qkLMTphG5-h) , <br> by *Namyeong Kwon, Hwidong Na, Gabriel Huang and Simon Lacoste-Julien* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1971-L1977) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```kwon2021repurposing```
- [![](https://img.shields.io/badge/ICLR-2021-green)](https://openreview.net/forum?id=D3PcGLdMx0)<a href="https://scholar.google.com.hk/scholar?q=\MELR\:+Meta-Learning+via+Modeling+Episode-Level+Relationships+for+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**\MELR\: Meta-Learning via Modeling Episode-Level Relationships for Few-Shot Learning**](https://openreview.net/forum?id=D3PcGLdMx0) , <br> by *Nanyi Fei, Zhiwu Lu, Tao Xiang and Songfang Huang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1980-L1986) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```fei2021melr```
- [![](https://img.shields.io/badge/ICLR-2021-green)](https://openreview.net/forum?id=-Lr-u0b42he)<a href="https://scholar.google.com.hk/scholar?q=Disentangling+3D+Prototypical+Networks+for+Few-Shot+Concept+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Disentangling 3D Prototypical Networks for Few-Shot Concept Learning**](https://openreview.net/forum?id=-Lr-u0b42he) , <br> by *Mihir Prabhudesai, Shamit Lal, Darshan Patil, Hsiao-Yu Tung, Adam W Harley and Katerina Fragkiadaki* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1989-L1995) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```prabhudesai2021disentangling```
- [![](https://img.shields.io/badge/ICLR-2021-green)](https://openreview.net/forum?id=vujTf_I8Kmc)<a href="https://scholar.google.com.hk/scholar?q=Attentional+Constellation+Nets+for+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Attentional Constellation Nets for Few-Shot Learning**](https://openreview.net/forum?id=vujTf_I8Kmc) , <br> by *Weijian Xu, yifan xu, Huaijin Wang and Zhuowen Tu* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1998-L2004) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```xu2021attentional```
- [![](https://img.shields.io/badge/ICLR-2021-green)](https://openreview.net/forum?id=umIdUL8rMH)<a href="https://scholar.google.com.hk/scholar?q=\BOIL\:+Towards+Representation+Change+for+Few-shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**\BOIL\: Towards Representation Change for Few-shot Learning**](https://openreview.net/forum?id=umIdUL8rMH) , <br> by *Jaehoon Oh, Hyungjun Yoo, ChangHwan Kim and Se-Young Yun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2007-L2013) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```oh2021boil```
- [![](https://img.shields.io/badge/ICLR-2021-green)](https://openreview.net/forum?id=SZ3wtsXfzQR)<a href="https://scholar.google.com.hk/scholar?q=Theoretical+bounds+on+estimation+error+for+meta-learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Theoretical bounds on estimation error for meta-learning**](https://openreview.net/forum?id=SZ3wtsXfzQR) , <br> by *James Lucas, Mengye Ren, Irene Raissa KAMENI KAMENI, Toniann Pitassi and Richard Zemel* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2016-L2022) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```lucas2021theoretical```
- [![](https://img.shields.io/badge/ICLR-2021-green)](https://openreview.net/forum?id=--gvHfE3Xf5)<a href="https://scholar.google.com.hk/scholar?q=Meta-Learning+of+Structured+Task+Distributions+in+Humans+and+Machines"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta-Learning of Structured Task Distributions in Humans and Machines**](https://openreview.net/forum?id=--gvHfE3Xf5) , <br> by *Sreejan Kumar, Ishita Dasgupta, Jonathan Cohen, Nathaniel Daw and Thomas Griffiths* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2025-L2031) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```kumar2021metalearning```
- [![](https://img.shields.io/badge/ICLR-2020-green)](https://openreview.net/forum?id=rklp93EtwH)<a href="https://scholar.google.com.hk/scholar?q=Automated+Relational+Meta-learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Automated Relational Meta-learning**](https://openreview.net/forum?id=rklp93EtwH) , <br> by *Huaxiu Yao, Xian Wu, Zhiqiang Tao, Yaliang Li, Bolin Ding, Ruirui Li and Zhenhui Li* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2222-L2228) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Yao2020Automated```
- [![](https://img.shields.io/badge/ICLR-2020-green)](https://openreview.net/forum?id=rkgAGAVKPr)<a href="https://scholar.google.com.hk/scholar?q=Meta-Dataset:+A+Dataset+of+Datasets+for+Learning+to+Learn+from+Few+Examples"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples**](https://openreview.net/forum?id=rkgAGAVKPr) , <br> by *Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol and Hugo Larochelle* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2241-L2247) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Triantafillou2020Meta-Dataset```
- [![](https://img.shields.io/badge/ICLR-2020-green)](https://openreview.net/forum?id=r1eowANFvr)<a href="https://scholar.google.com.hk/scholar?q=Towards+Fast+Adaptation+of+Neural+Architectures+with+Meta+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Towards Fast Adaptation of Neural Architectures with Meta Learning**](https://openreview.net/forum?id=r1eowANFvr) , <br> by *Dongze Lian, Yin Zheng, Yintao Xu, Yanxiong Lu, Leyu Lin, Peilin Zhao, Junzhou Huang and Shenghua Gao* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2251-L2257) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Lian2020Towards```
- [![](https://img.shields.io/badge/ICLR-2020-green)](https://openreview.net/forum?id=Bkxv90EKPB)<a href="https://scholar.google.com.hk/scholar?q=Bayesian+Meta+Sampling+for+Fast+Uncertainty+Adaptation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Bayesian Meta Sampling for Fast Uncertainty Adaptation**](https://openreview.net/forum?id=Bkxv90EKPB) , <br> by *Zhenyi Wang, Yang Zhao, Ping Yu, Ruiyi Zhang and Changyou Chen* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2261-L2267) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Wang2020Bayesian```
- [![](https://img.shields.io/badge/ICLR-2020-green)](https://openreview.net/forum?id=BklEFpEYwS)<a href="https://scholar.google.com.hk/scholar?q=Meta-Learning+without+Memorization"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta-Learning without Memorization**](https://openreview.net/forum?id=BklEFpEYwS) , <br> by *Mingzhang Yin, George Tucker, Mingyuan Zhou, Sergey Levine and Chelsea Finn* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2271-L2277) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Yin2020Meta-Learning```
- [![](https://img.shields.io/badge/ICLR-2020-green)](https://openreview.net/forum?id=ryeYpJSKwr)<a href="https://scholar.google.com.hk/scholar?q=Meta-Learning+Acquisition+Functions+for+Transfer+Learning+in+Bayesian+Optimization"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta-Learning Acquisition Functions for Transfer Learning in Bayesian Optimization**](https://openreview.net/forum?id=ryeYpJSKwr) , <br> by *Michael Volpp, Lukas P. Fröhlich, Kirsten Fischer, Andreas Doerr, Stefan Falkner, Frank Hutter and Christian Daniel* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2280-L2286) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Volpp2020Meta-Learning```
- [![](https://img.shields.io/badge/ICLR-2020-green)](https://openreview.net/forum?id=rkeZIJBYvr)<a href="https://scholar.google.com.hk/scholar?q=Learning+to+Balance:+Bayesian+Meta-Learning+for+Imbalanced+and+Out-of-distribution+Tasks"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning to Balance: Bayesian Meta-Learning for Imbalanced and Out-of-distribution Tasks**](https://openreview.net/forum?id=rkeZIJBYvr) , <br> by *Hae Beom Lee, Hayeon Lee, Donghyun Na, Saehoon Kim, Minseop Park, Eunho Yang and Sung Ju Hwang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2289-L2295) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Lee2020Learning```
- [![](https://img.shields.io/badge/ICLR-2020-green)](https://openreview.net/forum?id=H1emfT4twB)<a href="https://scholar.google.com.hk/scholar?q=Few-shot+Text+Classification+with+Distributional+Signatures"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-shot Text Classification with Distributional Signatures**](https://openreview.net/forum?id=H1emfT4twB) , <br> by *Yujia Bao, Menghua Wu, Shiyu Chang and Regina Barzilay* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2299-L2305) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Bao2020Few-shot```
- [![](https://img.shields.io/badge/ICLR-2020-green)](https://openreview.net/forum?id=SygagpEKwB)<a href="https://scholar.google.com.hk/scholar?q=Disentangling+Factors+of+Variations+Using+Few+Labels"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Disentangling Factors of Variations Using Few Labels**](https://openreview.net/forum?id=SygagpEKwB) , <br> by *Francesco Locatello, Michael Tschannen, Stefan Bauer, Gunnar Rätsch, Bernhard Schölkopf and Olivier Bachem* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2308-L2314) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Locatello2020Disentangling```
- [![](https://img.shields.io/badge/ICLR-2020-green)](https://openreview.net/forum?id=Bkeeca4Kvr)<a href="https://scholar.google.com.hk/scholar?q=FEW-SHOT+LEARNING+ON+GRAPHS+VIA+SUPER-CLASSES+BASED+ON+GRAPH+SPECTRAL+MEASURES"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**FEW-SHOT LEARNING ON GRAPHS VIA SUPER-CLASSES BASED ON GRAPH SPECTRAL MEASURES**](https://openreview.net/forum?id=Bkeeca4Kvr) , <br> by *Jatin Chauhan, Deepak Nathani and Manohar Kaul* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2317-L2323) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Chauhan2020FEW-SHOT```
- [![](https://img.shields.io/badge/ICLR-2020-green)](https://openreview.net/forum?id=HkgB2TNYPS)<a href="https://scholar.google.com.hk/scholar?q=A+Theoretical+Analysis+of+the+Number+of+Shots+in+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**A Theoretical Analysis of the Number of Shots in Few-Shot Learning**](https://openreview.net/forum?id=HkgB2TNYPS) , <br> by *Tianshi Cao, Marc T Law and Sanja Fidler* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2326-L2332) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Cao2020A```
- [![](https://img.shields.io/badge/ICLR-2020-green)](https://openreview.net/forum?id=rylXBkrYDS)<a href="https://scholar.google.com.hk/scholar?q=A+Baseline+for+Few-Shot+Image+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**A Baseline for Few-Shot Image Classification**](https://openreview.net/forum?id=rylXBkrYDS) , <br> by *Guneet Singh Dhillon, Pratik Chaudhari, Avinash Ravichandran and Stefano Soatto* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2335-L2341) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Dhillon2020A```
- [![](https://img.shields.io/badge/ICLR-2020-green)](https://openreview.net/forum?id=SJl5Np4tPr)<a href="https://scholar.google.com.hk/scholar?q=Cross-Domain+Few-Shot+Classification+via+Learned+Feature-Wise+Transformation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Cross-Domain Few-Shot Classification via Learned Feature-Wise Transformation**](https://openreview.net/forum?id=SJl5Np4tPr) , <br> by *Hung-Yu Tseng, Hsin-Ying Lee, Jia-Bin Huang and Ming-Hsuan Yang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2344-L2350) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Tseng2020Cross-Domain```
- [![](https://img.shields.io/badge/ICLR-2019-green)](https://openreview.net/forum?id=ByeSdsC9Km)<a href="https://scholar.google.com.hk/scholar?q=Adaptive+Posterior+Learning:+few-shot+learning+with+a+surprise-based+memory+module"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Adaptive Posterior Learning: few-shot learning with a surprise-based memory module**](https://openreview.net/forum?id=ByeSdsC9Km) , <br> by *Tiago Ramalho and Marta Garnelo* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2353-L2359) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ramalho2018adaptive```
- [![](https://img.shields.io/badge/ICLR-2019-green)](https://openreview.net/forum?id=HkxLXnAcFQ)<a href="https://scholar.google.com.hk/scholar?q=A+Closer+Look+at+Few-shot+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**A Closer Look at Few-shot Classification**](https://openreview.net/forum?id=HkxLXnAcFQ) , <br> by *Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang and Jia-Bin Huang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2362-L2368) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```chen2018a```
- [![](https://img.shields.io/badge/ICLR-2019-green)](https://openreview.net/forum?id=SyVuRiC5K7)<a href="https://scholar.google.com.hk/scholar?q=LEARNING+TO+PROPAGATE+LABELS:+TRANSDUCTIVE+PROPAGATION+NETWORK+FOR+FEW-SHOT+LEARNING"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING**](https://openreview.net/forum?id=SyVuRiC5K7) , <br> by *Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, Eunho Yang, Sungju Hwang and Yi Yang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2371-L2377) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```liu2018learning```
- [![](https://img.shields.io/badge/ICLR-2019-green)](https://openreview.net/forum?id=HygBZnRctX)<a href="https://scholar.google.com.hk/scholar?q=Transferring+Knowledge+across+Learning+Processes"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Transferring Knowledge across Learning Processes**](https://openreview.net/forum?id=HygBZnRctX) , <br> by *Sebastian Flennerhag, Pablo Garcia Moreno, Neil Lawrence and Andreas Damianou* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2380-L2386) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```flennerhag2018transferring```
- [![](https://img.shields.io/badge/ICLR-2019-green)](https://openreview.net/forum?id=HkxStoC5F7)<a href="https://scholar.google.com.hk/scholar?q=Meta-Learning+Probabilistic+Inference+for+Prediction"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta-Learning Probabilistic Inference for Prediction**](https://openreview.net/forum?id=HkxStoC5F7) , <br> by *Jonathan Gordon, John Bronskill, Matthias Bauer, Sebastian Nowozin and Richard Turner* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2389-L2395) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```gordon2018metalearning```
- [![](https://img.shields.io/badge/ICLR-2019-green)](https://openreview.net/forum?id=BJfOXnActQ)<a href="https://scholar.google.com.hk/scholar?q=Learning+to+Learn+with+Conditional+Class+Dependencies"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning to Learn with Conditional Class Dependencies**](https://openreview.net/forum?id=BJfOXnActQ) , <br> by *Xiang Jiang, Mohammad Havaei, Farshid Varno, Gabriel Chartrand, Nicolas Chapados and Stan Matwin* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2398-L2404) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```jiang2018learning```
- [![](https://img.shields.io/badge/ICLR-2018-green)](https://openreview.net/forum?id=r1wEFyWCW)<a href="https://scholar.google.com.hk/scholar?q=Few-shot+Autoregressive+Density+Estimation:+Towards+Learning+to+Learn+Distributions"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions**](https://openreview.net/forum?id=r1wEFyWCW) , <br> by *Scott Reed, Yutian Chen, Thomas Paine, Aäron van den Oord, S. M. Ali Eslami, Danilo Rezende, Oriol Vinyals and Nando de Freitas* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2407-L2413) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```reed2018fewshot```
- [![](https://img.shields.io/badge/ICLR-2018-green)](https://openreview.net/forum?id=BJj6qGbRW)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Learning+with+Graph+Neural+Networks"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Learning with Graph Neural Networks**](https://openreview.net/forum?id=BJj6qGbRW) , <br> by *Victor Garcia Satorras and Joan Bruna Estrach* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2416-L2422) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```garcia2018fewshot```
- [![](https://img.shields.io/badge/ICLR-2018-green)](https://openreview.net/forum?id=HJcSzz-CZ)<a href="https://scholar.google.com.hk/scholar?q=Meta-Learning+for+Semi-Supervised+Few-Shot+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta-Learning for Semi-Supervised Few-Shot Classification**](https://openreview.net/forum?id=HJcSzz-CZ) , <br> by *Mengye Ren, Sachin Ravi, Eleni Triantafillou, Jake Snell, Kevin Swersky, Josh B. Tenenbaum, Hugo Larochelle and Richard S. Zemel* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2425-L2431) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ren2018metalearning```
- [![](https://img.shields.io/badge/ICLR-2018-green)](https://openreview.net/forum?id=HyjC5yWCW)<a href="https://scholar.google.com.hk/scholar?q=Meta-Learning+and+Universality:+Deep+Representations+and+Gradient+Descent+can+Approximate+any+Learning+Algorithm"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm**](https://openreview.net/forum?id=HyjC5yWCW) , <br> by *Chelsea Finn and Sergey Levine* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2434-L2440) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```finn2018metalearning```
- [![](https://img.shields.io/badge/ICLR-2018-green)](https://openreview.net/forum?id=SyX0IeWAW)<a href="https://scholar.google.com.hk/scholar?q=META+LEARNING+SHARED+HIERARCHIES"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**META LEARNING SHARED HIERARCHIES**](https://openreview.net/forum?id=SyX0IeWAW) , <br> by *Kevin Frans, Jonathan Ho, Xi Chen, Pieter Abbeel and John Schulman* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2443-L2449) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```frans2018meta```
- [![](https://img.shields.io/badge/ICLR-2017-green)](https://openreview.net/forum?id=rJY0-Kcll)<a href="https://scholar.google.com.hk/scholar?q=Optimization+as+a+Model+for+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Optimization as a Model for Few-Shot Learning**](https://openreview.net/forum?id=rJY0-Kcll) , <br> by *Sachin Ravi and
Hugo Larochelle* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2451-L2458) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```RaviL17```
## NeurIPS

- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/4d7a968bb636e25818ff2a3941db08c1-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Realistic+evaluation+of+transductive+few-shot+learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Realistic evaluation of transductive few-shot learning**](https://proceedings.neurips.cc/paper/2021/hash/4d7a968bb636e25818ff2a3941db08c1-Abstract.html) , <br> by *Olivier Veilleux and
Malik Boudiaf and
Pablo Piantanida and
Ismail Ben Ayed* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L654-L664) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```VeilleuxBPA21```
- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/d9fc0cdb67638d50f411432d0d41d0ba-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Re-ranking+for+image+retrieval+and+transductive+few-shot+classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Re-ranking for image retrieval and transductive few-shot classification**](https://proceedings.neurips.cc/paper/2021/hash/d9fc0cdb67638d50f411432d0d41d0ba-Abstract.html) , <br> by *Xi Shen and
Yang Xiao and
Shell Xu Hu and
Othman Sbai and
Mathieu Aubry* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L666-L677) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ShenXHSA21```
- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/5c04925674920eb58467fb52ce4ef728-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=True+Few-Shot+Learning+with+Language+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**True Few-Shot Learning with Language Models**](https://proceedings.neurips.cc/paper/2021/hash/5c04925674920eb58467fb52ce4ef728-Abstract.html) , <br> by *Ethan Perez and
Douwe Kiela and
Kyunghyun Cho* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L681-L690) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```PerezKC21```
- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/33a854e247155d590883b93bca53848a-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Grad2Task:+Improved+Few-shot+Text+Classification+Using+Gradients+for+Task+Representation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Grad2Task: Improved Few-shot Text Classification Using Gradients for
Task Representation**](https://proceedings.neurips.cc/paper/2021/hash/33a854e247155d590883b93bca53848a-Abstract.html) , <br> by *Jixuan Wang and
Kuan{-}Chieh Wang and
Frank Rudzicz and
Michael Brudno* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L692-L703) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```WangWRB21```
- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/682e0e796084e163c5ca053dd8573b0c-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=D2C:+Diffusion-Decoding+Models+for+Few-Shot+Conditional+Generation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**D2C: Diffusion-Decoding Models for Few-Shot Conditional Generation**](https://proceedings.neurips.cc/paper/2021/hash/682e0e796084e163c5ca053dd8573b0c-Abstract.html) , <br> by *Abhishek Sinha and
Jiaming Song and
Chenlin Meng and
Stefano Ermon* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L705-L715) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```SinhaSME21```
- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/af5d5ef24881f3c3049a7b9bfe74d58b-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=TOHAN:+A+One-step+Approach+towards+Few-shot+Hypothesis+Adaptation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**TOHAN: A One-step Approach towards Few-shot Hypothesis Adaptation**](https://proceedings.neurips.cc/paper/2021/hash/af5d5ef24881f3c3049a7b9bfe74d58b-Abstract.html) , <br> by *Haoang Chi and
Feng Liu and
Wenjing Yang and
Long Lan and
Tongliang Liu and
Bo Han and
William K. Cheung and
James T. Kwok* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L717-L731) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ChiLYLLHCK21```
- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/e3b6fb0fd4df098162eede3313c54a8d-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=The+Role+of+Global+Labels+in+Few-Shot+Classification+and+How+to+Infer+Them"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**The Role of Global Labels in Few-Shot Classification and How to Infer
Them**](https://proceedings.neurips.cc/paper/2021/hash/e3b6fb0fd4df098162eede3313c54a8d-Abstract.html) , <br> by *Ruohan Wang and
Massimiliano Pontil and
Carlo Ciliberto* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L733-L743) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```WangPC21```
- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/1d6408264d31d453d556c60fe7d0459e-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Dynamic+Distillation+Network+for+Cross-Domain+Few-Shot+Recognition+with+Unlabeled+Data"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Dynamic Distillation Network for Cross-Domain Few-Shot Recognition
with Unlabeled Data**](https://proceedings.neurips.cc/paper/2021/hash/1d6408264d31d453d556c60fe7d0459e-Abstract.html) , <br> by *Ashraful Islam and
Chun{-}Fu (Richard) Chen and
Rameswar Panda and
Leonid Karlinsky and
Rog{\'{e}}rio Feris and
Richard J. Radke* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L745-L758) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```IslamCPKFR21```
- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/6e2713a6efee97bacb63e52c54f0ada0-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Learning+to+Learn+Dense+Gaussian+Processes+for+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning to Learn Dense Gaussian Processes for Few-Shot Learning**](https://proceedings.neurips.cc/paper/2021/hash/6e2713a6efee97bacb63e52c54f0ada0-Abstract.html) , <br> by *Ze Wang and
Zichen Miao and
Xiantong Zhen and
Qiang Qiu* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L760-L771) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```WangMZQ21```
- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/6cfe0e6127fa25df2a0ef2ae1067d915-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Rectifying+the+Shortcut+Learning+of+Background+for+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Rectifying the Shortcut Learning of Background for Few-Shot Learning**](https://proceedings.neurips.cc/paper/2021/hash/6cfe0e6127fa25df2a0ef2ae1067d915-Abstract.html) , <br> by *Xu Luo and
Longhui Wei and
Liangjian Wen and
Jinrong Yang and
Lingxi Xie and
Zenglin Xu and
Qi Tian* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L773-L786) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```LuoWWYXXT21```
- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/8493eeaccb772c0878f99d60a0bd2bb3-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=FLEX:+Unifying+Evaluation+for+Few-Shot+NLP"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**FLEX: Unifying Evaluation for Few-Shot NLP**](https://proceedings.neurips.cc/paper/2021/hash/8493eeaccb772c0878f99d60a0bd2bb3-Abstract.html) , <br> by *Jonathan Bragg and
Arman Cohan and
Kyle Lo and
Iz Beltagy* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L788-L798) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```BraggCLB21```
- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/01b7575c38dac42f3cfb7d500438b875-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Multimodal+Few-Shot+Learning+with+Frozen+Language+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Multimodal Few-Shot Learning with Frozen Language Models**](https://proceedings.neurips.cc/paper/2021/hash/01b7575c38dac42f3cfb7d500438b875-Abstract.html) , <br> by *Maria Tsimpoukelli and
Jacob Menick and
Serkan Cabi and
S. M. Ali Eslami and
Oriol Vinyals and
Felix Hill* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L800-L812) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```TsimpoukelliMCE21```
- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/cdfa4c42f465a5a66871587c69fcfa34-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=On+Episodes,+Prototypical+Networks,+and+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**On Episodes, Prototypical Networks, and Few-Shot Learning**](https://proceedings.neurips.cc/paper/2021/hash/cdfa4c42f465a5a66871587c69fcfa34-Abstract.html) , <br> by *Steinar Laenen and
Luca Bertinetto* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L814-L822) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```LaenenB21```
- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/c91591a8d461c2869b9f535ded3e213e-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=POODLE:+Improving+Few-shot+Learning+via+Penalizing+Out-of-Distribution+Samples"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**POODLE: Improving Few-shot Learning via Penalizing Out-of-Distribution
Samples**](https://proceedings.neurips.cc/paper/2021/hash/c91591a8d461c2869b9f535ded3e213e-Abstract.html) , <br> by *Duong H. Le and
Khoi Duc Nguyen and
Khoi Nguyen and
Quoc{-}Huy Tran and
Rang Nguyen and
Binh{-}Son Hua* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L824-L837) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```LeNNTNH21```
- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/357cfba15668cc2e1e73111e09d54383-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Overcoming+Catastrophic+Forgetting+in+Incremental+Few-Shot+Learning+by+Finding+Flat+Minima"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Overcoming Catastrophic Forgetting in Incremental Few-Shot Learning
by Finding Flat Minima**](https://proceedings.neurips.cc/paper/2021/hash/357cfba15668cc2e1e73111e09d54383-Abstract.html) , <br> by *Guangyuan Shi and
Jiaxin Chen and
Wenlong Zhang and
Li{-}Ming Zhan and
Xiao{-}Ming Wu* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L839-L851) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ShiCZZW21```
- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/3644a684f98ea8fe223c713b77189a77-Abstract-round2.html)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Learning+Evaluation+in+Natural+Language+Understanding"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Learning Evaluation in Natural Language Understanding**](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/3644a684f98ea8fe223c713b77189a77-Abstract-round2.html) , <br> by *Subhabrata Mukherjee and
Xiaodong Liu and
Guoqing Zheng and
Saghar Hosseini and
Saghar Hosseini and
Hao Cheng and
Ge Yang and
Christopher Meek and
Ahmed Hassan Awadallah and
Jianfeng Gao* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1016-L1031) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```MukherjeeLZHH0Y21```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/0663a4ddceacb40b095eda264a85f15c-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Learning+to+Extrapolate+Knowledge:+Transductive+Few-shot+Out-of-Graph+Link+Prediction"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning to Extrapolate Knowledge: Transductive Few-shot Out-of-Graph
Link Prediction**](https://proceedings.neurips.cc/paper/2020/hash/0663a4ddceacb40b095eda264a85f15c-Abstract.html) , <br> by *Jinheon Baek and
Dong Bok Lee and
Sung Ju Hwang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2460-L2469) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```BaekLH20```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/196f5641aa9dc87067da4ff90fd81e7b-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Information+Maximization+for+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Information Maximization for Few-Shot Learning**](https://proceedings.neurips.cc/paper/2020/hash/196f5641aa9dc87067da4ff90fd81e7b-Abstract.html) , <br> by *Malik Boudiaf and
Imtiaz Masud Ziko and
J{\'{e}}r{\^{o}}me Rony and
Jose Dolz and
Pablo Piantanida and
Ismail Ben Ayed* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2471-L2482) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```BoudiafZRDPA20```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/1cc8a8ea51cd0adddf5dab504a285915-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Interventional+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Interventional Few-Shot Learning**](https://proceedings.neurips.cc/paper/2020/hash/1cc8a8ea51cd0adddf5dab504a285915-Abstract.html) , <br> by *Zhongqi Yue and
Hanwang Zhang and
Qianru Sun and
Xian{-}Sheng Hua* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2484-L2493) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```YueZS020```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/240ac9371ec2671ae99847c3ae2e6384-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Restoring+Negative+Information+in+Few-Shot+Object+Detection"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Restoring Negative Information in Few-Shot Object Detection**](https://proceedings.neurips.cc/paper/2020/hash/240ac9371ec2671ae99847c3ae2e6384-Abstract.html) , <br> by *Yukuan Yang and
Fangyun Wei and
Miaojing Shi and
Guoqi Li* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2495-L2504) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```YangWSL20```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/28e209b61a52482a0ae1cb9f5959c792-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=OOD-MAML:+Meta-Learning+for+Few-Shot+Out-of-Distribution+Detection+and+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**OOD-MAML: Meta-Learning for Few-Shot Out-of-Distribution Detection
and Classification**](https://proceedings.neurips.cc/paper/2020/hash/28e209b61a52482a0ae1cb9f5959c792-Abstract.html) , <br> by *Taewon Jeong and
Heeyoung Kim* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2506-L2514) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```JeongK20```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/b6d767d2f8ed5d21a44b0e5886680cb9-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Few-shot+Image+Generation+with+Elastic+Weight+Consolidation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-shot Image Generation with Elastic Weight Consolidation**](https://proceedings.neurips.cc/paper/2020/hash/b6d767d2f8ed5d21a44b0e5886680cb9-Abstract.html) , <br> by *Yijun Li and
Richard Zhang and
Jingwan Lu and
Eli Shechtman* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2517-L2526) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Li0LS20```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/c055dcc749c2632fd4dd806301f05ba6-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Node+Classification+on+Graphs+with+Few-Shot+Novel+Labels+via+Meta+Transformed+Network+Embedding"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Node Classification on Graphs with Few-Shot Novel Labels via Meta
Transformed Network Embedding**](https://proceedings.neurips.cc/paper/2020/hash/c055dcc749c2632fd4dd806301f05ba6-Abstract.html) , <br> by *Lin Lan and
Pinghui Wang and
Xuefeng Du and
Kaikai Song and
Jing Tao and
Xiaohong Guan* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2529-L2541) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```LanWDSTG20```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/c39e1a03859f9ee215bc49131d0caf33-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Few-shot+Visual+Reasoning+with+Meta-Analogical+Contrastive+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-shot Visual Reasoning with Meta-Analogical Contrastive Learning**](https://proceedings.neurips.cc/paper/2020/hash/c39e1a03859f9ee215bc49131d0caf33-Abstract.html) , <br> by *Youngsung Kim and
Jinwoo Shin and
Eunho Yang and
Sung Ju Hwang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2543-L2552) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```KimSYH20```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/cfee398643cbc3dc5eefc89334cacdc1-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Adversarially+Robust+Few-Shot+Learning:+A+Meta-Learning+Approach"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Adversarially Robust Few-Shot Learning: A Meta-Learning Approach**](https://proceedings.neurips.cc/paper/2020/hash/cfee398643cbc3dc5eefc89334cacdc1-Abstract.html) , <br> by *Micah Goldblum and
Liam Fowl and
Tom Goldstein* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2554-L2562) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```GoldblumFG20```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/f23d125da1e29e34c552f448610ff25f-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Uncertainty-aware+Self-training+for+Few-shot+Text+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Uncertainty-aware Self-training for Few-shot Text Classification**](https://proceedings.neurips.cc/paper/2020/hash/f23d125da1e29e34c552f448610ff25f-Abstract.html) , <br> by *Subhabrata Mukherjee and
Ahmed Hassan Awadallah* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2564-L2571) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```MukherjeeA20```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/0415740eaa4d9decbc8da001d3fd805f-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=A+Closer+Look+at+the+Training+Strategy+for+Modern+Meta-Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**A Closer Look at the Training Strategy for Modern Meta-Learning**](https://proceedings.neurips.cc/paper/2020/hash/0415740eaa4d9decbc8da001d3fd805f-Abstract.html) , <br> by *Jiaxin Chen and
Xiao{-}Ming Wu and
Yanke Li and
Qimai Li and
Li{-}Ming Zhan and
Fu{-}Lai Chung* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2574-L2585) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ChenWLLZC20```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/0a716fe8c7745e51a3185fc8be6ca23a-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=The+Advantage+of+Conditional+Meta-Learning+for+Biased+Regularization+and+Fine+Tuning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**The Advantage of Conditional Meta-Learning for Biased Regularization
and Fine Tuning**](https://proceedings.neurips.cc/paper/2020/hash/0a716fe8c7745e51a3185fc8be6ca23a-Abstract.html) , <br> by *Giulia Denevi and
Massimiliano Pontil and
Carlo Ciliberto* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2587-L2596) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```DeneviPC20```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/1b69ebedb522700034547abc5652ffac-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Structured+Prediction+for+Conditional+Meta-Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Structured Prediction for Conditional Meta-Learning**](https://proceedings.neurips.cc/paper/2020/hash/1b69ebedb522700034547abc5652ffac-Abstract.html) , <br> by *Ruohan Wang and
Yiannis Demiris and
Carlo Ciliberto* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2598-L2606) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```WangDC20```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/2ba61cc3a8f44143e1f2f13b2b729ab3-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Balanced+Meta-Softmax+for+Long-Tailed+Visual+Recognition"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Balanced Meta-Softmax for Long-Tailed Visual Recognition**](https://proceedings.neurips.cc/paper/2020/hash/2ba61cc3a8f44143e1f2f13b2b729ab3-Abstract.html) , <br> by *Jiawei Ren and
Cunjun Yu and
Shunan Sheng and
Xiao Ma and
Haiyu Zhao and
Shuai Yi and
Hongsheng Li* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2609-L2621) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```RenYSMZYL20```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/3e5190eeb51ebe6c5bbc54ee8950c548-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Meta-Learning+Requires+Meta-Augmentation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta-Learning Requires Meta-Augmentation**](https://proceedings.neurips.cc/paper/2020/hash/3e5190eeb51ebe6c5bbc54ee8950c548-Abstract.html) , <br> by *Janarthanan Rajendran and
Alexander Irpan and
Eric Jang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2623-L2631) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```RajendranIJ20```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/438124b4c06f3a5caffab2c07863b617-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Meta-learning+from+Tasks+with+Heterogeneous+Attribute+Spaces"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta-learning from Tasks with Heterogeneous Attribute Spaces**](https://proceedings.neurips.cc/paper/2020/hash/438124b4c06f3a5caffab2c07863b617-Abstract.html) , <br> by *Tomoharu Iwata and
Atsutoshi Kumagai* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2633-L2640) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```IwataK20```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/4b86ca48d90bd5f0978afa3a012503a4-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Online+Structured+Meta-learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Online Structured Meta-learning**](https://proceedings.neurips.cc/paper/2020/hash/4b86ca48d90bd5f0978afa3a012503a4-Abstract.html) , <br> by *Huaxiu Yao and
Yingbo Zhou and
Mehrdad Mahdavi and
Zhenhui Li and
Richard Socher and
Caiming Xiong* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2642-L2653) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```YaoZMLSX20```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/7fc63ff01769c4fa7d9279e97e307829-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Modeling+and+Optimization+Trade-off+in+Meta-learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Modeling and Optimization Trade-off in Meta-learning**](https://proceedings.neurips.cc/paper/2020/hash/7fc63ff01769c4fa7d9279e97e307829-Abstract.html) , <br> by *Katelyn Gao and
Ozan Sener* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2666-L2673) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```GaoS20```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/84c578f202616448a2f80e6f56d5f16d-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Convergence+of+Meta-Learning+with+Task-Specific+Adaptation+over+Partial+Parameters"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Convergence of Meta-Learning with Task-Specific Adaptation over Partial
Parameters**](https://proceedings.neurips.cc/paper/2020/hash/84c578f202616448a2f80e6f56d5f16d-Abstract.html) , <br> by *Kaiyi Ji and
Jason D. Lee and
Yingbin Liang and
H. Vincent Poor* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2675-L2685) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```JiLLP20```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/8989e07fc124e7a9bcbdebcc8ace2bc0-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=MATE:+Plugging+in+Model+Awareness+to+Task+Embedding+for+Meta+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**MATE: Plugging in Model Awareness to Task Embedding for Meta Learning**](https://proceedings.neurips.cc/paper/2020/hash/8989e07fc124e7a9bcbdebcc8ace2bc0-Abstract.html) , <br> by *Xiaohan Chen and
Zhangyang Wang and
Siyu Tang and
Krikamol Muandet* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2687-L2696) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ChenWTM20```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/cc3f5463bc4d26bc38eadc8bcffbc654-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Continuous+Meta-Learning+without+Tasks"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Continuous Meta-Learning without Tasks**](https://proceedings.neurips.cc/paper/2020/hash/cc3f5463bc4d26bc38eadc8bcffbc654-Abstract.html) , <br> by *James Harrison and
Apoorva Sharma and
Chelsea Finn and
Marco Pavone* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2698-L2707) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```HarrisonSFP20```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/da8ce53cf0240070ce6c69c48cd588ee-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Task-Robust+Model-Agnostic+Meta-Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Task-Robust Model-Agnostic Meta-Learning**](https://proceedings.neurips.cc/paper/2020/hash/da8ce53cf0240070ce6c69c48cd588ee-Abstract.html) , <br> by *Liam Collins and
Aryan Mokhtari and
Sanjay Shakkottai* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2709-L2717) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```CollinsMS20```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/ee89223a2b625b5152132ed77abbcc79-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Meta-Learning+with+Adaptive+Hyperparameters"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta-Learning with Adaptive Hyperparameters**](https://proceedings.neurips.cc/paper/2020/hash/ee89223a2b625b5152132ed77abbcc79-Abstract.html) , <br> by *Sungyong Baik and
Myungsub Choi and
Janghoon Choi and
Heewon Kim and
Kyoung Mu Lee* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2719-L2729) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```BaikCCKL20```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/ef0d17b3bdb4ee2aa741ba28c7255c53-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Probabilistic+Active+Meta-Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Probabilistic Active Meta-Learning**](https://proceedings.neurips.cc/paper/2020/hash/ef0d17b3bdb4ee2aa741ba28c7255c53-Abstract.html) , <br> by *Jean Kaddour and
Steind{\'{o}}r S{\ae}mundsson and
Marc Peter Deisenroth* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2731-L2739) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```KaddourSD20```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/67d16d00201083a2b118dd5128dd6f59-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Learning+to+Learn+Variational+Semantic+Memory"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning to Learn Variational Semantic Memory**](https://proceedings.neurips.cc/paper/2020/hash/67d16d00201083a2b118dd5128dd6f59-Abstract.html) , <br> by *Xiantong Zhen and
Ying{-}Jun Du and
Huan Xiong and
Qiang Qiu and
Cees Snoek and
Ling Shao* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2741-L2752) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ZhenDXQS020```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Language+Models+are+Few-Shot+Learners"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Language Models are Few-Shot Learners**](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html) , <br> by *Tom B. Brown and
Benjamin Mann and
Nick Ryder and
Melanie Subbiah and
Jared Kaplan and
Prafulla Dhariwal and
Arvind Neelakantan and
Pranav Shyam and
Girish Sastry and
Amanda Askell and
Sandhini Agarwal and
Ariel Herbert{-}Voss and
Gretchen Krueger and
Tom Henighan and
Rewon Child and
Aditya Ramesh and
Daniel M. Ziegler and
Jeffrey Wu and
Clemens Winter and
Christopher Hesse and
Mark Chen and
Eric Sigler and
Mateusz Litwin and
Scott Gray and
Benjamin Chess and
Jack Clark and
Christopher Berner and
Sam McCandlish and
Alec Radford and
Ilya Sutskever and
Dario Amodei* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2931-L2967) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```BrownMRSKDNSSAA20```
- [![](https://img.shields.io/badge/NeurIPS-2019-green)](https://proceedings.neurips.cc/paper/2019/hash/01894d6f048493d2cacde3c579c315a3-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Cross+Attention+Network+for+Few-shot+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Cross Attention Network for Few-shot Classification**](https://proceedings.neurips.cc/paper/2019/hash/01894d6f048493d2cacde3c579c315a3-Abstract.html) , <br> by *Ruibing Hou and
Hong Chang and
Bingpeng Ma and
Shiguang Shan and
Xilin Chen* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2754-L2765) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```HouCMSC19```
- [![](https://img.shields.io/badge/NeurIPS-2019-green)](https://proceedings.neurips.cc/paper/2019/hash/d790c9e6c0b5e02c87b375e782ac01bc-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Adaptive+Cross-Modal+Few-shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Adaptive Cross-Modal Few-shot Learning**](https://proceedings.neurips.cc/paper/2019/hash/d790c9e6c0b5e02c87b375e782ac01bc-Abstract.html) , <br> by *Chen Xing and
Negar Rostamzadeh and
Boris N. Oreshkin and
Pedro O. Pinheiro* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2767-L2777) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```XingROP19```
- [![](https://img.shields.io/badge/NeurIPS-2019-green)](https://proceedings.neurips.cc/paper/2019/hash/fd0a5a5e367a0955d81278062ef37429-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Unsupervised+Meta-Learning+for+Few-Shot+Image+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Unsupervised Meta-Learning for Few-Shot Image Classification**](https://proceedings.neurips.cc/paper/2019/hash/fd0a5a5e367a0955d81278062ef37429-Abstract.html) , <br> by *Siavash Khodadadeh and
Ladislau B{\"{o}}l{\"{o}}ni and
Mubarak Shah* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2779-L2788) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```KhodadadehBS19```
- [![](https://img.shields.io/badge/NeurIPS-2019-green)](https://proceedings.neurips.cc/paper/2019/hash/bf25356fd2a6e038f1a3a59c26687e80-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Learning+to+Self-Train+for+Semi-Supervised+Few-Shot+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning to Self-Train for Semi-Supervised Few-Shot Classification**](https://proceedings.neurips.cc/paper/2019/hash/bf25356fd2a6e038f1a3a59c26687e80-Abstract.html) , <br> by *Xinzhe Li and
Qianru Sun and
Yaoyao Liu and
Qin Zhou and
Shibao Zheng and
Tat{-}Seng Chua and
Bernt Schiele* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2790-L2803) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```LiSLZZCS19```
- [![](https://img.shields.io/badge/NeurIPS-2019-green)](https://proceedings.neurips.cc/paper/2019/hash/e4da3b7fbbce2345d7772b0674a318d5-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Multimodal+Model-Agnostic+Meta-Learning+via+Task-Aware+Modulation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Multimodal Model-Agnostic Meta-Learning via Task-Aware Modulation**](https://proceedings.neurips.cc/paper/2019/hash/e4da3b7fbbce2345d7772b0674a318d5-Abstract.html) , <br> by *Risto Vuorio and
Shao{-}Hua Sun and
Hexiang Hu and
Joseph J. Lim* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2805-L2815) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```VuorioSHL19```
- [![](https://img.shields.io/badge/NeurIPS-2019-green)](https://proceedings.neurips.cc/paper/2019/hash/92262bf907af914b95a0fc33c3f33bf6-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Self-Supervised+Generalisation+with+Meta+Auxiliary+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Self-Supervised Generalisation with Meta Auxiliary Learning**](https://proceedings.neurips.cc/paper/2019/hash/92262bf907af914b95a0fc33c3f33bf6-Abstract.html) , <br> by *Shikun Liu and
Andrew J. Davison and
Edward Johns* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2817-L2826) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```LiuDJ19```
- [![](https://img.shields.io/badge/NeurIPS-2019-green)](https://proceedings.neurips.cc/paper/2019/hash/f4aa0dd960521e045ae2f20621fb4ee9-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Adaptive+Gradient-Based+Meta-Learning+Methods"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Adaptive Gradient-Based Meta-Learning Methods**](https://proceedings.neurips.cc/paper/2019/hash/f4aa0dd960521e045ae2f20621fb4ee9-Abstract.html) , <br> by *Mikhail Khodak and
Maria{-}Florina Balcan and
Ameet S. Talwalkar* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2828-L2837) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```KhodakBT19```
- [![](https://img.shields.io/badge/NeurIPS-2019-green)](https://proceedings.neurips.cc/paper/2019/hash/6fe43269967adbb64ec6149852b5cc3e-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Meta+Learning+with+Relational+Information+for+Short+Sequences"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta Learning with Relational Information for Short Sequences**](https://proceedings.neurips.cc/paper/2019/hash/6fe43269967adbb64ec6149852b5cc3e-Abstract.html) , <br> by *Yujia Xie and
Haoming Jiang and
Feng Liu and
Tuo Zhao and
Hongyuan Zha* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2839-L2850) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```XieJLZZ19```
- [![](https://img.shields.io/badge/NeurIPS-2018-green)](https://proceedings.neurips.cc/paper/2018/hash/66808e327dc79d135ba18e051673d906-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=TADAM:+Task+dependent+adaptive+metric+for+improved+few-shot+learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**TADAM: Task dependent adaptive metric for improved few-shot learning**](https://proceedings.neurips.cc/paper/2018/hash/66808e327dc79d135ba18e051673d906-Abstract.html) , <br> by *Boris N. Oreshkin and
Pau Rodr{\'{\i}}guez L{\'{o}}pez and
Alexandre Lacoste* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2852-L2861) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```OreshkinLL18```
- [![](https://img.shields.io/badge/NeurIPS-2018-green)](https://proceedings.neurips.cc/paper/2018/hash/b9a25e422ba96f7572089a00b838c3f8-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Learning+To+Learn+Around+A+Common+Mean"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning To Learn Around A Common Mean**](https://proceedings.neurips.cc/paper/2018/hash/b9a25e422ba96f7572089a00b838c3f8-Abstract.html) , <br> by *Giulia Denevi and
Carlo Ciliberto and
Dimitris Stamos and
Massimiliano Pontil* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2863-L2873) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```DeneviCSP18```
- [![](https://img.shields.io/badge/NeurIPS-2018-green)](https://proceedings.neurips.cc/paper/2018/hash/81448138f5f163ccdba4acc69819f280-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Low-shot+Learning+via+Covariance-Preserving+Adversarial+Augmentation+Networks"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Low-shot Learning via Covariance-Preserving Adversarial Augmentation
Networks**](https://proceedings.neurips.cc/paper/2018/hash/81448138f5f163ccdba4acc69819f280-Abstract.html) , <br> by *Hang Gao and
Zheng Shou and
Alireza Zareian and
Hanwang Zhang and
Shih{-}Fu Chang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2875-L2887) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```GaoSZZC18```
- [![](https://img.shields.io/badge/NeurIPS-2017-green)](https://proceedings.neurips.cc/paper/2017/hash/01e9565cecc4e989123f9620c1d09c09-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Learning+Through+an+Information+Retrieval+Lens"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Learning Through an Information Retrieval Lens**](https://proceedings.neurips.cc/paper/2017/hash/01e9565cecc4e989123f9620c1d09c09-Abstract.html) , <br> by *Eleni Triantafillou and
Richard S. Zemel and
Raquel Urtasun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2889-L2898) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```TriantafillouZU17```
- [![](https://img.shields.io/badge/NeurIPS-2017-green)](https://proceedings.neurips.cc/paper/2017/hash/cb8da6767461f2812ae4290eac7cbc42-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Prototypical+Networks+for+Few-shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Prototypical Networks for Few-shot Learning**](https://proceedings.neurips.cc/paper/2017/hash/cb8da6767461f2812ae4290eac7cbc42-Abstract.html) , <br> by *Jake Snell and
Kevin Swersky and
Richard S. Zemel* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2900-L2909) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```SnellSZ17```
- [![](https://img.shields.io/badge/NeurIPS-2017-green)](https://proceedings.neurips.cc/paper/2017/hash/21c5bba1dd6aed9ab48c2b34c1a0adde-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Adversarial+Domain+Adaptation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Adversarial Domain Adaptation**](https://proceedings.neurips.cc/paper/2017/hash/21c5bba1dd6aed9ab48c2b34c1a0adde-Abstract.html) , <br> by *Saeid Motiian and
Quinn Jones and
Seyed Mehdi Iranmanesh and
Gianfranco Doretto* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2911-L2921) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```MotiianJID17```
- [![](https://img.shields.io/badge/NeurIPS-2016-green)](https://proceedings.neurips.cc/paper/2016/hash/90e1357833654983612fb05e3ec9148c-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Matching+Networks+for+One+Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Matching Networks for One Shot Learning**](https://proceedings.neurips.cc/paper/2016/hash/90e1357833654983612fb05e3ec9148c-Abstract.html) , <br> by *Oriol Vinyals and
Charles Blundell and
Tim Lillicrap and
Koray Kavukcuoglu and
Daan Wierstra* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1113-L1124) <br>```MatchNet
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```VinyalsBLKW16```
## AAAI

- [![](https://img.shields.io/badge/AAAI-2021-green)](https://ojs.aaai.org/index.php/AAAI/article/view/17639)<a href="https://scholar.google.com.hk/scholar?q=FL-MSRE:+A+Few-Shot+Learning+based+Approach+to+Multimodal+Social+Relation+Extraction"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**FL-MSRE: A Few-Shot Learning based Approach to Multimodal Social
Relation Extraction**](https://ojs.aaai.org/index.php/AAAI/article/view/17639) , <br> by *Hai Wan and
Manrong Zhang and
Jianfeng Du and
Ziling Huang and
Yufei Yang and
Jeff Z. Pan* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1211-L1224) <br>```FL-MSRE
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```WanZDHYP21```
- [![](https://img.shields.io/badge/AAAI-2020-green)](https://aaai.org/ojs/index.php/AAAI/article/view/6281)<a href="https://scholar.google.com.hk/scholar?q=Neural+Snowball+for+Few-Shot+Relation+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Neural Snowball for Few-Shot Relation Learning**](https://aaai.org/ojs/index.php/AAAI/article/view/6281) , <br> by *Tianyu Gao and
Xu Han and
Ruobing Xie and
Zhiyuan Liu and
Fen Lin and
Leyu Lin and
Maosong Sun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1314-L1327) <br>```Neural Snowball
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```GaoHX0LLS20```
- [![](https://img.shields.io/badge/AAAI-2019-green)](https://doi.org/10.1609/aaai.v33i01.33016407)<a href="https://scholar.google.com.hk/scholar?q=Hybrid+Attention-Based+Prototypical+Networks+for+Noisy+Few-Shot+Relation+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Hybrid Attention-Based Prototypical Networks for Noisy Few-Shot Relation
Classification**](https://doi.org/10.1609/aaai.v33i01.33016407) , <br> by *Tianyu Gao and
Xu Han and
Zhiyuan Liu and
Maosong Sun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1300-L1311) <br>```HATT
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```GaoH0S19```
## IJCAI

- [![](https://img.shields.io/badge/IJCAI-2021-green)](https://doi.org/10.24963/ijcai.2021/149)<a href="https://scholar.google.com.hk/scholar?q=Cross-Domain+Few-Shot+Classification+via+Adversarial+Task+Augmentation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Cross-Domain Few-Shot Classification via Adversarial Task Augmentation**](https://doi.org/10.24963/ijcai.2021/149) , <br> by *Wang, Haoqing and Deng, Zhi-Hong* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1128-L1135) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ijcai2021-149```
- [![](https://img.shields.io/badge/IJCAI-2021-green)](https://doi.org/10.24963/ijcai.2021/419)<a href="https://scholar.google.com.hk/scholar?q=Self-supervised+Network+Evolution+for+Few-shot+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Self-supervised Network Evolution for Few-shot Classification**](https://doi.org/10.24963/ijcai.2021/419) , <br> by *Tang, Xuwen, Teng, Zhu, Zhang, Baopeng and Fan, Jianping* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1137-L1144) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ijcai2021-419```
- [![](https://img.shields.io/badge/IJCAI-2021-green)](https://doi.org/10.24963/ijcai.2021/295)<a href="https://scholar.google.com.hk/scholar?q=Conditional+Self-Supervised+Learning+for+Few-Shot+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Conditional Self-Supervised Learning for Few-Shot Classification**](https://doi.org/10.24963/ijcai.2021/295) , <br> by *An, Yuexuan, Xue, Hui, Zhao, Xingyu and Zhang, Lu* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1146-L1153) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ijcai2021-295```
- [![](https://img.shields.io/badge/IJCAI-2021-green)](https://doi.org/10.24963/ijcai.2021/475)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Partial-Label+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Partial-Label Learning**](https://doi.org/10.24963/ijcai.2021/475) , <br> by *Zhao, Yunfeng, Yu, Guoxian, Liu, Lei, Yan, Zhongmin, Cui, Lizhen and Domeniconi, Carlotta* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1155-L1162) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ijcai2021-475```
- [![](https://img.shields.io/badge/IJCAI-2021-green)](https://doi.org/10.24963/ijcai.2021/471)<a href="https://scholar.google.com.hk/scholar?q=Uncertainty-Aware+Few-Shot+Image+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Uncertainty-Aware Few-Shot Image Classification**](https://doi.org/10.24963/ijcai.2021/471) , <br> by *Zhang, Zhizheng, Lan, Cuiling, Zeng, Wenjun, Chen, Zhibo and Chang, Shih-Fu* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1164-L1171) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ijcai2021-471```
- [![](https://img.shields.io/badge/IJCAI-2021-green)](https://doi.org/10.24963/ijcai.2021/313)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Learning+with+Part+Discovery+and+Augmentation+from+Unlabeled+Images"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Learning with Part Discovery and Augmentation from Unlabeled Images**](https://doi.org/10.24963/ijcai.2021/313) , <br> by *Chen, Wentao, Si, Chenyang, Wang, Wei, Wang, Liang, Wang, Zilei and Tan, Tieniu* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1175-L1182) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ijcai2021-313```
## SIGIR

- [![](https://img.shields.io/badge/SIGIR-2021-green)](https://doi.org/10.1145/3404835.3463054)<a href="https://scholar.google.com.hk/scholar?q=Graph+Learning+Regularization+and+Transfer+Learning+for+Few-Shot+Event+Detection"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Graph Learning Regularization and Transfer Learning for Few-Shot Event Detection**](https://doi.org/10.1145/3404835.3463054) , <br> by *Lai, Viet Dac, Nguyen, Minh Van, Nguyen, Thien Huu and Dernoncourt, Franck* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1359-L1366) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```3404835.3463054```
- [![](https://img.shields.io/badge/SIGIR-2021-green)](https://doi.org/10.1145/3404835.3462995)<a href="https://scholar.google.com.hk/scholar?q=Pseudo+Siamese+Network+for+Few-Shot+Intent+Generation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Pseudo Siamese Network for Few-Shot Intent Generation**](https://doi.org/10.1145/3404835.3462995) , <br> by *Xia, Congying, Xiong, Caiming and Yu, Philip* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1368-L1375) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```3404835.3462995```
## KDD

- [![](https://img.shields.io/badge/KDD-2021-green)](https://doi.org/10.1145/3447548.3467438)<a href="https://scholar.google.com.hk/scholar?q=Knowledge-Enhanced+Domain+Adaptation+in+Few-Shot+Relation+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Knowledge-Enhanced Domain Adaptation in Few-Shot Relation Classification**](https://doi.org/10.1145/3447548.3467438) , <br> by *Zhang, Jiawen, Zhu, Jiaqi, Yang, Yi, Shi, Wandong, Zhang, Congcong and Wang, Hongan* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1332-L1339) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```3447548.3467438```
- [![](https://img.shields.io/badge/KDD-2021-green)](https://doi.org/10.1145/3447548.3467235)<a href="https://scholar.google.com.hk/scholar?q=Meta+Self-Training+for+Few-Shot+Neural+Sequence+Labeling"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta Self-Training for Few-Shot Neural Sequence Labeling**](https://doi.org/10.1145/3447548.3467235) , <br> by *Wang, Yaqing, Mukherjee, Subhabrata, Chu, Haoda, Tu, Yuancheng, Wu, Ming, Gao, Jing and Awadallah, Ahmed Hassan* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1341-L1348) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```3447548.3467235```
## CVPR

- [![](https://img.shields.io/badge/CVPR-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Yue_Prototypical_Cross-Domain_Self-Supervised_Learning_for_Few-Shot_Unsupervised_Domain_Adaptation_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Prototypical+Cross-Domain+Self-Supervised+Learning+for+Few-Shot+Unsupervised+Domain+Adaptation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Prototypical Cross-Domain Self-Supervised Learning for Few-Shot Unsupervised Domain Adaptation**](https://openaccess.thecvf.com/content/CVPR2021/html/Yue_Prototypical_Cross-Domain_Self-Supervised_Learning_for_Few-Shot_Unsupervised_Domain_Adaptation_CVPR_2021_paper.html) , <br> by *Yue, Xiangyu, Zheng, Zangwei, Zhang, Shanghang, Gao, Yang, Darrell, Trevor, Keutzer, Kurt and Vincentelli, Alberto Sangiovanni* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1682-L1689) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Yue_2021_CVPR```
- [![](https://img.shields.io/badge/CVPR-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Accurate_Few-Shot_Object_Detection_With_Support-Query_Mutual_Guidance_and_Hybrid_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Accurate+Few-Shot+Object+Detection+With+Support-Query+Mutual+Guidance+and+Hybrid+Loss"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Accurate Few-Shot Object Detection With Support-Query Mutual Guidance and Hybrid Loss**](https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Accurate_Few-Shot_Object_Detection_With_Support-Query_Mutual_Guidance_and_Hybrid_CVPR_2021_paper.html) , <br> by *Zhang, Lu, Zhou, Shuigeng, Guan, Jihong and Zhang, Ji* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1691-L1699) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Zhang_2021_CVPR```
- [![](https://img.shields.io/badge/CVPR-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Fan_Generalized_Few-Shot_Object_Detection_Without_Forgetting_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Generalized+Few-Shot+Object+Detection+Without+Forgetting"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Generalized Few-Shot Object Detection Without Forgetting**](https://openaccess.thecvf.com/content/CVPR2021/html/Fan_Generalized_Few-Shot_Object_Detection_Without_Forgetting_CVPR_2021_paper.html) , <br> by *Fan, Zhibo, Ma, Yuchen, Li, Zeming and Sun, Jian* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1701-L1708) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Fan_2021_CVPR```
- [![](https://img.shields.io/badge/CVPR-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Hallucination_Improves_Few-Shot_Object_Detection_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Hallucination+Improves+Few-Shot+Object+Detection"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Hallucination Improves Few-Shot Object Detection**](https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Hallucination_Improves_Few-Shot_Object_Detection_CVPR_2021_paper.html) , <br> by *Zhang, Weilin and Wang, Yu-Xiong* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1691-L1699) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Zhang_2021_CVPR```
- [![](https://img.shields.io/badge/CVPR-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Few-Shot_Incremental_Learning_With_Continually_Evolved_Classifiers_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Incremental+Learning+With+Continually+Evolved+Classifiers"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Incremental Learning With Continually Evolved Classifiers**](https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Few-Shot_Incremental_Learning_With_Continually_Evolved_Classifiers_CVPR_2021_paper.html) , <br> by *Zhang, Chi, Song, Nan, Lin, Guosheng, Zheng, Yun, Pan, Pan and Xu, Yinghui* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1691-L1699) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Zhang_2021_CVPR```
- [![](https://img.shields.io/badge/CVPR-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Rethinking_Class_Relations_Absolute-Relative_Supervised_and_Unsupervised_Few-Shot_Learning_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Rethinking+Class+Relations:+Absolute-Relative+Supervised+and+Unsupervised+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Rethinking Class Relations: Absolute-Relative Supervised and Unsupervised Few-Shot Learning**](https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Rethinking_Class_Relations_Absolute-Relative_Supervised_and_Unsupervised_Few-Shot_Learning_CVPR_2021_paper.html) , <br> by *Zhang, Hongguang, Koniusz, Piotr, Jian, Songlei, Li, Hongdong and Torr, Philip H. S.* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1691-L1699) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Zhang_2021_CVPR```
- [![](https://img.shields.io/badge/CVPR-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Prototype_Completion_With_Primitive_Knowledge_for_Few-Shot_Learning_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Prototype+Completion+With+Primitive+Knowledge+for+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Prototype Completion With Primitive Knowledge for Few-Shot Learning**](https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Prototype_Completion_With_Primitive_Knowledge_for_Few-Shot_Learning_CVPR_2021_paper.html) , <br> by *Zhang, Baoquan, Li, Xutao, Ye, Yunming, Huang, Zhichao and Zhang, Lisai* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1691-L1699) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Zhang_2021_CVPR```
- [![](https://img.shields.io/badge/CVPR-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Ganea_Incremental_Few-Shot_Instance_Segmentation_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Incremental+Few-Shot+Instance+Segmentation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Incremental Few-Shot Instance Segmentation**](https://openaccess.thecvf.com/content/CVPR2021/html/Ganea_Incremental_Few-Shot_Instance_Segmentation_CVPR_2021_paper.html) , <br> by *Ganea, Dan Andrei, Boom, Bas and Poppe, Ronald* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1749-L1757) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Ganea_2021_CVPR```
- [![](https://img.shields.io/badge/CVPR-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Boudiaf_Few-Shot_Segmentation_Without_Meta-Learning_A_Good_Transductive_Inference_Is_All_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Segmentation+Without+Meta-Learning:+A+Good+Transductive+Inference+Is+All+You+Need?"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Segmentation Without Meta-Learning: A Good Transductive Inference Is All You Need?**](https://openaccess.thecvf.com/content/CVPR2021/html/Boudiaf_Few-Shot_Segmentation_Without_Meta-Learning_A_Good_Transductive_Inference_Is_All_CVPR_2021_paper.html) , <br> by *Boudiaf, Malik, Kervadec, Hoel, Masud, Ziko Imtiaz, Piantanida, Pablo, Ben Ayed, Ismail and Dolz, Jose* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1759-L1767) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Boudiaf_2021_CVPR```
- [![](https://img.shields.io/badge/CVPR-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Zhu_Semantic_Relation_Reasoning_for_Shot-Stable_Few-Shot_Object_Detection_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Semantic+Relation+Reasoning+for+Shot-Stable+Few-Shot+Object+Detection"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Semantic Relation Reasoning for Shot-Stable Few-Shot Object Detection**](https://openaccess.thecvf.com/content/CVPR2021/html/Zhu_Semantic_Relation_Reasoning_for_Shot-Stable_Few-Shot_Object_Detection_CVPR_2021_paper.html) , <br> by *Zhu, Chenchen, Chen, Fangyi, Ahmed, Uzair, Shen, Zhiqiang and Savvides, Marios* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1769-L1777) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Zhu_2021_CVPR```
- [![](https://img.shields.io/badge/CVPR-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Zhu_Self-Promoted_Prototype_Refinement_for_Few-Shot_Class-Incremental_Learning_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Self-Promoted+Prototype+Refinement+for+Few-Shot+Class-Incremental+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Self-Promoted Prototype Refinement for Few-Shot Class-Incremental Learning**](https://openaccess.thecvf.com/content/CVPR2021/html/Zhu_Self-Promoted_Prototype_Refinement_for_Few-Shot_Class-Incremental_Learning_CVPR_2021_paper.html) , <br> by *Zhu, Kai, Cao, Yang, Zhai, Wei, Cheng, Jie and Zha, Zheng-Jun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1769-L1777) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Zhu_2021_CVPR```
- [![](https://img.shields.io/badge/CVPR-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Wertheimer_Few-Shot_Classification_With_Feature_Map_Reconstruction_Networks_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Classification+With+Feature+Map+Reconstruction+Networks"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Classification With Feature Map Reconstruction Networks**](https://openaccess.thecvf.com/content/CVPR2021/html/Wertheimer_Few-Shot_Classification_With_Feature_Map_Reconstruction_Networks_CVPR_2021_paper.html) , <br> by *Wertheimer, Davis, Tang, Luming and Hariharan, Bharath* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1789-L1797) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Wertheimer_2021_CVPR```
- [![](https://img.shields.io/badge/CVPR-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Nguyen_FAPIS_A_Few-Shot_Anchor-Free_Part-Based_Instance_Segmenter_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=FAPIS:+A+Few-Shot+Anchor-Free+Part-Based+Instance+Segmenter"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**FAPIS: A Few-Shot Anchor-Free Part-Based Instance Segmenter**](https://openaccess.thecvf.com/content/CVPR2021/html/Nguyen_FAPIS_A_Few-Shot_Anchor-Free_Part-Based_Instance_Segmenter_CVPR_2021_paper.html) , <br> by *Nguyen, Khoi and Todorovic, Sinisa* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1799-L1807) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Nguyen_2021_CVPR```
- [![](https://img.shields.io/badge/CVPR-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Hong_Reinforced_Attention_for_Few-Shot_Learning_and_Beyond_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Reinforced+Attention+for+Few-Shot+Learning+and+Beyond"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Reinforced Attention for Few-Shot Learning and Beyond**](https://openaccess.thecvf.com/content/CVPR2021/html/Hong_Reinforced_Attention_for_Few-Shot_Learning_and_Beyond_CVPR_2021_paper.html) , <br> by *Hong, Jie, Fang, Pengfei, Li, Weihao, Zhang, Tong, Simon, Christian, Harandi, Mehrtash and Petersson, Lars* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1809-L1817) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Hong_2021_CVPR```
- [![](https://img.shields.io/badge/CVPR-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Hu_Dense_Relation_Distillation_With_Context-Aware_Aggregation_for_Few-Shot_Object_Detection_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Dense+Relation+Distillation+With+Context-Aware+Aggregation+for+Few-Shot+Object+Detection"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Dense Relation Distillation With Context-Aware Aggregation for Few-Shot Object Detection**](https://openaccess.thecvf.com/content/CVPR2021/html/Hu_Dense_Relation_Distillation_With_Context-Aware_Aggregation_for_Few-Shot_Object_Detection_CVPR_2021_paper.html) , <br> by *Hu, Hanzhe, Bai, Shuai, Li, Aoxue, Cui, Jinshi and Wang, Liwei* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1819-L1827) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Hu_2021_CVPR```
- [![](https://img.shields.io/badge/CVPR-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Jeong_Few-Shot_Open-Set_Recognition_by_Transformation_Consistency_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Open-Set+Recognition+by+Transformation+Consistency"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Open-Set Recognition by Transformation Consistency**](https://openaccess.thecvf.com/content/CVPR2021/html/Jeong_Few-Shot_Open-Set_Recognition_by_Transformation_Consistency_CVPR_2021_paper.html) , <br> by *Jeong, Minki, Choi, Seokeon and Kim, Changick* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1829-L1837) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Jeong_2021_CVPR```
- [![](https://img.shields.io/badge/CVPR-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Xu_Learning_Dynamic_Alignment_via_Meta-Filter_for_Few-Shot_Learning_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Learning+Dynamic+Alignment+via+Meta-Filter+for+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning Dynamic Alignment via Meta-Filter for Few-Shot Learning**](https://openaccess.thecvf.com/content/CVPR2021/html/Xu_Learning_Dynamic_Alignment_via_Meta-Filter_for_Few-Shot_Learning_CVPR_2021_paper.html) , <br> by *Xu, Chengming, Fu, Yanwei, Liu, Chen, Wang, Chengjie, Li, Jilin, Huang, Feiyue, Zhang, Li and Xue, Xiangyang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1839-L1847) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Xu_2021_CVPR```
- [![](https://img.shields.io/badge/CVPR-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Rizve_Exploring_Complementary_Strengths_of_Invariant_and_Equivariant_Representations_for_Few-Shot_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Exploring+Complementary+Strengths+of+Invariant+and+Equivariant+Representations+for+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Exploring Complementary Strengths of Invariant and Equivariant Representations for Few-Shot Learning**](https://openaccess.thecvf.com/content/CVPR2021/html/Rizve_Exploring_Complementary_Strengths_of_Invariant_and_Equivariant_Representations_for_Few-Shot_CVPR_2021_paper.html) , <br> by *Rizve, Mamshad Nayeem, Khan, Salman, Khan, Fahad Shahbaz and Shah, Mubarak* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1849-L1857) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Rizve_2021_CVPR```
- [![](https://img.shields.io/badge/CVPR-2020-green)](https://doi.org/10.1109/CVPR42600.2020.01259)<a href="https://scholar.google.com.hk/scholar?q=Boosting+Few-Shot+Learning+With+Adaptive+Margin+Loss"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Boosting Few-Shot Learning With Adaptive Margin Loss**](https://doi.org/10.1109/CVPR42600.2020.01259) , <br> by *Aoxue Li and
Weiran Huang and
Xu Lan and
Jiashi Feng and
Zhenguo Li and
Liwei Wang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1185-L1197) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Li0LFLW20```
## ICCV

- [![](https://img.shields.io/badge/ICCV-2021-green)](https://openaccess.thecvf.com/content/ICCV2021/html/Chowdhury_Few-Shot_Image_Classification_Just_Use_a_Library_of_Pre-Trained_Feature_ICCV_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Image+Classification:+Just+Use+a+Library+of+Pre-Trained+Feature+Extractors+and+a+Simple+Classifier"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Image Classification: Just Use a Library of Pre-Trained Feature Extractors and a Simple Classifier**](https://openaccess.thecvf.com/content/ICCV2021/html/Chowdhury_Few-Shot_Image_Classification_Just_Use_a_Library_of_Pre-Trained_Feature_ICCV_2021_paper.html) , <br> by *Chowdhury, Arkabandhu, Jiang, Mingchao, Chaudhuri, Swarat and Jermaine, Chris* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1057-L1064) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Chowdhury_2021_ICCV```
- [![](https://img.shields.io/badge/ICCV-2021-green)](https://openaccess.thecvf.com/content/ICCV2021/html/Lazarou_Iterative_Label_Cleaning_for_Transductive_and_Semi-Supervised_Few-Shot_Learning_ICCV_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Iterative+Label+Cleaning+for+Transductive+and+Semi-Supervised+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Iterative Label Cleaning for Transductive and Semi-Supervised Few-Shot Learning**](https://openaccess.thecvf.com/content/ICCV2021/html/Lazarou_Iterative_Label_Cleaning_for_Transductive_and_Semi-Supervised_Few-Shot_Learning_ICCV_2021_paper.html) , <br> by *Lazarou, Michalis, Stathaki, Tania and Avrithis, Yannis* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1066-L1073) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Lazarou_2021_ICCV```
- [![](https://img.shields.io/badge/ICCV-2021-green)](https://openaccess.thecvf.com/content/ICCV2021/html/Das_On_the_Importance_of_Distractors_for_Few-Shot_Classification_ICCV_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=On+the+Importance+of+Distractors+for+Few-Shot+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**On the Importance of Distractors for Few-Shot Classification**](https://openaccess.thecvf.com/content/ICCV2021/html/Das_On_the_Importance_of_Distractors_for_Few-Shot_Classification_ICCV_2021_paper.html) , <br> by *Das, Rajshekhar, Wang, Yu-Xiong and Moura, Jos\'e M. F.* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1075-L1082) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Das_2021_ICCV```
## TACL

- [![](https://img.shields.io/badge/Trans._Assoc._Comput._Linguistics-2021-green)](https://aclanthology.org/2021.tacl-1.42)<a href="https://scholar.google.com.hk/scholar?q=Revisiting+Few-shot+Relation+Classification:+Evaluation+Data+and+Classification+Schemes"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Revisiting Few-shot Relation Classification: Evaluation Data and Classification Schemes**](https://aclanthology.org/2021.tacl-1.42) , <br> by *Sabo, Ofer  and
Elazar, Yanai  and
Goldberg, Yoav  and
Dagan, Ido* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1084-L1095) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```sabo-etal-2021-revisiting```
- [![](https://img.shields.io/badge/Trans._Assoc._Comput._Linguistics-2020-green)](https://transacl.org/ojs/index.php/tacl/article/view/1983)<a href="https://scholar.google.com.hk/scholar?q=How+Can+We+Know+What+Language+Models+Know"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**How Can We Know What Language Models Know**](https://transacl.org/ojs/index.php/tacl/article/view/1983) , <br> by *Zhengbao Jiang and
Frank F. Xu and
Jun Araki and
Graham Neubig* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3010-L3021) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```JiangXAN20```
## ACM Comput. Surv.

- [![](https://img.shields.io/badge/ACM_Comput._Surv.-2020-green)](https://doi.org/10.1145/3386252)<a href="https://scholar.google.com.hk/scholar?q=Generalizing+from+a+Few+Examples:+A+Survey+on+Few-shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Generalizing from a Few Examples: A Survey on Few-shot Learning**](https://doi.org/10.1145/3386252) , <br> by *Yaqing Wang and
Quanming Yao and
James T. Kwok and
Lionel M. Ni* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1199-L1209) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```WangYKN20```
## arXiv

- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2104.07650)<a href="https://scholar.google.com.hk/scholar?q=AdaPrompt:+Adaptive+Prompt-based+Finetuning+for+Relation+Extraction"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**AdaPrompt: Adaptive Prompt-based Finetuning for Relation Extraction**](https://arxiv.org/abs/2104.07650) , <br> by *Xiang Chen and
Xin Xie and
Ningyu Zhang and
Jiahuan Yan and
Shumin Deng and
Chuanqi Tan and
Fei Huang and
Luo Si and
Huajun Chen* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3406-L3421) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2104-07650```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2103.10385)<a href="https://scholar.google.com.hk/scholar?q=GPT+Understands,+Too"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**GPT Understands, Too**](https://arxiv.org/abs/2103.10385) , <br> by *Xiao Liu and
Yanan Zheng and
Zhengxiao Du and
Ming Ding and
Yujie Qian and
Zhilin Yang and
Jie Tang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3423-L3436) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2103-10385```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2101.00190)<a href="https://scholar.google.com.hk/scholar?q=Prefix-Tuning:+Optimizing+Continuous+Prompts+for+Generation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Prefix-Tuning: Optimizing Continuous Prompts for Generation**](https://arxiv.org/abs/2101.00190) , <br> by *Xiang Lisa Li and
Percy Liang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3439-L3447) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2101-00190```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2104.08773)<a href="https://scholar.google.com.hk/scholar?q=Natural+Instructions:+Benchmarking+Generalization+to+New+Tasks+from+Natural+Language+Instructions"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Natural Instructions: Benchmarking Generalization to New Tasks from
Natural Language Instructions**](https://arxiv.org/abs/2104.08773) , <br> by *Swaroop Mishra and
Daniel Khashabi and
Chitta Baral and
Hannaneh Hajishirzi* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3459-L3470) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2104-08773```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2105.11259)<a href="https://scholar.google.com.hk/scholar?q=PTR:+Prompt+Tuning+with+Rules+for+Text+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**PTR: Prompt Tuning with Rules for Text Classification**](https://arxiv.org/abs/2105.11259) , <br> by *Xu Han and
Weilin Zhao and
Ning Ding and
Zhiyuan Liu and
Maosong Sun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3472-L3483) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2105-11259```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2104.08691)<a href="https://scholar.google.com.hk/scholar?q=The+Power+of+Scale+for+Parameter-Efficient+Prompt+Tuning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**The Power of Scale for Parameter-Efficient Prompt Tuning**](https://arxiv.org/abs/2104.08691) , <br> by *Brian Lester and
Rami Al{-}Rfou and
Noah Constant* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3485-L3494) <br>```EMNLP 2021
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2104-08691```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2106.06411)<a href="https://scholar.google.com.hk/scholar?q=Zero-Shot+Controlled+Generation+with+Encoder-Decoder+Transformers"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Zero-Shot Controlled Generation with Encoder-Decoder Transformers**](https://arxiv.org/abs/2106.06411) , <br> by *Devamanyu Hazarika, Mahdi Namazifar and Dilek Hakkani-Tür* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3497-L3504) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```hazarika2021zeroshot```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2104.08826)<a href="https://scholar.google.com.hk/scholar?q=GPT3Mix:+Leveraging+Large-scale+Language+Models+for+Text+Augmentation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation**](https://arxiv.org/abs/2104.08826) , <br> by *Kang Min Yoo and
Dongju Park and
Jaewook Kang and
Sang{-}Woo Lee and
Woomyeong Park* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3506-L3517) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2104-08826```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2104.07540)<a href="https://scholar.google.com.hk/scholar?q=Generating+Datasets+with+Pretrained+Language+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Generating Datasets with Pretrained Language Models**](https://arxiv.org/abs/2104.07540) , <br> by *Timo Schick and
Hinrich Sch{\"{u}}tze* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3519-L3527) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2104-07540```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2102.01335)<a href="https://scholar.google.com.hk/scholar?q=Neural+Data+Augmentation+via+Example+Extrapolation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Neural Data Augmentation via Example Extrapolation**](https://arxiv.org/abs/2102.01335) , <br> by *Kenton Lee and
Kelvin Guu and
Luheng He and
Tim Dozat and
Hyung Won Chung* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3529-L3540) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2102-01335```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2104.14690)<a href="https://scholar.google.com.hk/scholar?q=Entailment+as+Few-Shot+Learner"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Entailment as Few-Shot Learner**](https://arxiv.org/abs/2104.14690) , <br> by *Sinong Wang and
Han Fang and
Madian Khabsa and
Hanzi Mao and
Hao Ma* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3542-L3553) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2104-14690```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2104.08786)<a href="https://scholar.google.com.hk/scholar?q=Fantastically+Ordered+Prompts+and+Where+to+Find+Them:+Overcoming+Few-Shot+Prompt+Order+Sensitivity"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot
Prompt Order Sensitivity**](https://arxiv.org/abs/2104.08786) , <br> by *Yao Lu and
Max Bartolo and
Alastair Moore and
Sebastian Riedel and
Pontus Stenetorp* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3555-L3567) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2104-08786```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2106.07499)<a href="https://scholar.google.com.hk/scholar?q=An+Empirical+Survey+of+Data+Augmentation+for+Limited+Data+Learning+in+NLP"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**An Empirical Survey of Data Augmentation for Limited Data Learning in NLP**](https://arxiv.org/abs/2106.07499) , <br> by *Jiaao Chen, Derek Tam, Colin Raffel, Mohit Bansal and Diyi Yang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3570-L3577) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```chen2021empirical```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2104.04670)<a href="https://scholar.google.com.hk/scholar?q=Meta-tuning+Language+Models+to+Answer+Prompts+Better"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta-tuning Language Models to Answer Prompts Better**](https://arxiv.org/abs/2104.04670) , <br> by *Ruiqi Zhong and
Kristy Lee and
Zheng Zhang and
Dan Klein* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3579-L3589) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2104-04670```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2106.02695)<a href="https://scholar.google.com.hk/scholar?q=Meta-Learning+with+Fewer+Tasks+through+Task+Interpolation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta-Learning with Fewer Tasks through Task Interpolation**](https://arxiv.org/abs/2106.02695) , <br> by *Huaxiu Yao, Linjun Zhang and Chelsea Finn* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3591-L3598) <br>```NeurIPS under-review
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```yao2021metalearning```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2106.07343)<a href="https://scholar.google.com.hk/scholar?q=Learning+to+Bridge+Metric+Spaces:+Few-shot+Joint+Learning+of+Intent+Detection+and+Slot+Filling"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning to Bridge Metric Spaces: Few-shot Joint Learning of Intent Detection and Slot Filling**](https://arxiv.org/abs/2106.07343) , <br> by *Yutai Hou, Yongkui Lai, Cheng Chen, Wanxiang Che and Ting Liu* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3602-L3609) <br>```ACL Findings 2021 preprint
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```hou2021learning```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2107.13586)<a href="https://scholar.google.com.hk/scholar?q=Pre-train,+Prompt,+and+Predict:+A+Systematic+Survey+of+Prompting+Methods+in+Natural+Language+Processing"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing**](https://arxiv.org/abs/2107.13586) , <br> by *Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi and Graham Neubig* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3644-L3652) <br>```Prompt-based learning -- survey paper
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```liu2021pretrain```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2108.02035)<a href="https://scholar.google.com.hk/scholar?q=Knowledgeable+Prompt-tuning:+Incorporating+Knowledge+into+Prompt+Verbalizer+for+Text+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer
for Text Classification**](https://arxiv.org/abs/2108.02035) , <br> by *Shengding Hu and
Ning Ding and
Huadong Wang and
Zhiyuan Liu and
Juanzi Li and
Maosong Sun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3679-L3692) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2108-02035```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2108.04106)<a href="https://scholar.google.com.hk/scholar?q=Noisy+Channel+Language+Model+Prompting+for+Few-Shot+Text+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Noisy Channel Language Model Prompting for Few-Shot Text Classification**](https://arxiv.org/abs/2108.04106) , <br> by *Sewon Min and
Mike Lewis and
Hannaneh Hajishirzi and
Luke Zettlemoyer* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3694-L3704) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2108-04106```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2109.01247)<a href="https://scholar.google.com.hk/scholar?q=Do+Prompt-Based+Models+Really+Understand+the+Meaning+of+their+Prompts?"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Do Prompt-Based Models Really Understand the Meaning of their Prompts?**](https://arxiv.org/abs/2109.01247) , <br> by *Albert Webson and Ellie Pavlick* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3706-L3713) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```webson2021promptbased```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2108.10604)<a href="https://scholar.google.com.hk/scholar?q=Prompt-Learning+for+Fine-Grained+Entity+Typing"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Prompt-Learning for Fine-Grained Entity Typing**](https://arxiv.org/abs/2108.10604) , <br> by *Ning Ding and
Yulin Chen and
Xu Han and
Guangwei Xu and
Pengjun Xie and
Hai{-}Tao Zheng and
Zhiyuan Liu and
Juanzi Li and
Hong{-}Gee Kim* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3717-L3732) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2108-10604```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2108.13487)<a href="https://scholar.google.com.hk/scholar?q=Want+To+Reduce+Labeling+Cost?+GPT-3+Can+Help"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Want To Reduce Labeling Cost? GPT-3 Can Help**](https://arxiv.org/abs/2108.13487) , <br> by *Shuohang Wang, Yang Liu, Yichong Xu, Chenguang Zhu and Michael Zeng* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3735-L3745) <br>```EMNLP Findings 2021, adopting GPT-3 for label generation.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```wang2021want```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2109.03630)<a href="https://scholar.google.com.hk/scholar?q=Discrete+and+Soft+Prompting+for+Multilingual+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Discrete and Soft Prompting for Multilingual Models**](https://arxiv.org/abs/2109.03630) , <br> by *Mengjie Zhao and Hinrich Schütze* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3749-L3759) <br>```EMNLP 2021
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhao2021discrete```
- [![](https://img.shields.io/badge/CoRR-2020-green)](https://arxiv.org/abs/2012.11926)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Text+Generation+with+Pattern-Exploiting+Training"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Text Generation with Pattern-Exploiting Training**](https://arxiv.org/abs/2012.11926) , <br> by *Timo Schick and
Hinrich Sch{\"{u\textsl{}}}tze* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3449-L3457) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2012-11926```
- [![](https://img.shields.io/badge/CoRR-2020-green)](https://arxiv.org/abs/2012.02353)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Event+Detection+with+Prototypical+Amortized+Conditional+Random+Field"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Event Detection with Prototypical Amortized Conditional Random
Field**](https://arxiv.org/abs/2012.02353) , <br> by *Xin Cong and
Shiyao Cui and
Bowen Yu and
Tingwen Liu and
Yubin Wang and
Bin Wang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3612-L3625) <br>```ACL 2021
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2012-02353```
- [![](https://img.shields.io/badge/CoRR-2020-green)](https://arxiv.org/abs/2012.15682)<a href="https://scholar.google.com.hk/scholar?q=A+Closer+Look+at+Few-Shot+Crosslingual+Transfer:+Variance,+Benchmarks+and+Baselines"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**A Closer Look at Few-Shot Crosslingual Transfer: Variance, Benchmarks
and Baselines**](https://arxiv.org/abs/2012.15682) , <br> by *Mengjie Zhao and
Yi Zhu and
Ehsan Shareghi and
Roi Reichart and
Anna Korhonen and
Hinrich Sch{\"{u}}tze* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3628-L3641) <br>```ACL 2021
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2012-15682```
- [![](https://img.shields.io/badge/CoRR-2020-green)](https://arxiv.org/abs/2009.02653)<a href="https://scholar.google.com.hk/scholar?q=Learning+from+Very+Few+Samples:+A+Survey"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning from Very Few Samples: A Survey**](https://arxiv.org/abs/2009.02653) , <br> by *Jiang Lu and
Pinghua Gong and
Jieping Ye and
Changshui Zhang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3655-L3665) <br>```Survey
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2009-02653```
- [![](https://img.shields.io/badge/CoRR-2020-green)](https://arxiv.org/abs/2106.13353)<a href="https://scholar.google.com.hk/scholar?q=Cutting+Down+on+Prompts+and+Parameters:+Simple+Few-Shot+Learning+with+Language+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models**](https://arxiv.org/abs/2106.13353) , <br> by *Robert L. Logan IV au2, Ivana Balažević, Eric Wallace, Fabio Petroni, Sameer Singh and Sebastian Riedel* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3669-L3676) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```logan2021cutting```