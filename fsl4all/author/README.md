[![](https://img.shields.io/badge/Awesome_Continual_Learning-yellow)](https://github.com/wutong8023/Awesome_Continual_Learning.git) [![](https://img.shields.io/badge/Awesome_Few_Shot_learning-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning.git) [![](https://img.shields.io/badge/Awesome_Information_Extraction-blue)](https://github.com/wutong8023/Awesome_Information_Extraction.git) [![](https://img.shields.io/badge/Awesome_Ideas-orange)](https://github.com/wutong8023/Awesome_Ideas.git)

# Few-shot Learning Literature 
This repository is maintained by [Tongtong Wu](http://wutong8023.site). Please don't hesitate to send me an email to collaborate or fix some entries (wutong8023 AT gmail.com). 
The automation script of this repo is powered by [Auto-Bibfile](https://github.com/wutong8023/Auto-Bibfile.git).

You can directly use our bibtex.bib in overleaf with this [link](https://www.overleaf.com/read/rgscdxhxbwhp).

This page categorizes the literature by the **Author**.

## Outline 
- [![](https://img.shields.io/badge/Hyperlink-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#hyperlink)
- [![](https://img.shields.io/badge/Zhiyuan_Liu-11-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#zhiyuan-liu)
- [![](https://img.shields.io/badge/Xu_Han-9-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#xu-han)
- [![](https://img.shields.io/badge/Philip_Yu-6-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#philip-yu)
- [![](https://img.shields.io/badge/Caiming_Xiong-6-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#caiming-xiong)
- [![](https://img.shields.io/badge/Maosong_Sun-6-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#maosong-sun)
- [![](https://img.shields.io/badge/Chelsea_Finn-5-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#chelsea-finn)
- [![](https://img.shields.io/badge/Huaxiu_Yao-5-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#huaxiu-yao)
- [![](https://img.shields.io/badge/Hugo_Larochelle-5-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#hugo-larochelle)
- [![](https://img.shields.io/badge/Sung_Ju_Hwang-5-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#sung-ju-hwang)
- [![](https://img.shields.io/badge/Congying_Xia-5-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#congying-xia)
- [![](https://img.shields.io/badge/Tianyu_Gao-5-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#tianyu-gao)
- [![](https://img.shields.io/badge/Timo_Schick-5-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#timo-schick)
- [![](https://img.shields.io/badge/Ning_Ding-5-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#ning-ding)
- [![](https://img.shields.io/badge/Richard_Socher-4-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#richard-socher)
- [![](https://img.shields.io/badge/Eleni_Triantafillou-4-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#eleni-triantafillou)
- [![](https://img.shields.io/badge/Graham_Neubig-4-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#graham-neubig)
- [![](https://img.shields.io/badge/Hinrich_Sch{\"u}tze-4-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#hinrich-sch{\"u}tze)
- [![](https://img.shields.io/badge/Subhabrata_Mukherjee-4-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#subhabrata-mukherjee)
- [![](https://img.shields.io/badge/Oriol_Vinyals-4-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#oriol-vinyals)
- [![](https://img.shields.io/badge/Carlo_Ciliberto-4-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#carlo-ciliberto)
- [![](https://img.shields.io/badge/Massimiliano_Pontil-4-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#massimiliano-pontil)
- [![](https://img.shields.io/badge/Yi_Yang-4-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#yi-yang)
- [![](https://img.shields.io/badge/Huajun_Chen-4-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#huajun-chen)
- [![](https://img.shields.io/badge/Shumin_Deng-4-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#shumin-deng)
- [![](https://img.shields.io/badge/Xiang_Chen-4-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#xiang-chen)
- [![](https://img.shields.io/badge/Ningyu_Zhang-4-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#ningyu-zhang)
- [![](https://img.shields.io/badge/Richard_S._Zemel-3-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#richard-s.-zemel)
- [![](https://img.shields.io/badge/Eunho_Yang-3-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#eunho-yang)
- [![](https://img.shields.io/badge/Kevin_Swersky-3-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#kevin-swersky)
- [![](https://img.shields.io/badge/Sergey_Levine-3-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#sergey-levine)
- [![](https://img.shields.io/badge/Zhenhui_Li-3-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#zhenhui-li)
- [![](https://img.shields.io/badge/Mengye_Ren-3-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#mengye-ren)
- [![](https://img.shields.io/badge/Tom_Goldstein-3-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#tom-goldstein)
- [![](https://img.shields.io/badge/Micah_Goldblum-3-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#micah-goldblum)
- [![](https://img.shields.io/badge/Richard_Zemel-3-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#richard-zemel)
- [![](https://img.shields.io/badge/Sameer_Singh-3-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#sameer-singh)
- [![](https://img.shields.io/badge/Dan_Klein-3-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#dan-klein)
- [![](https://img.shields.io/badge/Eric_Wallace-3-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#eric-wallace)
- [![](https://img.shields.io/badge/Hae_Beom_Lee-3-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#hae-beom-lee)
- [![](https://img.shields.io/badge/Juanzi_Li-3-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#juanzi-li)
- [![](https://img.shields.io/badge/Ivan_Vuli{\'c}-3-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#ivan-vuli{\'c})
- [![](https://img.shields.io/badge/Mengjie_Zhao-3-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#mengjie-zhao)
- [![](https://img.shields.io/badge/Hao_Zhu-3-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#hao-zhu)
- [![](https://img.shields.io/badge/Ahmed_Hassan_Awadallah-3-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#ahmed-hassan-awadallah)
- [![](https://img.shields.io/badge/Arzoo_Katiyar-3-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#arzoo-katiyar)
- [![](https://img.shields.io/badge/S._M._Ali_Eslami-3-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#s.-m.-ali-eslami)
- [![](https://img.shields.io/badge/Xiantong_Zhen-3-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#xiantong-zhen)
- [![](https://img.shields.io/badge/Pieter_Abbeel-3-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#pieter-abbeel)
- [![](https://img.shields.io/badge/Yu_Xiong_Wang-3-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#yu-xiong-wang)
- [![](https://img.shields.io/badge/Fei_Huang-3-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#fei-huang)
- [![](https://img.shields.io/badge/Chuanqi_Tan-3-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#chuanqi-tan)
- [![](https://img.shields.io/badge/Mohit_Bansal-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#mohit-bansal)
- [![](https://img.shields.io/badge/Colin_Raffel-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#colin-raffel)
- [![](https://img.shields.io/badge/Derek_Tam-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#derek-tam)
- [![](https://img.shields.io/badge/Sebastian_Riedel-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#sebastian-riedel)
- [![](https://img.shields.io/badge/Hinrich_Sch{\"{u}}tze-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#hinrich-sch{\"{u}}tze)
- [![](https://img.shields.io/badge/Hannaneh_Hajishirzi-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#hannaneh-hajishirzi)
- [![](https://img.shields.io/badge/Luheng_He-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#luheng-he)
- [![](https://img.shields.io/badge/Michael_Zeng-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#michael-zeng)
- [![](https://img.shields.io/badge/Chenguang_Zhu-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#chenguang-zhu)
- [![](https://img.shields.io/badge/Wenpeng_Yin-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#wenpeng-yin)
- [![](https://img.shields.io/badge/Jianguo_Zhang-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#jianguo-zhang)
- [![](https://img.shields.io/badge/Tingwen_Liu-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#tingwen-liu)
- [![](https://img.shields.io/badge/Percy_Liang-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#percy-liang)
- [![](https://img.shields.io/badge/Ting_Liu-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#ting-liu)
- [![](https://img.shields.io/badge/Yongkui_Lai-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#yongkui-lai)
- [![](https://img.shields.io/badge/Wanxiang_Che-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#wanxiang-che)
- [![](https://img.shields.io/badge/Yutai_Hou-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#yutai-hou)
- [![](https://img.shields.io/badge/Matthew_Henderson-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#matthew-henderson)
- [![](https://img.shields.io/badge/Zhengbao_Jiang-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#zhengbao-jiang)
- [![](https://img.shields.io/badge/Jason_Eisner-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#jason-eisner)
- [![](https://img.shields.io/badge/Boris_N._Oreshkin-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#boris-n.-oreshkin)
- [![](https://img.shields.io/badge/Giulia_Denevi-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#giulia-denevi)
- [![](https://img.shields.io/badge/Qianru_Sun-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#qianru-sun)
- [![](https://img.shields.io/badge/Hanwang_Zhang-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#hanwang-zhang)
- [![](https://img.shields.io/badge/Jake_Snell-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#jake-snell)
- [![](https://img.shields.io/badge/Sachin_Ravi-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#sachin-ravi)
- [![](https://img.shields.io/badge/Jia_Bin_Huang-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#jia-bin-huang)
- [![](https://img.shields.io/badge/Minseop_Park-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#minseop-park)
- [![](https://img.shields.io/badge/Saehoon_Kim-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#saehoon-kim)
- [![](https://img.shields.io/badge/Tsendsuren_Munkhdalai-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#tsendsuren-munkhdalai)
- [![](https://img.shields.io/badge/Junzhou_Huang-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#junzhou-huang)
- [![](https://img.shields.io/badge/Jaekyun_Moon-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#jaekyun-moon)
- [![](https://img.shields.io/badge/Jun_Seo-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#jun-seo)
- [![](https://img.shields.io/badge/Sung_Whan_Yoon-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#sung-whan-yoon)
- [![](https://img.shields.io/badge/Liam_Fowl-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#liam-fowl)
- [![](https://img.shields.io/badge/Richard_Turner-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#richard-turner)
- [![](https://img.shields.io/badge/Sebastian_Nowozin-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#sebastian-nowozin)
- [![](https://img.shields.io/badge/Jonathan_Gordon-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#jonathan-gordon)
- [![](https://img.shields.io/badge/John_Bronskill-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#john-bronskill)
- [![](https://img.shields.io/badge/Jason_D._Lee-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#jason-d.-lee)
- [![](https://img.shields.io/badge/Lu_Liu-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#lu-liu)
- [![](https://img.shields.io/badge/Mubarak_Shah-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#mubarak-shah)
- [![](https://img.shields.io/badge/Feiyue_Huang-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#feiyue-huang)
- [![](https://img.shields.io/badge/Yanwei_Fu-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#yanwei-fu)
- [![](https://img.shields.io/badge/Lars_Petersson-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#lars-petersson)
- [![](https://img.shields.io/badge/Mehrtash_Harandi-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#mehrtash-harandi)
- [![](https://img.shields.io/badge/Pengfei_Fang-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#pengfei-fang)
- [![](https://img.shields.io/badge/Bharath_Hariharan-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#bharath-hariharan)
- [![](https://img.shields.io/badge/Jose_Dolz-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#jose-dolz)
- [![](https://img.shields.io/badge/Ismail_Ben_Ayed-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#ismail-ben-ayed)
- [![](https://img.shields.io/badge/Pablo_Piantanida-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#pablo-piantanida)
- [![](https://img.shields.io/badge/Malik_Boudiaf-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#malik-boudiaf)
- [![](https://img.shields.io/badge/Jian_Sun-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#jian-sun)
- [![](https://img.shields.io/badge/Trevor_Darrell-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#trevor-darrell)
- [![](https://img.shields.io/badge/Wei_Hu-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#wei-hu)
- [![](https://img.shields.io/badge/Renkun_Ni-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#renkun-ni)
- [![](https://img.shields.io/badge/Tuo_Zhao-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#tuo-zhao)
- [![](https://img.shields.io/badge/Vincent_Dumoulin-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#vincent-dumoulin)
- [![](https://img.shields.io/badge/Hang_Gao-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#hang-gao)
- [![](https://img.shields.io/badge/Danqi_Chen-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#danqi-chen)
- [![](https://img.shields.io/badge/Adam_Fisch-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#adam-fisch)
- [![](https://img.shields.io/badge/Pengjun_Xie-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#pengjun-xie)
- [![](https://img.shields.io/badge/Yulin_Chen-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#yulin-chen)
- [![](https://img.shields.io/badge/Guangwei_Xu-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#guangwei-xu)
- [![](https://img.shields.io/badge/Anna_Korhonen-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#anna-korhonen)
- [![](https://img.shields.io/badge/Roi_Reichart-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#roi-reichart)
- [![](https://img.shields.io/badge/Ehsan_Shareghi-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#ehsan-shareghi)
- [![](https://img.shields.io/badge/Yi_Zhu-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#yi-zhu)
- [![](https://img.shields.io/badge/Leyu_Lin-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#leyu-lin)
- [![](https://img.shields.io/badge/Yuan_Yao-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#yuan-yao)
- [![](https://img.shields.io/badge/Yaqing_Wang-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#yaqing-wang)
- [![](https://img.shields.io/badge/Liwei_Wang-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#liwei-wang)
- [![](https://img.shields.io/badge/Aoxue_Li-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#aoxue-li)
- [![](https://img.shields.io/badge/Lu_Zhang-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#lu-zhang)
- [![](https://img.shields.io/badge/Gholamreza_Haffari-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#gholamreza-haffari)
- [![](https://img.shields.io/badge/Guilin_Qi-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#guilin-qi)
- [![](https://img.shields.io/badge/Tongtong_Wu-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#tongtong-wu)
- [![](https://img.shields.io/badge/Jianfeng_Gao-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#jianfeng-gao)
- [![](https://img.shields.io/badge/Saghar_Hosseini-1-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#saghar-hosseini)
- [![](https://img.shields.io/badge/Minlie_Huang-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#minlie-huang)
- [![](https://img.shields.io/badge/Yuxian_Gu-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#yuxian-gu)
- [![](https://img.shields.io/badge/Shengding_Hu-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#shengding-hu)
- [![](https://img.shields.io/badge/Zhilin_Yang-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#zhilin-yang)
- [![](https://img.shields.io/badge/Jie_Tang-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#jie-tang)
- [![](https://img.shields.io/badge/Yanan_Zheng-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#yanan-zheng)
- [![](https://img.shields.io/badge/Yu_Cheng-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#yu-cheng)
- [![](https://img.shields.io/badge/Ruiyi_Zhang-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#ruiyi-zhang)
- [![](https://img.shields.io/badge/Luke_Zettlemoyer-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#luke-zettlemoyer)
- [![](https://img.shields.io/badge/Xiao_Ming_Wu-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#xiao-ming-wu)
- [![](https://img.shields.io/badge/Li_Ming_Zhan-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#li-ming-zhan)
- [![](https://img.shields.io/badge/Jiaxin_Chen-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#jiaxin-chen)
- [![](https://img.shields.io/badge/Khoi_Nguyen-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#khoi-nguyen)
- [![](https://img.shields.io/badge/Felix_Hill-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#felix-hill)
- [![](https://img.shields.io/badge/Serkan_Cabi-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#serkan-cabi)
- [![](https://img.shields.io/badge/Jacob_Menick-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#jacob-menick)
- [![](https://img.shields.io/badge/Maria_Tsimpoukelli-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#maria-tsimpoukelli)
- [![](https://img.shields.io/badge/Iz_Beltagy-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#iz-beltagy)
- [![](https://img.shields.io/badge/Kyle_Lo-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#kyle-lo)
- [![](https://img.shields.io/badge/Arman_Cohan-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#arman-cohan)
- [![](https://img.shields.io/badge/Jonathan_Bragg-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#jonathan-bragg)
- [![](https://img.shields.io/badge/Qiang_Qiu-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#qiang-qiu)
- [![](https://img.shields.io/badge/Ruohan_Wang-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#ruohan-wang)
- [![](https://img.shields.io/badge/James_T._Kwok-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#james-t.-kwok)
- [![](https://img.shields.io/badge/Feng_Liu-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#feng-liu)
- [![](https://img.shields.io/badge/Kyunghyun_Cho-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#kyunghyun-cho)
- [![](https://img.shields.io/badge/Douwe_Kiela-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#douwe-kiela)
- [![](https://img.shields.io/badge/Ethan_Perez-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#ethan-perez)
- [![](https://img.shields.io/badge/Jiashi_Feng-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#jiashi-feng)
- [![](https://img.shields.io/badge/Zhenguo_Li-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#zhenguo-li)
- [![](https://img.shields.io/badge/Ling_Shao-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#ling-shao)
- [![](https://img.shields.io/badge/Shafiq_Joty-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#shafiq-joty)
- [![](https://img.shields.io/badge/Chengwei_Qin-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#chengwei-qin)
- [![](https://img.shields.io/badge/Jacob_Andreas-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#jacob-andreas)
- [![](https://img.shields.io/badge/Zhen_Bi-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#zhen-bi)
- [![](https://img.shields.io/badge/Luoqiu_Li-2-green)](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author/README.md#luoqiu-li)
## Hyperlink 
- [[Overview]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/README.md) -- [Homepage](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/README.md)
- [[NLP]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4nlp/./)  [[CV]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4cv/./) -- [Summary](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/./)
- [[NLP]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4nlp/application)  [[CV]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4cv/application) -- [Application](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/application)
- [[NLP]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4nlp/approach)  [[CV]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4cv/approach) -- [Approach](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/approach)
- [[NLP]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4nlp/author)  [[CV]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4cv/author) -- [Author](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/author)
- [[NLP]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4nlp/backbone_model)  [[CV]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4cv/backbone_model) -- [Backbone Model](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/backbone_model)
- [[NLP]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4nlp/contribution)  [[CV]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4cv/contribution) -- [Contribution](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/contribution)
- [[NLP]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4nlp/dataset)  [[CV]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4cv/dataset) -- [Dataset](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/dataset)
- [[NLP]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4nlp/metrics)  [[CV]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4cv/metrics) -- [Metrics](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/metrics)
- [[NLP]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4nlp/research_question)  [[CV]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4cv/research_question) -- [Research Questions](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/research_question)
- [[NLP]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4nlp/setting)  [[CV]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4cv/setting) -- [Setting](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/setting)
- [[NLP]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4nlp/supervision)  [[CV]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4cv/supervision) -- [ Learning Paradigm](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/supervision)
- [[NLP]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4nlp/time)  [[CV]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4cv/time) -- [Published Time](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/time)
- [[NLP]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4nlp/venue)  [[CV]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4cv/venue) -- [Published Venue](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/fsl4all/venue)

## Zhiyuan Liu

- [![](https://img.shields.io/badge/ACL-2022-green)](https://aclanthology.org/2022.acl-long.483)<a href="https://scholar.google.com.hk/scholar?q=Prototypical+Verbalizer+for+Prompt-based+Few-shot+Tuning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Prototypical Verbalizer for Prompt-based Few-shot Tuning**](https://aclanthology.org/2022.acl-long.483) , <br> by *Cui, Ganqu  and
Hu, Shengding  and
Ding, Ning  and
Huang, Longtao  and
Liu, Zhiyuan* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L399-L411) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```cui-etal-2022-prototypical```
- [![](https://img.shields.io/badge/ACL-2022-green)](https://aclanthology.org/2022.acl-long.576)<a href="https://scholar.google.com.hk/scholar?q=PPT:+Pre-trained+Prompt+Tuning+for+Few-shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**PPT: Pre-trained Prompt Tuning for Few-shot Learning**](https://aclanthology.org/2022.acl-long.576) , <br> by *Gu, Yuxian  and
Han, Xu  and
Liu, Zhiyuan  and
Huang, Minlie* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L438-L449) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```gu-etal-2022-ppt```
- [![](https://img.shields.io/badge/ACL-2021-green)](https://aclanthology.org/2021.acl-long.248)<a href="https://scholar.google.com.hk/scholar?q=Few-NERD:+A+Few-shot+Named+Entity+Recognition+Dataset"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-NERD: A Few-shot Named Entity Recognition Dataset**](https://aclanthology.org/2021.acl-long.248) , <br> by *Ding, Ning  and
Xu, Guangwei  and
Chen, Yulin  and
Wang, Xiaobin  and
Han, Xu  and
Xie, Pengjun  and
Zheng, Haitao  and
Liu, Zhiyuan* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L856-L870) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ding-etal-2021-nerd```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2105.11259)<a href="https://scholar.google.com.hk/scholar?q=PTR:+Prompt+Tuning+with+Rules+for+Text+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**PTR: Prompt Tuning with Rules for Text Classification**](https://arxiv.org/abs/2105.11259) , <br> by *Xu Han and
Weilin Zhao and
Ning Ding and
Zhiyuan Liu and
Maosong Sun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2923-L2934) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2105-11259```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2108.02035)<a href="https://scholar.google.com.hk/scholar?q=Knowledgeable+Prompt-tuning:+Incorporating+Knowledge+into+Prompt+Verbalizer+for+Text+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer
for Text Classification**](https://arxiv.org/abs/2108.02035) , <br> by *Shengding Hu and
Ning Ding and
Huadong Wang and
Zhiyuan Liu and
Juanzi Li and
Maosong Sun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3177-L3190) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2108-02035```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2109.04332)<a href="https://scholar.google.com.hk/scholar?q=PPT:+Pre-trained+Prompt+Tuning+for+Few-shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**PPT: Pre-trained Prompt Tuning for Few-shot Learning**](https://arxiv.org/abs/2109.04332) , <br> by *Yuxian Gu, Xu Han, Zhiyuan Liu and Minlie Huang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3222-L3229) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```gu2021ppt```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2108.10604)<a href="https://scholar.google.com.hk/scholar?q=Prompt-Learning+for+Fine-Grained+Entity+Typing"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Prompt-Learning for Fine-Grained Entity Typing**](https://arxiv.org/abs/2108.10604) , <br> by *Ning Ding and
Yulin Chen and
Xu Han and
Guangwei Xu and
Pengjun Xie and
Hai{-}Tao Zheng and
Zhiyuan Liu and
Juanzi Li and
Hong{-}Gee Kim* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3231-L3246) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2108-10604```
- [![](https://img.shields.io/badge/AAAI-2020-green)](https://aaai.org/ojs/index.php/AAAI/article/view/6281)<a href="https://scholar.google.com.hk/scholar?q=Neural+Snowball+for+Few-Shot+Relation+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Neural Snowball for Few-Shot Relation Learning**](https://aaai.org/ojs/index.php/AAAI/article/view/6281) , <br> by *Tianyu Gao and
Xu Han and
Ruobing Xie and
Zhiyuan Liu and
Fen Lin and
Leyu Lin and
Maosong Sun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L765-L778) <br>```Neural Snowball
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```GaoHX0LLS20```
- [![](https://img.shields.io/badge/EMNLP-2019-green)](https://doi.org/10.18653/v1/D19-1649)<a href="https://scholar.google.com.hk/scholar?q=FewRel+2.0:+Towards+More+Challenging+Few-Shot+Relation+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**FewRel 2.0: Towards More Challenging Few-Shot Relation Classification**](https://doi.org/10.18653/v1/D19-1649) , <br> by *Tianyu Gao and
Xu Han and
Hao Zhu and
Zhiyuan Liu and
Peng Li and
Maosong Sun and
Jie Zhou* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L723-L736) <br>```Fewrel 2.0 dataset
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```GaoHZLLSZ19```
- [![](https://img.shields.io/badge/AAAI-2019-green)](https://doi.org/10.1609/aaai.v33i01.33016407)<a href="https://scholar.google.com.hk/scholar?q=Hybrid+Attention-Based+Prototypical+Networks+for+Noisy+Few-Shot+Relation+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Hybrid Attention-Based Prototypical Networks for Noisy Few-Shot Relation
Classification**](https://doi.org/10.1609/aaai.v33i01.33016407) , <br> by *Tianyu Gao and
Xu Han and
Zhiyuan Liu and
Maosong Sun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L751-L762) <br>```HATT
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```GaoH0S19```
- [![](https://img.shields.io/badge/EMNLP-2018-green)](https://doi.org/10.18653/v1/d18-1514)<a href="https://scholar.google.com.hk/scholar?q=FewRel:+A+Large-Scale+Supervised+Few-shot+Relation+Classification+Dataset+with+State-of-the-Art+Evaluation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**FewRel: A Large-Scale Supervised Few-shot Relation Classification
Dataset with State-of-the-Art Evaluation**](https://doi.org/10.18653/v1/d18-1514) , <br> by *Xu Han and
Hao Zhu and
Pengfei Yu and
Ziyun Wang and
Yuan Yao and
Zhiyuan Liu and
Maosong Sun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L706-L720) <br>```FewRel dataset
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```HanZYWYLS18```
## Xu Han

- [![](https://img.shields.io/badge/ACL-2022-green)](https://aclanthology.org/2022.acl-long.576)<a href="https://scholar.google.com.hk/scholar?q=PPT:+Pre-trained+Prompt+Tuning+for+Few-shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**PPT: Pre-trained Prompt Tuning for Few-shot Learning**](https://aclanthology.org/2022.acl-long.576) , <br> by *Gu, Yuxian  and
Han, Xu  and
Liu, Zhiyuan  and
Huang, Minlie* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L438-L449) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```gu-etal-2022-ppt```
- [![](https://img.shields.io/badge/ACL-2021-green)](https://aclanthology.org/2021.acl-long.248)<a href="https://scholar.google.com.hk/scholar?q=Few-NERD:+A+Few-shot+Named+Entity+Recognition+Dataset"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-NERD: A Few-shot Named Entity Recognition Dataset**](https://aclanthology.org/2021.acl-long.248) , <br> by *Ding, Ning  and
Xu, Guangwei  and
Chen, Yulin  and
Wang, Xiaobin  and
Han, Xu  and
Xie, Pengjun  and
Zheng, Haitao  and
Liu, Zhiyuan* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L856-L870) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ding-etal-2021-nerd```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2105.11259)<a href="https://scholar.google.com.hk/scholar?q=PTR:+Prompt+Tuning+with+Rules+for+Text+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**PTR: Prompt Tuning with Rules for Text Classification**](https://arxiv.org/abs/2105.11259) , <br> by *Xu Han and
Weilin Zhao and
Ning Ding and
Zhiyuan Liu and
Maosong Sun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2923-L2934) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2105-11259```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2109.04332)<a href="https://scholar.google.com.hk/scholar?q=PPT:+Pre-trained+Prompt+Tuning+for+Few-shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**PPT: Pre-trained Prompt Tuning for Few-shot Learning**](https://arxiv.org/abs/2109.04332) , <br> by *Yuxian Gu, Xu Han, Zhiyuan Liu and Minlie Huang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3222-L3229) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```gu2021ppt```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2108.10604)<a href="https://scholar.google.com.hk/scholar?q=Prompt-Learning+for+Fine-Grained+Entity+Typing"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Prompt-Learning for Fine-Grained Entity Typing**](https://arxiv.org/abs/2108.10604) , <br> by *Ning Ding and
Yulin Chen and
Xu Han and
Guangwei Xu and
Pengjun Xie and
Hai{-}Tao Zheng and
Zhiyuan Liu and
Juanzi Li and
Hong{-}Gee Kim* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3231-L3246) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2108-10604```
- [![](https://img.shields.io/badge/AAAI-2020-green)](https://aaai.org/ojs/index.php/AAAI/article/view/6281)<a href="https://scholar.google.com.hk/scholar?q=Neural+Snowball+for+Few-Shot+Relation+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Neural Snowball for Few-Shot Relation Learning**](https://aaai.org/ojs/index.php/AAAI/article/view/6281) , <br> by *Tianyu Gao and
Xu Han and
Ruobing Xie and
Zhiyuan Liu and
Fen Lin and
Leyu Lin and
Maosong Sun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L765-L778) <br>```Neural Snowball
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```GaoHX0LLS20```
- [![](https://img.shields.io/badge/EMNLP-2019-green)](https://doi.org/10.18653/v1/D19-1649)<a href="https://scholar.google.com.hk/scholar?q=FewRel+2.0:+Towards+More+Challenging+Few-Shot+Relation+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**FewRel 2.0: Towards More Challenging Few-Shot Relation Classification**](https://doi.org/10.18653/v1/D19-1649) , <br> by *Tianyu Gao and
Xu Han and
Hao Zhu and
Zhiyuan Liu and
Peng Li and
Maosong Sun and
Jie Zhou* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L723-L736) <br>```Fewrel 2.0 dataset
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```GaoHZLLSZ19```
- [![](https://img.shields.io/badge/AAAI-2019-green)](https://doi.org/10.1609/aaai.v33i01.33016407)<a href="https://scholar.google.com.hk/scholar?q=Hybrid+Attention-Based+Prototypical+Networks+for+Noisy+Few-Shot+Relation+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Hybrid Attention-Based Prototypical Networks for Noisy Few-Shot Relation
Classification**](https://doi.org/10.1609/aaai.v33i01.33016407) , <br> by *Tianyu Gao and
Xu Han and
Zhiyuan Liu and
Maosong Sun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L751-L762) <br>```HATT
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```GaoH0S19```
- [![](https://img.shields.io/badge/EMNLP-2018-green)](https://doi.org/10.18653/v1/d18-1514)<a href="https://scholar.google.com.hk/scholar?q=FewRel:+A+Large-Scale+Supervised+Few-shot+Relation+Classification+Dataset+with+State-of-the-Art+Evaluation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**FewRel: A Large-Scale Supervised Few-shot Relation Classification
Dataset with State-of-the-Art Evaluation**](https://doi.org/10.18653/v1/d18-1514) , <br> by *Xu Han and
Hao Zhu and
Pengfei Yu and
Ziyun Wang and
Yuan Yao and
Zhiyuan Liu and
Maosong Sun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L706-L720) <br>```FewRel dataset
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```HanZYWYLS18```
## Philip Yu

- [![](https://img.shields.io/badge/SIGIR-2021-green)](https://doi.org/10.1145/3404835.3462995)<a href="https://scholar.google.com.hk/scholar?q=Pseudo+Siamese+Network+for+Few-Shot+Intent+Generation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Pseudo Siamese Network for Few-Shot Intent Generation**](https://doi.org/10.1145/3404835.3462995) , <br> by *Xia, Congying, Xiong, Caiming and Yu, Philip* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L819-L826) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```3404835.3462995```
- [![](https://img.shields.io/badge/NAACL_HLT-2021-green)](https://www.aclweb.org/anthology/2021.naacl-main.106)<a href="https://scholar.google.com.hk/scholar?q=Incremental+Few-shot+Text+Classification+with+Multi-round+New+Classes:+Formulation,+Dataset+and+System"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Incremental Few-shot Text Classification with Multi-round New Classes: Formulation, Dataset and System**](https://www.aclweb.org/anthology/2021.naacl-main.106) , <br> by *Xia, Congying  and
Yin, Wenpeng  and
Feng, Yihao  and
Yu, Philip* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2786-L2796) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```xia-etal-2021-incremental```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2109.06349)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Intent+Detection+via+Contrastive+Pre-Training+and+Fine-Tuning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Intent Detection via Contrastive Pre-Training and Fine-Tuning**](https://arxiv.org/abs/2109.06349) , <br> by *Jianguo Zhang, Trung Bui, Seunghyun Yoon, Xiang Chen, Zhiwei Liu, Congying Xia, Quan Hung Tran, Walter Chang and Philip Yu* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3279-L3286) <br>```EMNLP 2021
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhang2021fewshot```
- [![](https://img.shields.io/badge/EMNLP-2020-green)](https://www.aclweb.org/anthology/2020.emnlp-main.411)<a href="https://scholar.google.com.hk/scholar?q=Discriminative+Nearest+Neighbor+Few-Shot+Intent+Detection+by+Transferring+Natural+Language+Inference"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Discriminative Nearest Neighbor Few-Shot Intent Detection by Transferring Natural Language Inference**](https://www.aclweb.org/anthology/2020.emnlp-main.411) , <br> by *Zhang, Jianguo  and
Hashimoto, Kazuma  and
Liu, Wenhao  and
Wu, Chien-Sheng  and
Wan, Yao  and
Yu, Philip  and
Socher, Richard  and
Xiong, Caiming* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2666-L2680) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhang-etal-2020-discriminative```
- [![](https://img.shields.io/badge/EMNLP_Findings-2020-green)](https://www.aclweb.org/anthology/2020.findings-emnlp.108)<a href="https://scholar.google.com.hk/scholar?q=Dynamic+Semantic+Matching+and+Aggregation+Network+for+Few-shot+Intent+Detection"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Dynamic Semantic Matching and Aggregation Network for Few-shot Intent Detection**](https://www.aclweb.org/anthology/2020.findings-emnlp.108) , <br> by *Nguyen, Hoang  and
Zhang, Chenwei  and
Xia, Congying  and
Yu, Philip* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2747-L2757) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```nguyen-etal-2020-dynamic```
- [![](https://img.shields.io/badge/EMNLP_Findings-2020-green)](https://www.aclweb.org/anthology/2020.findings-emnlp.303)<a href="https://scholar.google.com.hk/scholar?q=Composed+Variational+Natural+Language+Generation+for+Few-shot+Intents"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Composed Variational Natural Language Generation for Few-shot Intents**](https://www.aclweb.org/anthology/2020.findings-emnlp.303) , <br> by *Xia, Congying  and
Xiong, Caiming  and
Yu, Philip  and
Socher, Richard* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2759-L2769) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```xia-etal-2020-composed```
## Caiming Xiong

- [![](https://img.shields.io/badge/SIGIR-2021-green)](https://doi.org/10.1145/3404835.3462995)<a href="https://scholar.google.com.hk/scholar?q=Pseudo+Siamese+Network+for+Few-Shot+Intent+Generation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Pseudo Siamese Network for Few-Shot Intent Generation**](https://doi.org/10.1145/3404835.3462995) , <br> by *Xia, Congying, Xiong, Caiming and Yu, Philip* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L819-L826) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```3404835.3462995```
- [![](https://img.shields.io/badge/ICML-2021-green)](http://proceedings.mlr.press/v139/bai21a.html)<a href="https://scholar.google.com.hk/scholar?q=How+Important+is+the+Train-Validation+Split+in+Meta-Learning?"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**How Important is the Train-Validation Split in Meta-Learning?**](http://proceedings.mlr.press/v139/bai21a.html) , <br> by *Bai, Yu, Chen, Minshuo, Zhou, Pan, Zhao, Tuo, Lee, Jason, Kakade, Sham, Wang, Huan and Xiong, Caiming* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1035-L1042) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v139-bai21a```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/4b86ca48d90bd5f0978afa3a012503a4-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Online+Structured+Meta-learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Online Structured Meta-learning**](https://proceedings.neurips.cc/paper/2020/hash/4b86ca48d90bd5f0978afa3a012503a4-Abstract.html) , <br> by *Huaxiu Yao and
Yingbo Zhou and
Mehrdad Mahdavi and
Zhenhui Li and
Richard Socher and
Caiming Xiong* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2093-L2104) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```YaoZMLSX20```
- [![](https://img.shields.io/badge/EMNLP-2020-green)](https://www.aclweb.org/anthology/2020.emnlp-main.411)<a href="https://scholar.google.com.hk/scholar?q=Discriminative+Nearest+Neighbor+Few-Shot+Intent+Detection+by+Transferring+Natural+Language+Inference"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Discriminative Nearest Neighbor Few-Shot Intent Detection by Transferring Natural Language Inference**](https://www.aclweb.org/anthology/2020.emnlp-main.411) , <br> by *Zhang, Jianguo  and
Hashimoto, Kazuma  and
Liu, Wenhao  and
Wu, Chien-Sheng  and
Wan, Yao  and
Yu, Philip  and
Socher, Richard  and
Xiong, Caiming* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2666-L2680) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhang-etal-2020-discriminative```
- [![](https://img.shields.io/badge/EMNLP-2020-green)](https://www.aclweb.org/anthology/2020.emnlp-main.660)<a href="https://scholar.google.com.hk/scholar?q=Universal+Natural+Language+Processing+with+Limited+Annotations:+Try+Few-shot+Textual+Entailment+as+a+Start"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Universal Natural Language Processing with Limited Annotations: Try Few-shot Textual Entailment as a Start**](https://www.aclweb.org/anthology/2020.emnlp-main.660) , <br> by *Yin, Wenpeng  and
Rajani, Nazneen Fatema  and
Radev, Dragomir  and
Socher, Richard  and
Xiong, Caiming* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2719-L2730) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```yin-etal-2020-universal```
- [![](https://img.shields.io/badge/EMNLP_Findings-2020-green)](https://www.aclweb.org/anthology/2020.findings-emnlp.303)<a href="https://scholar.google.com.hk/scholar?q=Composed+Variational+Natural+Language+Generation+for+Few-shot+Intents"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Composed Variational Natural Language Generation for Few-shot Intents**](https://www.aclweb.org/anthology/2020.findings-emnlp.303) , <br> by *Xia, Congying  and
Xiong, Caiming  and
Yu, Philip  and
Socher, Richard* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2759-L2769) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```xia-etal-2020-composed```
## Maosong Sun

- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2105.11259)<a href="https://scholar.google.com.hk/scholar?q=PTR:+Prompt+Tuning+with+Rules+for+Text+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**PTR: Prompt Tuning with Rules for Text Classification**](https://arxiv.org/abs/2105.11259) , <br> by *Xu Han and
Weilin Zhao and
Ning Ding and
Zhiyuan Liu and
Maosong Sun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2923-L2934) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2105-11259```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2108.02035)<a href="https://scholar.google.com.hk/scholar?q=Knowledgeable+Prompt-tuning:+Incorporating+Knowledge+into+Prompt+Verbalizer+for+Text+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer
for Text Classification**](https://arxiv.org/abs/2108.02035) , <br> by *Shengding Hu and
Ning Ding and
Huadong Wang and
Zhiyuan Liu and
Juanzi Li and
Maosong Sun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3177-L3190) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2108-02035```
- [![](https://img.shields.io/badge/AAAI-2020-green)](https://aaai.org/ojs/index.php/AAAI/article/view/6281)<a href="https://scholar.google.com.hk/scholar?q=Neural+Snowball+for+Few-Shot+Relation+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Neural Snowball for Few-Shot Relation Learning**](https://aaai.org/ojs/index.php/AAAI/article/view/6281) , <br> by *Tianyu Gao and
Xu Han and
Ruobing Xie and
Zhiyuan Liu and
Fen Lin and
Leyu Lin and
Maosong Sun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L765-L778) <br>```Neural Snowball
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```GaoHX0LLS20```
- [![](https://img.shields.io/badge/EMNLP-2019-green)](https://doi.org/10.18653/v1/D19-1649)<a href="https://scholar.google.com.hk/scholar?q=FewRel+2.0:+Towards+More+Challenging+Few-Shot+Relation+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**FewRel 2.0: Towards More Challenging Few-Shot Relation Classification**](https://doi.org/10.18653/v1/D19-1649) , <br> by *Tianyu Gao and
Xu Han and
Hao Zhu and
Zhiyuan Liu and
Peng Li and
Maosong Sun and
Jie Zhou* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L723-L736) <br>```Fewrel 2.0 dataset
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```GaoHZLLSZ19```
- [![](https://img.shields.io/badge/AAAI-2019-green)](https://doi.org/10.1609/aaai.v33i01.33016407)<a href="https://scholar.google.com.hk/scholar?q=Hybrid+Attention-Based+Prototypical+Networks+for+Noisy+Few-Shot+Relation+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Hybrid Attention-Based Prototypical Networks for Noisy Few-Shot Relation
Classification**](https://doi.org/10.1609/aaai.v33i01.33016407) , <br> by *Tianyu Gao and
Xu Han and
Zhiyuan Liu and
Maosong Sun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L751-L762) <br>```HATT
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```GaoH0S19```
- [![](https://img.shields.io/badge/EMNLP-2018-green)](https://doi.org/10.18653/v1/d18-1514)<a href="https://scholar.google.com.hk/scholar?q=FewRel:+A+Large-Scale+Supervised+Few-shot+Relation+Classification+Dataset+with+State-of-the-Art+Evaluation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**FewRel: A Large-Scale Supervised Few-shot Relation Classification
Dataset with State-of-the-Art Evaluation**](https://doi.org/10.18653/v1/d18-1514) , <br> by *Xu Han and
Hao Zhu and
Pengfei Yu and
Ziyun Wang and
Yuan Yao and
Zhiyuan Liu and
Maosong Sun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L706-L720) <br>```FewRel dataset
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```HanZYWYLS18```
## Chelsea Finn

- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2106.02695)<a href="https://scholar.google.com.hk/scholar?q=Meta-Learning+with+Fewer+Tasks+through+Task+Interpolation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta-Learning with Fewer Tasks through Task Interpolation**](https://arxiv.org/abs/2106.02695) , <br> by *Huaxiu Yao, Linjun Zhang and Chelsea Finn* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3059-L3066) <br>```NeurIPS under-review
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```yao2021metalearning```
- [![](https://img.shields.io/badge/ICLR-2020-green)](https://openreview.net/forum?id=BklEFpEYwS)<a href="https://scholar.google.com.hk/scholar?q=Meta-Learning+without+Memorization"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta-Learning without Memorization**](https://openreview.net/forum?id=BklEFpEYwS) , <br> by *Mingzhang Yin, George Tucker, Mingyuan Zhou, Sergey Levine and Chelsea Finn* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1722-L1728) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Yin2020Meta-Learning```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/cc3f5463bc4d26bc38eadc8bcffbc654-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Continuous+Meta-Learning+without+Tasks"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Continuous Meta-Learning without Tasks**](https://proceedings.neurips.cc/paper/2020/hash/cc3f5463bc4d26bc38eadc8bcffbc654-Abstract.html) , <br> by *James Harrison and
Apoorva Sharma and
Chelsea Finn and
Marco Pavone* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2149-L2158) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```HarrisonSFP20```
- [![](https://img.shields.io/badge/ICLR-2018-green)](https://openreview.net/forum?id=HyjC5yWCW)<a href="https://scholar.google.com.hk/scholar?q=Meta-Learning+and+Universality:+Deep+Representations+and+Gradient+Descent+can+Approximate+any+Learning+Algorithm"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm**](https://openreview.net/forum?id=HyjC5yWCW) , <br> by *Chelsea Finn and Sergey Levine* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1885-L1891) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```finn2018metalearning```
- [![](https://img.shields.io/badge/ICML-2017-green)](http://proceedings.mlr.press/v70/finn17a.html)<a href="https://scholar.google.com.hk/scholar?q=Model-Agnostic+Meta-Learning+for+Fast+Adaptation+of+Deep+Networks"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks**](http://proceedings.mlr.press/v70/finn17a.html) , <br> by *Chelsea Finn, Pieter Abbeel and Sergey Levine* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1654-L1661) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v70-finn17a```
## Huaxiu Yao

- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2106.02695)<a href="https://scholar.google.com.hk/scholar?q=Meta-Learning+with+Fewer+Tasks+through+Task+Interpolation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta-Learning with Fewer Tasks through Task Interpolation**](https://arxiv.org/abs/2106.02695) , <br> by *Huaxiu Yao, Linjun Zhang and Chelsea Finn* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3059-L3066) <br>```NeurIPS under-review
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```yao2021metalearning```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2109.04707)<a href="https://scholar.google.com.hk/scholar?q=Knowledge-Aware+Meta-learning+for+Low-Resource+Text+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Knowledge-Aware Meta-learning for Low-Resource Text Classification**](https://arxiv.org/abs/2109.04707) , <br> by *Huaxiu Yao, Yingxin Wu, Maruan Al-Shedivat and Eric P. Xing* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3269-L3276) <br>```EMNLP 2021
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```yao2021knowledgeaware```
- [![](https://img.shields.io/badge/ICLR-2020-green)](https://openreview.net/forum?id=rklp93EtwH)<a href="https://scholar.google.com.hk/scholar?q=Automated+Relational+Meta-learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Automated Relational Meta-learning**](https://openreview.net/forum?id=rklp93EtwH) , <br> by *Huaxiu Yao, Xian Wu, Zhiqiang Tao, Yaliang Li, Bolin Ding, Ruirui Li and Zhenhui Li* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1673-L1679) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Yao2020Automated```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/4b86ca48d90bd5f0978afa3a012503a4-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Online+Structured+Meta-learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Online Structured Meta-learning**](https://proceedings.neurips.cc/paper/2020/hash/4b86ca48d90bd5f0978afa3a012503a4-Abstract.html) , <br> by *Huaxiu Yao and
Yingbo Zhou and
Mehrdad Mahdavi and
Zhenhui Li and
Richard Socher and
Caiming Xiong* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2093-L2104) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```YaoZMLSX20```
- [![](https://img.shields.io/badge/ICML-2019-green)](
http://proceedings.mlr.press/v97/yao19b.html)<a href="https://scholar.google.com.hk/scholar?q=Hierarchically+Structured+Meta-learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Hierarchically Structured Meta-learning**](
http://proceedings.mlr.press/v97/yao19b.html) , <br> by *Yao, Huaxiu, Wei, Ying, Huang, Junzhou and Li, Zhenhui* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1591-L1600) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v97-yao19b```
## Hugo Larochelle

- [![](https://img.shields.io/badge/ICML-2021-green)](http://proceedings.mlr.press/v139/triantafillou21a.html)<a href="https://scholar.google.com.hk/scholar?q=Learning+a+Universal+Template+for+Few-shot+Dataset+Generalization"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning a Universal Template for Few-shot Dataset Generalization**](http://proceedings.mlr.press/v139/triantafillou21a.html) , <br> by *Triantafillou, Eleni, Larochelle, Hugo, Zemel, Richard and Dumoulin, Vincent* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1015-L1022) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v139-triantafillou21a```
- [![](https://img.shields.io/badge/ICLR-2021-green)](https://openreview.net/forum?id=04cII6MumYV)<a href="https://scholar.google.com.hk/scholar?q=A+Universal+Representation+Transformer+Layer+for+Few-Shot+Image+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**A Universal Representation Transformer Layer for Few-Shot Image Classification**](https://openreview.net/forum?id=04cII6MumYV) , <br> by *Lu Liu, William L. Hamilton, Guodong Long, Jing Jiang and Hugo Larochelle* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1377-L1383) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```liu2021a```
- [![](https://img.shields.io/badge/ICLR-2020-green)](https://openreview.net/forum?id=rkgAGAVKPr)<a href="https://scholar.google.com.hk/scholar?q=Meta-Dataset:+A+Dataset+of+Datasets+for+Learning+to+Learn+from+Few+Examples"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples**](https://openreview.net/forum?id=rkgAGAVKPr) , <br> by *Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol and Hugo Larochelle* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1692-L1698) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Triantafillou2020Meta-Dataset```
- [![](https://img.shields.io/badge/ICLR-2018-green)](https://openreview.net/forum?id=HJcSzz-CZ)<a href="https://scholar.google.com.hk/scholar?q=Meta-Learning+for+Semi-Supervised+Few-Shot+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta-Learning for Semi-Supervised Few-Shot Classification**](https://openreview.net/forum?id=HJcSzz-CZ) , <br> by *Mengye Ren, Sachin Ravi, Eleni Triantafillou, Jake Snell, Kevin Swersky, Josh B. Tenenbaum, Hugo Larochelle and Richard S. Zemel* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1876-L1882) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ren2018metalearning```
- [![](https://img.shields.io/badge/ICLR-2017-green)](https://openreview.net/forum?id=rJY0-Kcll)<a href="https://scholar.google.com.hk/scholar?q=Optimization+as+a+Model+for+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Optimization as a Model for Few-Shot Learning**](https://openreview.net/forum?id=rJY0-Kcll) , <br> by *Sachin Ravi and
Hugo Larochelle* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1902-L1909) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```RaviL17```
## Sung Ju Hwang

- [![](https://img.shields.io/badge/ICML-2021-green)](http://proceedings.mlr.press/v139/shin21a.html)<a href="https://scholar.google.com.hk/scholar?q=Large-Scale+Meta-Learning+with+Continual+Trajectory+Shifting"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Large-Scale Meta-Learning with Continual Trajectory Shifting**](http://proceedings.mlr.press/v139/shin21a.html) , <br> by *Shin, Jaewoong, Lee, Hae Beom, Gong, Boqing and Hwang, Sung Ju* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L974-L981) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v139-shin21a```
- [![](https://img.shields.io/badge/ICML-2020-green)](http://proceedings.mlr.press/v119/park20b.html)<a href="https://scholar.google.com.hk/scholar?q=Meta+Variance+Transfer:+Learning+to+Augment+from+the+Others"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta Variance Transfer: Learning to Augment from the Others**](http://proceedings.mlr.press/v119/park20b.html) , <br> by *Park, Seong-Jin, Han, Seungju, Baek, Ji-Won, Kim, Insoo, Song, Juhwan, Lee, Hae Beom, Han, Jae-Joon and Hwang, Sung Ju* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1514-L1521) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v119-park20b```
- [![](https://img.shields.io/badge/ICLR-2020-green)](https://openreview.net/forum?id=rkeZIJBYvr)<a href="https://scholar.google.com.hk/scholar?q=Learning+to+Balance:+Bayesian+Meta-Learning+for+Imbalanced+and+Out-of-distribution+Tasks"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning to Balance: Bayesian Meta-Learning for Imbalanced and Out-of-distribution Tasks**](https://openreview.net/forum?id=rkeZIJBYvr) , <br> by *Hae Beom Lee, Hayeon Lee, Donghyun Na, Saehoon Kim, Minseop Park, Eunho Yang and Sung Ju Hwang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1740-L1746) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Lee2020Learning```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/0663a4ddceacb40b095eda264a85f15c-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Learning+to+Extrapolate+Knowledge:+Transductive+Few-shot+Out-of-Graph+Link+Prediction"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning to Extrapolate Knowledge: Transductive Few-shot Out-of-Graph
Link Prediction**](https://proceedings.neurips.cc/paper/2020/hash/0663a4ddceacb40b095eda264a85f15c-Abstract.html) , <br> by *Jinheon Baek and
Dong Bok Lee and
Sung Ju Hwang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1911-L1920) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```BaekLH20```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/c39e1a03859f9ee215bc49131d0caf33-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Few-shot+Visual+Reasoning+with+Meta-Analogical+Contrastive+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-shot Visual Reasoning with Meta-Analogical Contrastive Learning**](https://proceedings.neurips.cc/paper/2020/hash/c39e1a03859f9ee215bc49131d0caf33-Abstract.html) , <br> by *Youngsung Kim and
Jinwoo Shin and
Eunho Yang and
Sung Ju Hwang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1994-L2003) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```KimSYH20```
## Congying Xia

- [![](https://img.shields.io/badge/SIGIR-2021-green)](https://doi.org/10.1145/3404835.3462995)<a href="https://scholar.google.com.hk/scholar?q=Pseudo+Siamese+Network+for+Few-Shot+Intent+Generation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Pseudo Siamese Network for Few-Shot Intent Generation**](https://doi.org/10.1145/3404835.3462995) , <br> by *Xia, Congying, Xiong, Caiming and Yu, Philip* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L819-L826) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```3404835.3462995```
- [![](https://img.shields.io/badge/NAACL_HLT-2021-green)](https://www.aclweb.org/anthology/2021.naacl-main.106)<a href="https://scholar.google.com.hk/scholar?q=Incremental+Few-shot+Text+Classification+with+Multi-round+New+Classes:+Formulation,+Dataset+and+System"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Incremental Few-shot Text Classification with Multi-round New Classes: Formulation, Dataset and System**](https://www.aclweb.org/anthology/2021.naacl-main.106) , <br> by *Xia, Congying  and
Yin, Wenpeng  and
Feng, Yihao  and
Yu, Philip* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2786-L2796) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```xia-etal-2021-incremental```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2109.06349)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Intent+Detection+via+Contrastive+Pre-Training+and+Fine-Tuning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Intent Detection via Contrastive Pre-Training and Fine-Tuning**](https://arxiv.org/abs/2109.06349) , <br> by *Jianguo Zhang, Trung Bui, Seunghyun Yoon, Xiang Chen, Zhiwei Liu, Congying Xia, Quan Hung Tran, Walter Chang and Philip Yu* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3279-L3286) <br>```EMNLP 2021
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhang2021fewshot```
- [![](https://img.shields.io/badge/EMNLP_Findings-2020-green)](https://www.aclweb.org/anthology/2020.findings-emnlp.108)<a href="https://scholar.google.com.hk/scholar?q=Dynamic+Semantic+Matching+and+Aggregation+Network+for+Few-shot+Intent+Detection"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Dynamic Semantic Matching and Aggregation Network for Few-shot Intent Detection**](https://www.aclweb.org/anthology/2020.findings-emnlp.108) , <br> by *Nguyen, Hoang  and
Zhang, Chenwei  and
Xia, Congying  and
Yu, Philip* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2747-L2757) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```nguyen-etal-2020-dynamic```
- [![](https://img.shields.io/badge/EMNLP_Findings-2020-green)](https://www.aclweb.org/anthology/2020.findings-emnlp.303)<a href="https://scholar.google.com.hk/scholar?q=Composed+Variational+Natural+Language+Generation+for+Few-shot+Intents"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Composed Variational Natural Language Generation for Few-shot Intents**](https://www.aclweb.org/anthology/2020.findings-emnlp.303) , <br> by *Xia, Congying  and
Xiong, Caiming  and
Yu, Philip  and
Socher, Richard* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2759-L2769) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```xia-etal-2020-composed```
## Tianyu Gao

- [![](https://img.shields.io/badge/ACL-2021-green)](https://aclanthology.org/2021.acl-long.295)<a href="https://scholar.google.com.hk/scholar?q=Making+Pre-trained+Language+Models+Better+Few-shot+Learners"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Making Pre-trained Language Models Better Few-shot Learners**](https://aclanthology.org/2021.acl-long.295) , <br> by *Gao, Tianyu  and
Fisch, Adam  and
Chen, Danqi* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L872-L881) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```gao-etal-2021-making```
- [![](https://img.shields.io/badge/AAAI-2020-green)](https://aaai.org/ojs/index.php/AAAI/article/view/6281)<a href="https://scholar.google.com.hk/scholar?q=Neural+Snowball+for+Few-Shot+Relation+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Neural Snowball for Few-Shot Relation Learning**](https://aaai.org/ojs/index.php/AAAI/article/view/6281) , <br> by *Tianyu Gao and
Xu Han and
Ruobing Xie and
Zhiyuan Liu and
Fen Lin and
Leyu Lin and
Maosong Sun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L765-L778) <br>```Neural Snowball
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```GaoHX0LLS20```
- [![](https://img.shields.io/badge/ICML-2020-green)](http://proceedings.mlr.press/v119/qu20a.html)<a href="https://scholar.google.com.hk/scholar?q=Few-shot+Relation+Extraction+via+Bayesian+Meta-learning+on+Relation+Graphs"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-shot Relation Extraction via Bayesian Meta-learning on Relation Graphs**](http://proceedings.mlr.press/v119/qu20a.html) , <br> by *Qu, Meng, Gao, Tianyu, Xhonneux, Louis-Pascal and Tang, Jian* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1524-L1531) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v119-qu20a```
- [![](https://img.shields.io/badge/EMNLP-2019-green)](https://doi.org/10.18653/v1/D19-1649)<a href="https://scholar.google.com.hk/scholar?q=FewRel+2.0:+Towards+More+Challenging+Few-Shot+Relation+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**FewRel 2.0: Towards More Challenging Few-Shot Relation Classification**](https://doi.org/10.18653/v1/D19-1649) , <br> by *Tianyu Gao and
Xu Han and
Hao Zhu and
Zhiyuan Liu and
Peng Li and
Maosong Sun and
Jie Zhou* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L723-L736) <br>```Fewrel 2.0 dataset
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```GaoHZLLSZ19```
- [![](https://img.shields.io/badge/AAAI-2019-green)](https://doi.org/10.1609/aaai.v33i01.33016407)<a href="https://scholar.google.com.hk/scholar?q=Hybrid+Attention-Based+Prototypical+Networks+for+Noisy+Few-Shot+Relation+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Hybrid Attention-Based Prototypical Networks for Noisy Few-Shot Relation
Classification**](https://doi.org/10.1609/aaai.v33i01.33016407) , <br> by *Tianyu Gao and
Xu Han and
Zhiyuan Liu and
Maosong Sun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L751-L762) <br>```HATT
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```GaoH0S19```
## Timo Schick

- [![](https://img.shields.io/badge/Proceeding_of_EMNLP-2021-green)](https://aclanthology.org/2021.emnlp-main.32)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Text+Generation+with+Natural+Language+Instructions"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Text Generation with Natural Language Instructions**](https://aclanthology.org/2021.emnlp-main.32) , <br> by *Schick, Timo  and
Sch{\"u}tze, Hinrich* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L482-L490) <br>```GenPET, few-shot instruction-based generation.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```schick-schutze-2021-shot```
- [![](https://img.shields.io/badge/EACL-2021-green)](https://www.aclweb.org/anthology/2021.eacl-main.20)<a href="https://scholar.google.com.hk/scholar?q=Exploiting+Cloze-Questions+for+Few-Shot+Text+Classification+and+Natural+Language+Inference"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference**](https://www.aclweb.org/anthology/2021.eacl-main.20) , <br> by *Schick, Timo and
Sch{\"u}tze, Hinrich* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2430-L2438) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```schick-schutze-2021-exploiting```
- [![](https://img.shields.io/badge/NAACL_HLT-2021-green)](https://www.aclweb.org/anthology/2021.naacl-main.185)<a href="https://scholar.google.com.hk/scholar?q=It's+Not+Just+Size+That+Matters:+Small+Language+Models+Are+Also+Few-Shot+Learners"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners**](https://www.aclweb.org/anthology/2021.naacl-main.185) , <br> by *Schick, Timo and
Sch{\"u}tze, Hinrich* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2451-L2459) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```schick-schutze-2021-just```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2104.07540)<a href="https://scholar.google.com.hk/scholar?q=Generating+Datasets+with+Pretrained+Language+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Generating Datasets with Pretrained Language Models**](https://arxiv.org/abs/2104.07540) , <br> by *Timo Schick and
Hinrich Sch{\"{u}}tze* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2970-L2978) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2104-07540```
- [![](https://img.shields.io/badge/CoRR-2020-green)](https://arxiv.org/abs/2012.11926)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Text+Generation+with+Pattern-Exploiting+Training"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Text Generation with Pattern-Exploiting Training**](https://arxiv.org/abs/2012.11926) , <br> by *Timo Schick and
Hinrich Sch{\"{u\textsl{}}}tze* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2900-L2908) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2012-11926```
## Ning Ding

- [![](https://img.shields.io/badge/ACL-2022-green)](https://aclanthology.org/2022.acl-long.483)<a href="https://scholar.google.com.hk/scholar?q=Prototypical+Verbalizer+for+Prompt-based+Few-shot+Tuning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Prototypical Verbalizer for Prompt-based Few-shot Tuning**](https://aclanthology.org/2022.acl-long.483) , <br> by *Cui, Ganqu  and
Hu, Shengding  and
Ding, Ning  and
Huang, Longtao  and
Liu, Zhiyuan* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L399-L411) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```cui-etal-2022-prototypical```
- [![](https://img.shields.io/badge/ACL-2021-green)](https://aclanthology.org/2021.acl-long.248)<a href="https://scholar.google.com.hk/scholar?q=Few-NERD:+A+Few-shot+Named+Entity+Recognition+Dataset"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-NERD: A Few-shot Named Entity Recognition Dataset**](https://aclanthology.org/2021.acl-long.248) , <br> by *Ding, Ning  and
Xu, Guangwei  and
Chen, Yulin  and
Wang, Xiaobin  and
Han, Xu  and
Xie, Pengjun  and
Zheng, Haitao  and
Liu, Zhiyuan* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L856-L870) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ding-etal-2021-nerd```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2105.11259)<a href="https://scholar.google.com.hk/scholar?q=PTR:+Prompt+Tuning+with+Rules+for+Text+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**PTR: Prompt Tuning with Rules for Text Classification**](https://arxiv.org/abs/2105.11259) , <br> by *Xu Han and
Weilin Zhao and
Ning Ding and
Zhiyuan Liu and
Maosong Sun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2923-L2934) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2105-11259```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2108.02035)<a href="https://scholar.google.com.hk/scholar?q=Knowledgeable+Prompt-tuning:+Incorporating+Knowledge+into+Prompt+Verbalizer+for+Text+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer
for Text Classification**](https://arxiv.org/abs/2108.02035) , <br> by *Shengding Hu and
Ning Ding and
Huadong Wang and
Zhiyuan Liu and
Juanzi Li and
Maosong Sun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3177-L3190) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2108-02035```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2108.10604)<a href="https://scholar.google.com.hk/scholar?q=Prompt-Learning+for+Fine-Grained+Entity+Typing"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Prompt-Learning for Fine-Grained Entity Typing**](https://arxiv.org/abs/2108.10604) , <br> by *Ning Ding and
Yulin Chen and
Xu Han and
Guangwei Xu and
Pengjun Xie and
Hai{-}Tao Zheng and
Zhiyuan Liu and
Juanzi Li and
Hong{-}Gee Kim* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3231-L3246) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2108-10604```
## Richard Socher

- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/4b86ca48d90bd5f0978afa3a012503a4-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Online+Structured+Meta-learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Online Structured Meta-learning**](https://proceedings.neurips.cc/paper/2020/hash/4b86ca48d90bd5f0978afa3a012503a4-Abstract.html) , <br> by *Huaxiu Yao and
Yingbo Zhou and
Mehrdad Mahdavi and
Zhenhui Li and
Richard Socher and
Caiming Xiong* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2093-L2104) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```YaoZMLSX20```
- [![](https://img.shields.io/badge/EMNLP-2020-green)](https://www.aclweb.org/anthology/2020.emnlp-main.411)<a href="https://scholar.google.com.hk/scholar?q=Discriminative+Nearest+Neighbor+Few-Shot+Intent+Detection+by+Transferring+Natural+Language+Inference"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Discriminative Nearest Neighbor Few-Shot Intent Detection by Transferring Natural Language Inference**](https://www.aclweb.org/anthology/2020.emnlp-main.411) , <br> by *Zhang, Jianguo  and
Hashimoto, Kazuma  and
Liu, Wenhao  and
Wu, Chien-Sheng  and
Wan, Yao  and
Yu, Philip  and
Socher, Richard  and
Xiong, Caiming* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2666-L2680) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhang-etal-2020-discriminative```
- [![](https://img.shields.io/badge/EMNLP-2020-green)](https://www.aclweb.org/anthology/2020.emnlp-main.660)<a href="https://scholar.google.com.hk/scholar?q=Universal+Natural+Language+Processing+with+Limited+Annotations:+Try+Few-shot+Textual+Entailment+as+a+Start"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Universal Natural Language Processing with Limited Annotations: Try Few-shot Textual Entailment as a Start**](https://www.aclweb.org/anthology/2020.emnlp-main.660) , <br> by *Yin, Wenpeng  and
Rajani, Nazneen Fatema  and
Radev, Dragomir  and
Socher, Richard  and
Xiong, Caiming* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2719-L2730) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```yin-etal-2020-universal```
- [![](https://img.shields.io/badge/EMNLP_Findings-2020-green)](https://www.aclweb.org/anthology/2020.findings-emnlp.303)<a href="https://scholar.google.com.hk/scholar?q=Composed+Variational+Natural+Language+Generation+for+Few-shot+Intents"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Composed Variational Natural Language Generation for Few-shot Intents**](https://www.aclweb.org/anthology/2020.findings-emnlp.303) , <br> by *Xia, Congying  and
Xiong, Caiming  and
Yu, Philip  and
Socher, Richard* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2759-L2769) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```xia-etal-2020-composed```
## Eleni Triantafillou

- [![](https://img.shields.io/badge/ICML-2021-green)](http://proceedings.mlr.press/v139/triantafillou21a.html)<a href="https://scholar.google.com.hk/scholar?q=Learning+a+Universal+Template+for+Few-shot+Dataset+Generalization"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning a Universal Template for Few-shot Dataset Generalization**](http://proceedings.mlr.press/v139/triantafillou21a.html) , <br> by *Triantafillou, Eleni, Larochelle, Hugo, Zemel, Richard and Dumoulin, Vincent* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1015-L1022) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v139-triantafillou21a```
- [![](https://img.shields.io/badge/ICLR-2020-green)](https://openreview.net/forum?id=rkgAGAVKPr)<a href="https://scholar.google.com.hk/scholar?q=Meta-Dataset:+A+Dataset+of+Datasets+for+Learning+to+Learn+from+Few+Examples"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples**](https://openreview.net/forum?id=rkgAGAVKPr) , <br> by *Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol and Hugo Larochelle* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1692-L1698) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Triantafillou2020Meta-Dataset```
- [![](https://img.shields.io/badge/ICLR-2018-green)](https://openreview.net/forum?id=HJcSzz-CZ)<a href="https://scholar.google.com.hk/scholar?q=Meta-Learning+for+Semi-Supervised+Few-Shot+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta-Learning for Semi-Supervised Few-Shot Classification**](https://openreview.net/forum?id=HJcSzz-CZ) , <br> by *Mengye Ren, Sachin Ravi, Eleni Triantafillou, Jake Snell, Kevin Swersky, Josh B. Tenenbaum, Hugo Larochelle and Richard S. Zemel* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1876-L1882) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ren2018metalearning```
- [![](https://img.shields.io/badge/NeurIPS-2017-green)](https://proceedings.neurips.cc/paper/2017/hash/01e9565cecc4e989123f9620c1d09c09-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Learning+Through+an+Information+Retrieval+Lens"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Learning Through an Information Retrieval Lens**](https://proceedings.neurips.cc/paper/2017/hash/01e9565cecc4e989123f9620c1d09c09-Abstract.html) , <br> by *Eleni Triantafillou and
Richard S. Zemel and
Raquel Urtasun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2340-L2349) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```TriantafillouZU17```
## Graham Neubig

- [![](https://img.shields.io/badge/ICML-2021-green)](http://proceedings.mlr.press/v139/zhu21d.html)<a href="https://scholar.google.com.hk/scholar?q=Few-shot+Language+Coordination+by+Modeling+Theory+of+Mind"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-shot Language Coordination by Modeling Theory of Mind**](http://proceedings.mlr.press/v139/zhu21d.html) , <br> by *Zhu, Hao, Neubig, Graham and Bisk, Yonatan* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L984-L991) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v139-zhu21d```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2107.13586)<a href="https://scholar.google.com.hk/scholar?q=Pre-train,+Prompt,+and+Predict:+A+Systematic+Survey+of+Prompting+Methods+in+Natural+Language+Processing"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing**](https://arxiv.org/abs/2107.13586) , <br> by *Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi and Graham Neubig* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3112-L3120) <br>```Prompt-based learning -- survey paper
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```liu2021pretrain```
- [![](https://img.shields.io/badge/Trans._Assoc._Comput._Linguistics-2020-green)](https://transacl.org/ojs/index.php/tacl/article/view/1983)<a href="https://scholar.google.com.hk/scholar?q=How+Can+We+Know+What+Language+Models+Know"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**How Can We Know What Language Models Know**](https://transacl.org/ojs/index.php/tacl/article/view/1983) , <br> by *Zhengbao Jiang and
Frank F. Xu and
Jun Araki and
Graham Neubig* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2461-L2472) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```JiangXAN20```
- [![](https://img.shields.io/badge/ACL-2020-green)](https://www.aclweb.org/anthology/2020.acl-main.722)<a href="https://scholar.google.com.hk/scholar?q=Soft+Gazetteers+for+Low-Resource+Named+Entity+Recognition"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Soft Gazetteers for Low-Resource Named Entity Recognition**](https://www.aclweb.org/anthology/2020.acl-main.722) , <br> by *Rijhwani, Shruti  and
Zhou, Shuyan  and
Neubig, Graham  and
Carbonell, Jaime* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2601-L2611) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```rijhwani-etal-2020-soft```
## Hinrich Sch{\"u}tze

- [![](https://img.shields.io/badge/Proceeding_of_EMNLP-2021-green)](https://aclanthology.org/2021.emnlp-main.32)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Text+Generation+with+Natural+Language+Instructions"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Text Generation with Natural Language Instructions**](https://aclanthology.org/2021.emnlp-main.32) , <br> by *Schick, Timo  and
Sch{\"u}tze, Hinrich* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L482-L490) <br>```GenPET, few-shot instruction-based generation.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```schick-schutze-2021-shot```
- [![](https://img.shields.io/badge/ACL-2021-green)](https://aclanthology.org/2021.acl-long.447)<a href="https://scholar.google.com.hk/scholar?q=A+Closer+Look+at+Few-Shot+Crosslingual+Transfer:+The+Choice+of+Shots+Matters"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**A Closer Look at Few-Shot Crosslingual Transfer: The Choice of Shots Matters**](https://aclanthology.org/2021.acl-long.447) , <br> by *Zhao, Mengjie  and
Zhu, Yi  and
Shareghi, Ehsan  and
Vuli{\'c}, Ivan  and
Reichart, Roi  and
Korhonen, Anna  and
Sch{\"u}tze, Hinrich* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L828-L841) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhao-etal-2021-closer```
- [![](https://img.shields.io/badge/EACL-2021-green)](https://www.aclweb.org/anthology/2021.eacl-main.20)<a href="https://scholar.google.com.hk/scholar?q=Exploiting+Cloze-Questions+for+Few-Shot+Text+Classification+and+Natural+Language+Inference"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference**](https://www.aclweb.org/anthology/2021.eacl-main.20) , <br> by *Schick, Timo and
Sch{\"u}tze, Hinrich* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2430-L2438) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```schick-schutze-2021-exploiting```
- [![](https://img.shields.io/badge/NAACL_HLT-2021-green)](https://www.aclweb.org/anthology/2021.naacl-main.185)<a href="https://scholar.google.com.hk/scholar?q=It's+Not+Just+Size+That+Matters:+Small+Language+Models+Are+Also+Few-Shot+Learners"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners**](https://www.aclweb.org/anthology/2021.naacl-main.185) , <br> by *Schick, Timo and
Sch{\"u}tze, Hinrich* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2451-L2459) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```schick-schutze-2021-just```
## Subhabrata Mukherjee

- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/3644a684f98ea8fe223c713b77189a77-Abstract-round2.html)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Learning+Evaluation+in+Natural+Language+Understanding"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Learning Evaluation in Natural Language Understanding**](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/3644a684f98ea8fe223c713b77189a77-Abstract-round2.html) , <br> by *Subhabrata Mukherjee and
Xiaodong Liu and
Guoqing Zheng and
Saghar Hosseini and
Saghar Hosseini and
Hao Cheng and
Ge Yang and
Christopher Meek and
Ahmed Hassan Awadallah and
Jianfeng Gao* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L464-L479) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```MukherjeeLZHH0Y21```
- [![](https://img.shields.io/badge/KDD-2021-green)](https://doi.org/10.1145/3447548.3467235)<a href="https://scholar.google.com.hk/scholar?q=Meta+Self-Training+for+Few-Shot+Neural+Sequence+Labeling"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta Self-Training for Few-Shot Neural Sequence Labeling**](https://doi.org/10.1145/3447548.3467235) , <br> by *Wang, Yaqing, Mukherjee, Subhabrata, Chu, Haoda, Tu, Yuancheng, Wu, Ming, Gao, Jing and Awadallah, Ahmed Hassan* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L792-L799) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```3447548.3467235```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2109.08259)<a href="https://scholar.google.com.hk/scholar?q=Self-training+with+Few-shot+Rationalization:+Teacher+Explanations+Aid+Student+in+Few-shot+NLU"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Self-training with Few-shot Rationalization: Teacher Explanations Aid Student in Few-shot NLU**](https://arxiv.org/abs/2109.08259) , <br> by *Meghana Moorthy Bhat, Alessandro Sordoni and Subhabrata Mukherjee* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3299-L3306) <br>```EMNLP 2021
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```bhat2021selftraining```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/f23d125da1e29e34c552f448610ff25f-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Uncertainty-aware+Self-training+for+Few-shot+Text+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Uncertainty-aware Self-training for Few-shot Text Classification**](https://proceedings.neurips.cc/paper/2020/hash/f23d125da1e29e34c552f448610ff25f-Abstract.html) , <br> by *Subhabrata Mukherjee and
Ahmed Hassan Awadallah* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2015-L2022) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```MukherjeeA20```
## Oriol Vinyals

- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/01b7575c38dac42f3cfb7d500438b875-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Multimodal+Few-Shot+Learning+with+Frozen+Language+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Multimodal Few-Shot Learning with Frozen Language Models**](https://proceedings.neurips.cc/paper/2021/hash/01b7575c38dac42f3cfb7d500438b875-Abstract.html) , <br> by *Maria Tsimpoukelli and
Jacob Menick and
Serkan Cabi and
S. M. Ali Eslami and
Oriol Vinyals and
Felix Hill* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L248-L260) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```TsimpoukelliMCE21```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2106.13884)<a href="https://scholar.google.com.hk/scholar?q=Multimodal+Few-Shot+Learning+with+Frozen+Language+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Multimodal Few-Shot Learning with Frozen Language Models**](https://arxiv.org/abs/2106.13884) , <br> by *Maria Tsimpoukelli and
Jacob Menick and
Serkan Cabi and
S. M. Ali Eslami and
Oriol Vinyals and
Felix Hill* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3154-L3166) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2106-13884```
- [![](https://img.shields.io/badge/ICLR-2018-green)](https://openreview.net/forum?id=r1wEFyWCW)<a href="https://scholar.google.com.hk/scholar?q=Few-shot+Autoregressive+Density+Estimation:+Towards+Learning+to+Learn+Distributions"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions**](https://openreview.net/forum?id=r1wEFyWCW) , <br> by *Scott Reed, Yutian Chen, Thomas Paine, Aäron van den Oord, S. M. Ali Eslami, Danilo Rezende, Oriol Vinyals and Nando de Freitas* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1858-L1864) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```reed2018fewshot```
- [![](https://img.shields.io/badge/NeurIPS-2016-green)](https://proceedings.neurips.cc/paper/2016/hash/90e1357833654983612fb05e3ec9148c-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Matching+Networks+for+One+Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Matching Networks for One Shot Learning**](https://proceedings.neurips.cc/paper/2016/hash/90e1357833654983612fb05e3ec9148c-Abstract.html) , <br> by *Oriol Vinyals and
Charles Blundell and
Tim Lillicrap and
Koray Kavukcuoglu and
Daan Wierstra* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L564-L575) <br>```MatchNet
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```VinyalsBLKW16```
## Carlo Ciliberto

- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/e3b6fb0fd4df098162eede3313c54a8d-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=The+Role+of+Global+Labels+in+Few-Shot+Classification+and+How+to+Infer+Them"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**The Role of Global Labels in Few-Shot Classification and How to Infer
Them**](https://proceedings.neurips.cc/paper/2021/hash/e3b6fb0fd4df098162eede3313c54a8d-Abstract.html) , <br> by *Ruohan Wang and
Massimiliano Pontil and
Carlo Ciliberto* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L181-L191) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```WangPC21```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/0a716fe8c7745e51a3185fc8be6ca23a-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=The+Advantage+of+Conditional+Meta-Learning+for+Biased+Regularization+and+Fine+Tuning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**The Advantage of Conditional Meta-Learning for Biased Regularization
and Fine Tuning**](https://proceedings.neurips.cc/paper/2020/hash/0a716fe8c7745e51a3185fc8be6ca23a-Abstract.html) , <br> by *Giulia Denevi and
Massimiliano Pontil and
Carlo Ciliberto* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2038-L2047) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```DeneviPC20```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/1b69ebedb522700034547abc5652ffac-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Structured+Prediction+for+Conditional+Meta-Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Structured Prediction for Conditional Meta-Learning**](https://proceedings.neurips.cc/paper/2020/hash/1b69ebedb522700034547abc5652ffac-Abstract.html) , <br> by *Ruohan Wang and
Yiannis Demiris and
Carlo Ciliberto* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2049-L2057) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```WangDC20```
- [![](https://img.shields.io/badge/NeurIPS-2018-green)](https://proceedings.neurips.cc/paper/2018/hash/b9a25e422ba96f7572089a00b838c3f8-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Learning+To+Learn+Around+A+Common+Mean"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning To Learn Around A Common Mean**](https://proceedings.neurips.cc/paper/2018/hash/b9a25e422ba96f7572089a00b838c3f8-Abstract.html) , <br> by *Giulia Denevi and
Carlo Ciliberto and
Dimitris Stamos and
Massimiliano Pontil* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2314-L2324) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```DeneviCSP18```
## Massimiliano Pontil

- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/e3b6fb0fd4df098162eede3313c54a8d-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=The+Role+of+Global+Labels+in+Few-Shot+Classification+and+How+to+Infer+Them"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**The Role of Global Labels in Few-Shot Classification and How to Infer
Them**](https://proceedings.neurips.cc/paper/2021/hash/e3b6fb0fd4df098162eede3313c54a8d-Abstract.html) , <br> by *Ruohan Wang and
Massimiliano Pontil and
Carlo Ciliberto* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L181-L191) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```WangPC21```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/0a716fe8c7745e51a3185fc8be6ca23a-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=The+Advantage+of+Conditional+Meta-Learning+for+Biased+Regularization+and+Fine+Tuning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**The Advantage of Conditional Meta-Learning for Biased Regularization
and Fine Tuning**](https://proceedings.neurips.cc/paper/2020/hash/0a716fe8c7745e51a3185fc8be6ca23a-Abstract.html) , <br> by *Giulia Denevi and
Massimiliano Pontil and
Carlo Ciliberto* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2038-L2047) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```DeneviPC20```
- [![](https://img.shields.io/badge/ICML-2018-green)](http://proceedings.mlr.press/v80/franceschi18a.html)<a href="https://scholar.google.com.hk/scholar?q=Bilevel+Programming+for+Hyperparameter+Optimization+and+Meta-Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Bilevel Programming for Hyperparameter Optimization and Meta-Learning**](http://proceedings.mlr.press/v80/franceschi18a.html) , <br> by *Franceschi, Luca, Frasconi, Paolo, Salzo, Saverio, Grazzi, Riccardo and Pontil, Massimiliano* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1634-L1641) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v80-franceschi18a```
- [![](https://img.shields.io/badge/NeurIPS-2018-green)](https://proceedings.neurips.cc/paper/2018/hash/b9a25e422ba96f7572089a00b838c3f8-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Learning+To+Learn+Around+A+Common+Mean"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning To Learn Around A Common Mean**](https://proceedings.neurips.cc/paper/2018/hash/b9a25e422ba96f7572089a00b838c3f8-Abstract.html) , <br> by *Giulia Denevi and
Carlo Ciliberto and
Dimitris Stamos and
Massimiliano Pontil* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2314-L2324) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```DeneviCSP18```
## Yi Yang

- [![](https://img.shields.io/badge/International_Conference_on_Learning_Representations-2022-green)](https://openreview.net/forum?id=H-iABMvzIc)<a href="https://scholar.google.com.hk/scholar?q=Switch+to+Generalize:+Domain-Switch+Learning+for+Cross-Domain+Few-Shot+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Switch to Generalize: Domain-Switch Learning for Cross-Domain Few-Shot Classification**](https://openreview.net/forum?id=H-iABMvzIc) , <br> by *Zhengdong Hu, Yifan Sun and Yi Yang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L47-L53) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```hu2022switch```
- [![](https://img.shields.io/badge/KDD-2021-green)](https://doi.org/10.1145/3447548.3467438)<a href="https://scholar.google.com.hk/scholar?q=Knowledge-Enhanced+Domain+Adaptation+in+Few-Shot+Relation+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Knowledge-Enhanced Domain Adaptation in Few-Shot Relation Classification**](https://doi.org/10.1145/3447548.3467438) , <br> by *Zhang, Jiawen, Zhu, Jiaqi, Yang, Yi, Shi, Wandong, Zhang, Congcong and Wang, Hongan* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L783-L790) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```3447548.3467438```
- [![](https://img.shields.io/badge/EMNLP-2020-green)](https://www.aclweb.org/anthology/2020.emnlp-main.516)<a href="https://scholar.google.com.hk/scholar?q=Simple+and+Effective+Few-Shot+Named+Entity+Recognition+with+Structured+Nearest+Neighbor+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Simple and Effective Few-Shot Named Entity Recognition with Structured Nearest Neighbor Learning**](https://www.aclweb.org/anthology/2020.emnlp-main.516) , <br> by *Yang, Yi  and
Katiyar, Arzoo* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2695-L2703) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```yang-katiyar-2020-simple```
- [![](https://img.shields.io/badge/ICLR-2019-green)](https://openreview.net/forum?id=SyVuRiC5K7)<a href="https://scholar.google.com.hk/scholar?q=LEARNING+TO+PROPAGATE+LABELS:+TRANSDUCTIVE+PROPAGATION+NETWORK+FOR+FEW-SHOT+LEARNING"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING**](https://openreview.net/forum?id=SyVuRiC5K7) , <br> by *Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, Eunho Yang, Sungju Hwang and Yi Yang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1822-L1828) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```liu2018learning```
## Huajun Chen

- [![](https://img.shields.io/badge/International_Conference_on_Learning_Representations-2022-green)](https://openreview.net/forum?id=ek9a0qIafW)<a href="https://scholar.google.com.hk/scholar?q=Differentiable+Prompt+Makes+Pre-trained+Language+Models+Better+Few-shot+Learners"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners**](https://openreview.net/forum?id=ek9a0qIafW) , <br> by *Ningyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang and Huajun Chen* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2-L8) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhang2022differentiable```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2104.07650)<a href="https://scholar.google.com.hk/scholar?q=AdaPrompt:+Adaptive+Prompt-based+Finetuning+for+Relation+Extraction"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**AdaPrompt: Adaptive Prompt-based Finetuning for Relation Extraction**](https://arxiv.org/abs/2104.07650) , <br> by *Xiang Chen and
Xin Xie and
Ningyu Zhang and
Jiahuan Yan and
Shumin Deng and
Chuanqi Tan and
Fei Huang and
Luo Si and
Huajun Chen* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2857-L2872) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2104-07650```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2108.13161)<a href="https://scholar.google.com.hk/scholar?q=Differentiable+Prompt+Makes+Pre-trained+Language+Models+Better+Few-shot+Learners"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners**](https://arxiv.org/abs/2108.13161) , <br> by *Ningyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang and Huajun Chen* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3204-L3211) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhang2021differentiable```
- [![](https://img.shields.io/badge/COLING-2020-green)](https://doi.org/10.18653/v1/2020.coling-main.563)<a href="https://scholar.google.com.hk/scholar?q=Bridging+Text+and+Knowledge+with+Multi-Prototype+Embedding+for+Few-Shot+Relational+Triple+Extraction"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Bridging Text and Knowledge with Multi-Prototype Embedding for Few-Shot
Relational Triple Extraction**](https://doi.org/10.18653/v1/2020.coling-main.563) , <br> by *Haiyang Yu and
Ningyu Zhang and
Shumin Deng and
Hongbin Ye and
Wei Zhang and
Huajun Chen* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L691-L704) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```YuZDYZC20```
## Shumin Deng

- [![](https://img.shields.io/badge/International_Conference_on_Learning_Representations-2022-green)](https://openreview.net/forum?id=ek9a0qIafW)<a href="https://scholar.google.com.hk/scholar?q=Differentiable+Prompt+Makes+Pre-trained+Language+Models+Better+Few-shot+Learners"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners**](https://openreview.net/forum?id=ek9a0qIafW) , <br> by *Ningyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang and Huajun Chen* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2-L8) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhang2022differentiable```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2104.07650)<a href="https://scholar.google.com.hk/scholar?q=AdaPrompt:+Adaptive+Prompt-based+Finetuning+for+Relation+Extraction"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**AdaPrompt: Adaptive Prompt-based Finetuning for Relation Extraction**](https://arxiv.org/abs/2104.07650) , <br> by *Xiang Chen and
Xin Xie and
Ningyu Zhang and
Jiahuan Yan and
Shumin Deng and
Chuanqi Tan and
Fei Huang and
Luo Si and
Huajun Chen* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2857-L2872) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2104-07650```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2108.13161)<a href="https://scholar.google.com.hk/scholar?q=Differentiable+Prompt+Makes+Pre-trained+Language+Models+Better+Few-shot+Learners"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners**](https://arxiv.org/abs/2108.13161) , <br> by *Ningyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang and Huajun Chen* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3204-L3211) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhang2021differentiable```
- [![](https://img.shields.io/badge/COLING-2020-green)](https://doi.org/10.18653/v1/2020.coling-main.563)<a href="https://scholar.google.com.hk/scholar?q=Bridging+Text+and+Knowledge+with+Multi-Prototype+Embedding+for+Few-Shot+Relational+Triple+Extraction"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Bridging Text and Knowledge with Multi-Prototype Embedding for Few-Shot
Relational Triple Extraction**](https://doi.org/10.18653/v1/2020.coling-main.563) , <br> by *Haiyang Yu and
Ningyu Zhang and
Shumin Deng and
Hongbin Ye and
Wei Zhang and
Huajun Chen* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L691-L704) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```YuZDYZC20```
## Xiang Chen

- [![](https://img.shields.io/badge/International_Conference_on_Learning_Representations-2022-green)](https://openreview.net/forum?id=ek9a0qIafW)<a href="https://scholar.google.com.hk/scholar?q=Differentiable+Prompt+Makes+Pre-trained+Language+Models+Better+Few-shot+Learners"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners**](https://openreview.net/forum?id=ek9a0qIafW) , <br> by *Ningyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang and Huajun Chen* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2-L8) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhang2022differentiable```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2104.07650)<a href="https://scholar.google.com.hk/scholar?q=AdaPrompt:+Adaptive+Prompt-based+Finetuning+for+Relation+Extraction"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**AdaPrompt: Adaptive Prompt-based Finetuning for Relation Extraction**](https://arxiv.org/abs/2104.07650) , <br> by *Xiang Chen and
Xin Xie and
Ningyu Zhang and
Jiahuan Yan and
Shumin Deng and
Chuanqi Tan and
Fei Huang and
Luo Si and
Huajun Chen* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2857-L2872) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2104-07650```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2108.13161)<a href="https://scholar.google.com.hk/scholar?q=Differentiable+Prompt+Makes+Pre-trained+Language+Models+Better+Few-shot+Learners"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners**](https://arxiv.org/abs/2108.13161) , <br> by *Ningyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang and Huajun Chen* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3204-L3211) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhang2021differentiable```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2109.06349)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Intent+Detection+via+Contrastive+Pre-Training+and+Fine-Tuning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Intent Detection via Contrastive Pre-Training and Fine-Tuning**](https://arxiv.org/abs/2109.06349) , <br> by *Jianguo Zhang, Trung Bui, Seunghyun Yoon, Xiang Chen, Zhiwei Liu, Congying Xia, Quan Hung Tran, Walter Chang and Philip Yu* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3279-L3286) <br>```EMNLP 2021
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhang2021fewshot```
## Ningyu Zhang

- [![](https://img.shields.io/badge/International_Conference_on_Learning_Representations-2022-green)](https://openreview.net/forum?id=ek9a0qIafW)<a href="https://scholar.google.com.hk/scholar?q=Differentiable+Prompt+Makes+Pre-trained+Language+Models+Better+Few-shot+Learners"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners**](https://openreview.net/forum?id=ek9a0qIafW) , <br> by *Ningyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang and Huajun Chen* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2-L8) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhang2022differentiable```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2104.07650)<a href="https://scholar.google.com.hk/scholar?q=AdaPrompt:+Adaptive+Prompt-based+Finetuning+for+Relation+Extraction"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**AdaPrompt: Adaptive Prompt-based Finetuning for Relation Extraction**](https://arxiv.org/abs/2104.07650) , <br> by *Xiang Chen and
Xin Xie and
Ningyu Zhang and
Jiahuan Yan and
Shumin Deng and
Chuanqi Tan and
Fei Huang and
Luo Si and
Huajun Chen* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2857-L2872) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2104-07650```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2108.13161)<a href="https://scholar.google.com.hk/scholar?q=Differentiable+Prompt+Makes+Pre-trained+Language+Models+Better+Few-shot+Learners"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners**](https://arxiv.org/abs/2108.13161) , <br> by *Ningyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang and Huajun Chen* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3204-L3211) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhang2021differentiable```
- [![](https://img.shields.io/badge/COLING-2020-green)](https://doi.org/10.18653/v1/2020.coling-main.563)<a href="https://scholar.google.com.hk/scholar?q=Bridging+Text+and+Knowledge+with+Multi-Prototype+Embedding+for+Few-Shot+Relational+Triple+Extraction"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Bridging Text and Knowledge with Multi-Prototype Embedding for Few-Shot
Relational Triple Extraction**](https://doi.org/10.18653/v1/2020.coling-main.563) , <br> by *Haiyang Yu and
Ningyu Zhang and
Shumin Deng and
Hongbin Ye and
Wei Zhang and
Huajun Chen* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L691-L704) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```YuZDYZC20```
## Richard S. Zemel

- [![](https://img.shields.io/badge/ICLR-2018-green)](https://openreview.net/forum?id=HJcSzz-CZ)<a href="https://scholar.google.com.hk/scholar?q=Meta-Learning+for+Semi-Supervised+Few-Shot+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta-Learning for Semi-Supervised Few-Shot Classification**](https://openreview.net/forum?id=HJcSzz-CZ) , <br> by *Mengye Ren, Sachin Ravi, Eleni Triantafillou, Jake Snell, Kevin Swersky, Josh B. Tenenbaum, Hugo Larochelle and Richard S. Zemel* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1876-L1882) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ren2018metalearning```
- [![](https://img.shields.io/badge/NeurIPS-2017-green)](https://proceedings.neurips.cc/paper/2017/hash/01e9565cecc4e989123f9620c1d09c09-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Learning+Through+an+Information+Retrieval+Lens"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Learning Through an Information Retrieval Lens**](https://proceedings.neurips.cc/paper/2017/hash/01e9565cecc4e989123f9620c1d09c09-Abstract.html) , <br> by *Eleni Triantafillou and
Richard S. Zemel and
Raquel Urtasun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2340-L2349) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```TriantafillouZU17```
- [![](https://img.shields.io/badge/NeurIPS-2017-green)](https://proceedings.neurips.cc/paper/2017/hash/cb8da6767461f2812ae4290eac7cbc42-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Prototypical+Networks+for+Few-shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Prototypical Networks for Few-shot Learning**](https://proceedings.neurips.cc/paper/2017/hash/cb8da6767461f2812ae4290eac7cbc42-Abstract.html) , <br> by *Jake Snell and
Kevin Swersky and
Richard S. Zemel* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2351-L2360) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```SnellSZ17```
## Eunho Yang

- [![](https://img.shields.io/badge/ICLR-2020-green)](https://openreview.net/forum?id=rkeZIJBYvr)<a href="https://scholar.google.com.hk/scholar?q=Learning+to+Balance:+Bayesian+Meta-Learning+for+Imbalanced+and+Out-of-distribution+Tasks"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning to Balance: Bayesian Meta-Learning for Imbalanced and Out-of-distribution Tasks**](https://openreview.net/forum?id=rkeZIJBYvr) , <br> by *Hae Beom Lee, Hayeon Lee, Donghyun Na, Saehoon Kim, Minseop Park, Eunho Yang and Sung Ju Hwang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1740-L1746) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Lee2020Learning```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/c39e1a03859f9ee215bc49131d0caf33-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Few-shot+Visual+Reasoning+with+Meta-Analogical+Contrastive+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-shot Visual Reasoning with Meta-Analogical Contrastive Learning**](https://proceedings.neurips.cc/paper/2020/hash/c39e1a03859f9ee215bc49131d0caf33-Abstract.html) , <br> by *Youngsung Kim and
Jinwoo Shin and
Eunho Yang and
Sung Ju Hwang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1994-L2003) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```KimSYH20```
- [![](https://img.shields.io/badge/ICLR-2019-green)](https://openreview.net/forum?id=SyVuRiC5K7)<a href="https://scholar.google.com.hk/scholar?q=LEARNING+TO+PROPAGATE+LABELS:+TRANSDUCTIVE+PROPAGATION+NETWORK+FOR+FEW-SHOT+LEARNING"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING**](https://openreview.net/forum?id=SyVuRiC5K7) , <br> by *Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, Eunho Yang, Sungju Hwang and Yi Yang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1822-L1828) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```liu2018learning```
## Kevin Swersky

- [![](https://img.shields.io/badge/ICLR-2020-green)](https://openreview.net/forum?id=rkgAGAVKPr)<a href="https://scholar.google.com.hk/scholar?q=Meta-Dataset:+A+Dataset+of+Datasets+for+Learning+to+Learn+from+Few+Examples"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples**](https://openreview.net/forum?id=rkgAGAVKPr) , <br> by *Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol and Hugo Larochelle* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1692-L1698) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Triantafillou2020Meta-Dataset```
- [![](https://img.shields.io/badge/ICLR-2018-green)](https://openreview.net/forum?id=HJcSzz-CZ)<a href="https://scholar.google.com.hk/scholar?q=Meta-Learning+for+Semi-Supervised+Few-Shot+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta-Learning for Semi-Supervised Few-Shot Classification**](https://openreview.net/forum?id=HJcSzz-CZ) , <br> by *Mengye Ren, Sachin Ravi, Eleni Triantafillou, Jake Snell, Kevin Swersky, Josh B. Tenenbaum, Hugo Larochelle and Richard S. Zemel* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1876-L1882) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ren2018metalearning```
- [![](https://img.shields.io/badge/NeurIPS-2017-green)](https://proceedings.neurips.cc/paper/2017/hash/cb8da6767461f2812ae4290eac7cbc42-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Prototypical+Networks+for+Few-shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Prototypical Networks for Few-shot Learning**](https://proceedings.neurips.cc/paper/2017/hash/cb8da6767461f2812ae4290eac7cbc42-Abstract.html) , <br> by *Jake Snell and
Kevin Swersky and
Richard S. Zemel* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2351-L2360) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```SnellSZ17```
## Sergey Levine

- [![](https://img.shields.io/badge/ICLR-2020-green)](https://openreview.net/forum?id=BklEFpEYwS)<a href="https://scholar.google.com.hk/scholar?q=Meta-Learning+without+Memorization"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta-Learning without Memorization**](https://openreview.net/forum?id=BklEFpEYwS) , <br> by *Mingzhang Yin, George Tucker, Mingyuan Zhou, Sergey Levine and Chelsea Finn* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1722-L1728) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Yin2020Meta-Learning```
- [![](https://img.shields.io/badge/ICLR-2018-green)](https://openreview.net/forum?id=HyjC5yWCW)<a href="https://scholar.google.com.hk/scholar?q=Meta-Learning+and+Universality:+Deep+Representations+and+Gradient+Descent+can+Approximate+any+Learning+Algorithm"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm**](https://openreview.net/forum?id=HyjC5yWCW) , <br> by *Chelsea Finn and Sergey Levine* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1885-L1891) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```finn2018metalearning```
- [![](https://img.shields.io/badge/ICML-2017-green)](http://proceedings.mlr.press/v70/finn17a.html)<a href="https://scholar.google.com.hk/scholar?q=Model-Agnostic+Meta-Learning+for+Fast+Adaptation+of+Deep+Networks"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks**](http://proceedings.mlr.press/v70/finn17a.html) , <br> by *Chelsea Finn, Pieter Abbeel and Sergey Levine* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1654-L1661) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v70-finn17a```
## Zhenhui Li

- [![](https://img.shields.io/badge/ICLR-2020-green)](https://openreview.net/forum?id=rklp93EtwH)<a href="https://scholar.google.com.hk/scholar?q=Automated+Relational+Meta-learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Automated Relational Meta-learning**](https://openreview.net/forum?id=rklp93EtwH) , <br> by *Huaxiu Yao, Xian Wu, Zhiqiang Tao, Yaliang Li, Bolin Ding, Ruirui Li and Zhenhui Li* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1673-L1679) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Yao2020Automated```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/4b86ca48d90bd5f0978afa3a012503a4-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Online+Structured+Meta-learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Online Structured Meta-learning**](https://proceedings.neurips.cc/paper/2020/hash/4b86ca48d90bd5f0978afa3a012503a4-Abstract.html) , <br> by *Huaxiu Yao and
Yingbo Zhou and
Mehrdad Mahdavi and
Zhenhui Li and
Richard Socher and
Caiming Xiong* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2093-L2104) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```YaoZMLSX20```
- [![](https://img.shields.io/badge/ICML-2019-green)](
http://proceedings.mlr.press/v97/yao19b.html)<a href="https://scholar.google.com.hk/scholar?q=Hierarchically+Structured+Meta-learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Hierarchically Structured Meta-learning**](
http://proceedings.mlr.press/v97/yao19b.html) , <br> by *Yao, Huaxiu, Wei, Ying, Huang, Junzhou and Li, Zhenhui* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1591-L1600) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v97-yao19b```
## Mengye Ren

- [![](https://img.shields.io/badge/ICLR-2021-green)](https://openreview.net/forum?id=oZIvHV04XgC)<a href="https://scholar.google.com.hk/scholar?q=Wandering+within+a+world:+Online+contextualized+few-shot+learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Wandering within a world: Online contextualized few-shot learning**](https://openreview.net/forum?id=oZIvHV04XgC) , <br> by *Mengye Ren, Michael Louis Iuzzolino, Michael Curtis Mozer and Richard Zemel* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1359-L1365) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ren2021wandering```
- [![](https://img.shields.io/badge/ICLR-2021-green)](https://openreview.net/forum?id=SZ3wtsXfzQR)<a href="https://scholar.google.com.hk/scholar?q=Theoretical+bounds+on+estimation+error+for+meta-learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Theoretical bounds on estimation error for meta-learning**](https://openreview.net/forum?id=SZ3wtsXfzQR) , <br> by *James Lucas, Mengye Ren, Irene Raissa KAMENI KAMENI, Toniann Pitassi and Richard Zemel* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1467-L1473) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```lucas2021theoretical```
- [![](https://img.shields.io/badge/ICLR-2018-green)](https://openreview.net/forum?id=HJcSzz-CZ)<a href="https://scholar.google.com.hk/scholar?q=Meta-Learning+for+Semi-Supervised+Few-Shot+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta-Learning for Semi-Supervised Few-Shot Classification**](https://openreview.net/forum?id=HJcSzz-CZ) , <br> by *Mengye Ren, Sachin Ravi, Eleni Triantafillou, Jake Snell, Kevin Swersky, Josh B. Tenenbaum, Hugo Larochelle and Richard S. Zemel* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1876-L1882) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ren2018metalearning```
## Tom Goldstein

- [![](https://img.shields.io/badge/ICML-2021-green)](http://proceedings.mlr.press/v139/ni21a.html)<a href="https://scholar.google.com.hk/scholar?q=Data+Augmentation+for+Meta-Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Data Augmentation for Meta-Learning**](http://proceedings.mlr.press/v139/ni21a.html) , <br> by *Ni, Renkun, Goldblum, Micah, Sharaf, Amr, Kong, Kezhi and Goldstein, Tom* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1065-L1072) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v139-ni21a```
- [![](https://img.shields.io/badge/ICML-2020-green)](http://proceedings.mlr.press/v119/goldblum20a.html)<a href="https://scholar.google.com.hk/scholar?q=Unraveling+Meta-Learning:+Understanding+Feature+Representations+for+Few-Shot+Tasks"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Unraveling Meta-Learning: Understanding Feature Representations for Few-Shot Tasks**](http://proceedings.mlr.press/v119/goldblum20a.html) , <br> by *Goldblum, Micah, Reich, Steven, Fowl, Liam, Ni, Renkun, Cherepanova, Valeriia and Goldstein, Tom* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1494-L1501) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v119-goldblum20a```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/cfee398643cbc3dc5eefc89334cacdc1-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Adversarially+Robust+Few-Shot+Learning:+A+Meta-Learning+Approach"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Adversarially Robust Few-Shot Learning: A Meta-Learning Approach**](https://proceedings.neurips.cc/paper/2020/hash/cfee398643cbc3dc5eefc89334cacdc1-Abstract.html) , <br> by *Micah Goldblum and
Liam Fowl and
Tom Goldstein* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2005-L2013) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```GoldblumFG20```
## Micah Goldblum

- [![](https://img.shields.io/badge/ICML-2021-green)](http://proceedings.mlr.press/v139/ni21a.html)<a href="https://scholar.google.com.hk/scholar?q=Data+Augmentation+for+Meta-Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Data Augmentation for Meta-Learning**](http://proceedings.mlr.press/v139/ni21a.html) , <br> by *Ni, Renkun, Goldblum, Micah, Sharaf, Amr, Kong, Kezhi and Goldstein, Tom* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1065-L1072) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v139-ni21a```
- [![](https://img.shields.io/badge/ICML-2020-green)](http://proceedings.mlr.press/v119/goldblum20a.html)<a href="https://scholar.google.com.hk/scholar?q=Unraveling+Meta-Learning:+Understanding+Feature+Representations+for+Few-Shot+Tasks"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Unraveling Meta-Learning: Understanding Feature Representations for Few-Shot Tasks**](http://proceedings.mlr.press/v119/goldblum20a.html) , <br> by *Goldblum, Micah, Reich, Steven, Fowl, Liam, Ni, Renkun, Cherepanova, Valeriia and Goldstein, Tom* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1494-L1501) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v119-goldblum20a```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/cfee398643cbc3dc5eefc89334cacdc1-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Adversarially+Robust+Few-Shot+Learning:+A+Meta-Learning+Approach"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Adversarially Robust Few-Shot Learning: A Meta-Learning Approach**](https://proceedings.neurips.cc/paper/2020/hash/cfee398643cbc3dc5eefc89334cacdc1-Abstract.html) , <br> by *Micah Goldblum and
Liam Fowl and
Tom Goldstein* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2005-L2013) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```GoldblumFG20```
## Richard Zemel

- [![](https://img.shields.io/badge/ICML-2021-green)](http://proceedings.mlr.press/v139/triantafillou21a.html)<a href="https://scholar.google.com.hk/scholar?q=Learning+a+Universal+Template+for+Few-shot+Dataset+Generalization"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning a Universal Template for Few-shot Dataset Generalization**](http://proceedings.mlr.press/v139/triantafillou21a.html) , <br> by *Triantafillou, Eleni, Larochelle, Hugo, Zemel, Richard and Dumoulin, Vincent* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1015-L1022) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v139-triantafillou21a```
- [![](https://img.shields.io/badge/ICLR-2021-green)](https://openreview.net/forum?id=oZIvHV04XgC)<a href="https://scholar.google.com.hk/scholar?q=Wandering+within+a+world:+Online+contextualized+few-shot+learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Wandering within a world: Online contextualized few-shot learning**](https://openreview.net/forum?id=oZIvHV04XgC) , <br> by *Mengye Ren, Michael Louis Iuzzolino, Michael Curtis Mozer and Richard Zemel* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1359-L1365) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ren2021wandering```
- [![](https://img.shields.io/badge/ICLR-2021-green)](https://openreview.net/forum?id=SZ3wtsXfzQR)<a href="https://scholar.google.com.hk/scholar?q=Theoretical+bounds+on+estimation+error+for+meta-learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Theoretical bounds on estimation error for meta-learning**](https://openreview.net/forum?id=SZ3wtsXfzQR) , <br> by *James Lucas, Mengye Ren, Irene Raissa KAMENI KAMENI, Toniann Pitassi and Richard Zemel* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1467-L1473) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```lucas2021theoretical```
## Sameer Singh

- [![](https://img.shields.io/badge/ICML-2021-green)](http://proceedings.mlr.press/v139/zhao21c.html)<a href="https://scholar.google.com.hk/scholar?q=Calibrate+Before+Use:+Improving+Few-shot+Performance+of+Language+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Calibrate Before Use: Improving Few-shot Performance of Language Models**](http://proceedings.mlr.press/v139/zhao21c.html) , <br> by *Zhao, Zihao, Wallace, Eric, Feng, Shi, Klein, Dan and Singh, Sameer* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L995-L1002) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v139-zhao21c```
- [![](https://img.shields.io/badge/EMNLP-2020-green)](https://www.aclweb.org/anthology/2020.emnlp-main.346)<a href="https://scholar.google.com.hk/scholar?q=AutoPrompt:+Eliciting+Knowledge+from+Language+Models+with+Automatically+Generated+Prompts"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts**](https://www.aclweb.org/anthology/2020.emnlp-main.346) , <br> by *Shin, Taylor  and
Razeghi, Yasaman  and
Logan IV, Robert L.  and
Wallace, Eric  and
Singh, Sameer* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2474-L2485) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```shin-etal-2020-autoprompt```
- [![](https://img.shields.io/badge/CoRR-2020-green)](https://arxiv.org/abs/2106.13353)<a href="https://scholar.google.com.hk/scholar?q=Cutting+Down+on+Prompts+and+Parameters:+Simple+Few-Shot+Learning+with+Language+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models**](https://arxiv.org/abs/2106.13353) , <br> by *Robert L. Logan IV au2, Ivana Balažević, Eric Wallace, Fabio Petroni, Sameer Singh and Sebastian Riedel* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3145-L3152) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```logan2021cutting```
## Dan Klein

- [![](https://img.shields.io/badge/ICML-2021-green)](http://proceedings.mlr.press/v139/zhao21c.html)<a href="https://scholar.google.com.hk/scholar?q=Calibrate+Before+Use:+Improving+Few-shot+Performance+of+Language+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Calibrate Before Use: Improving Few-shot Performance of Language Models**](http://proceedings.mlr.press/v139/zhao21c.html) , <br> by *Zhao, Zihao, Wallace, Eric, Feng, Shi, Klein, Dan and Singh, Sameer* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L995-L1002) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v139-zhao21c```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2104.08768)<a href="https://scholar.google.com.hk/scholar?q=Constrained+Language+Models+Yield+Few-Shot+Semantic+Parsers"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Constrained Language Models Yield Few-Shot Semantic Parsers**](https://arxiv.org/abs/2104.08768) , <br> by *Richard Shin and
Christopher H. Lin and
Sam Thomson and
Charles Chen and
Subhro Roy and
Emmanouil Antonios Platanios and
Adam Pauls and
Dan Klein and
Jason Eisner and
Benjamin Van Durme* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3020-L3036) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2104-08768```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2104.04670)<a href="https://scholar.google.com.hk/scholar?q=Meta-tuning+Language+Models+to+Answer+Prompts+Better"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta-tuning Language Models to Answer Prompts Better**](https://arxiv.org/abs/2104.04670) , <br> by *Ruiqi Zhong and
Kristy Lee and
Zheng Zhang and
Dan Klein* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3047-L3057) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2104-04670```
## Eric Wallace

- [![](https://img.shields.io/badge/ICML-2021-green)](http://proceedings.mlr.press/v139/zhao21c.html)<a href="https://scholar.google.com.hk/scholar?q=Calibrate+Before+Use:+Improving+Few-shot+Performance+of+Language+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Calibrate Before Use: Improving Few-shot Performance of Language Models**](http://proceedings.mlr.press/v139/zhao21c.html) , <br> by *Zhao, Zihao, Wallace, Eric, Feng, Shi, Klein, Dan and Singh, Sameer* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L995-L1002) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v139-zhao21c```
- [![](https://img.shields.io/badge/EMNLP-2020-green)](https://www.aclweb.org/anthology/2020.emnlp-main.346)<a href="https://scholar.google.com.hk/scholar?q=AutoPrompt:+Eliciting+Knowledge+from+Language+Models+with+Automatically+Generated+Prompts"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts**](https://www.aclweb.org/anthology/2020.emnlp-main.346) , <br> by *Shin, Taylor  and
Razeghi, Yasaman  and
Logan IV, Robert L.  and
Wallace, Eric  and
Singh, Sameer* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2474-L2485) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```shin-etal-2020-autoprompt```
- [![](https://img.shields.io/badge/CoRR-2020-green)](https://arxiv.org/abs/2106.13353)<a href="https://scholar.google.com.hk/scholar?q=Cutting+Down+on+Prompts+and+Parameters:+Simple+Few-Shot+Learning+with+Language+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models**](https://arxiv.org/abs/2106.13353) , <br> by *Robert L. Logan IV au2, Ivana Balažević, Eric Wallace, Fabio Petroni, Sameer Singh and Sebastian Riedel* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3145-L3152) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```logan2021cutting```
## Hae Beom Lee

- [![](https://img.shields.io/badge/ICML-2021-green)](http://proceedings.mlr.press/v139/shin21a.html)<a href="https://scholar.google.com.hk/scholar?q=Large-Scale+Meta-Learning+with+Continual+Trajectory+Shifting"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Large-Scale Meta-Learning with Continual Trajectory Shifting**](http://proceedings.mlr.press/v139/shin21a.html) , <br> by *Shin, Jaewoong, Lee, Hae Beom, Gong, Boqing and Hwang, Sung Ju* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L974-L981) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v139-shin21a```
- [![](https://img.shields.io/badge/ICML-2020-green)](http://proceedings.mlr.press/v119/park20b.html)<a href="https://scholar.google.com.hk/scholar?q=Meta+Variance+Transfer:+Learning+to+Augment+from+the+Others"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta Variance Transfer: Learning to Augment from the Others**](http://proceedings.mlr.press/v119/park20b.html) , <br> by *Park, Seong-Jin, Han, Seungju, Baek, Ji-Won, Kim, Insoo, Song, Juhwan, Lee, Hae Beom, Han, Jae-Joon and Hwang, Sung Ju* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1514-L1521) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v119-park20b```
- [![](https://img.shields.io/badge/ICLR-2020-green)](https://openreview.net/forum?id=rkeZIJBYvr)<a href="https://scholar.google.com.hk/scholar?q=Learning+to+Balance:+Bayesian+Meta-Learning+for+Imbalanced+and+Out-of-distribution+Tasks"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning to Balance: Bayesian Meta-Learning for Imbalanced and Out-of-distribution Tasks**](https://openreview.net/forum?id=rkeZIJBYvr) , <br> by *Hae Beom Lee, Hayeon Lee, Donghyun Na, Saehoon Kim, Minseop Park, Eunho Yang and Sung Ju Hwang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1740-L1746) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Lee2020Learning```
## Juanzi Li

- [![](https://img.shields.io/badge/ACL-2021-green)](https://aclanthology.org/2021.acl-long.487)<a href="https://scholar.google.com.hk/scholar?q=Learning+from+Miscellaneous+Other-Class+Words+for+Few-shot+Named+Entity+Recognition"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning from Miscellaneous Other-Class Words for Few-shot Named Entity Recognition**](https://aclanthology.org/2021.acl-long.487) , <br> by *Tong, Meihan  and
Wang, Shuai  and
Xu, Bin  and
Cao, Yixin  and
Liu, Minghui  and
Hou, Lei  and
Li, Juanzi* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L946-L959) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```tong-etal-2021-learning```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2108.02035)<a href="https://scholar.google.com.hk/scholar?q=Knowledgeable+Prompt-tuning:+Incorporating+Knowledge+into+Prompt+Verbalizer+for+Text+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer
for Text Classification**](https://arxiv.org/abs/2108.02035) , <br> by *Shengding Hu and
Ning Ding and
Huadong Wang and
Zhiyuan Liu and
Juanzi Li and
Maosong Sun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3177-L3190) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2108-02035```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2108.10604)<a href="https://scholar.google.com.hk/scholar?q=Prompt-Learning+for+Fine-Grained+Entity+Typing"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Prompt-Learning for Fine-Grained Entity Typing**](https://arxiv.org/abs/2108.10604) , <br> by *Ning Ding and
Yulin Chen and
Xu Han and
Guangwei Xu and
Pengjun Xie and
Hai{-}Tao Zheng and
Zhiyuan Liu and
Juanzi Li and
Hong{-}Gee Kim* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3231-L3246) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2108-10604```
## Ivan Vuli{\'c}

- [![](https://img.shields.io/badge/ACL-2021-green)](https://aclanthology.org/2021.acl-long.447)<a href="https://scholar.google.com.hk/scholar?q=A+Closer+Look+at+Few-Shot+Crosslingual+Transfer:+The+Choice+of+Shots+Matters"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**A Closer Look at Few-Shot Crosslingual Transfer: The Choice of Shots Matters**](https://aclanthology.org/2021.acl-long.447) , <br> by *Zhao, Mengjie  and
Zhu, Yi  and
Shareghi, Ehsan  and
Vuli{\'c}, Ivan  and
Reichart, Roi  and
Korhonen, Anna  and
Sch{\"u}tze, Hinrich* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L828-L841) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhao-etal-2021-closer```
- [![](https://img.shields.io/badge/NAACL_HLT-2021-green)](https://www.aclweb.org/anthology/2021.naacl-main.264)<a href="https://scholar.google.com.hk/scholar?q=ConVEx:+Data-Efficient+and+Few-Shot+Slot+Labeling"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**ConVEx: Data-Efficient and Few-Shot Slot Labeling**](https://www.aclweb.org/anthology/2021.naacl-main.264) , <br> by *Henderson, Matthew  and
Vuli{\'c}, Ivan* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2828-L2836) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```henderson-vulic-2021-convex```
- [![](https://img.shields.io/badge/ACL-2020-green)](https://www.aclweb.org/anthology/2020.acl-main.11)<a href="https://scholar.google.com.hk/scholar?q=Span-ConveRT:+Few-shot+Span+Extraction+for+Dialog+with+Pretrained+Conversational+Representations"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Span-ConveRT: Few-shot Span Extraction for Dialog with Pretrained Conversational Representations**](https://www.aclweb.org/anthology/2020.acl-main.11) , <br> by *Coope, Samuel  and
Farghly, Tyler  and
Gerz, Daniela  and
Vuli{\'c}, Ivan  and
Henderson, Matthew* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2487-L2498) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```coope-etal-2020-span```
## Mengjie Zhao

- [![](https://img.shields.io/badge/ACL-2021-green)](https://aclanthology.org/2021.acl-long.447)<a href="https://scholar.google.com.hk/scholar?q=A+Closer+Look+at+Few-Shot+Crosslingual+Transfer:+The+Choice+of+Shots+Matters"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**A Closer Look at Few-Shot Crosslingual Transfer: The Choice of Shots Matters**](https://aclanthology.org/2021.acl-long.447) , <br> by *Zhao, Mengjie  and
Zhu, Yi  and
Shareghi, Ehsan  and
Vuli{\'c}, Ivan  and
Reichart, Roi  and
Korhonen, Anna  and
Sch{\"u}tze, Hinrich* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L828-L841) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhao-etal-2021-closer```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2109.03630)<a href="https://scholar.google.com.hk/scholar?q=Discrete+and+Soft+Prompting+for+Multilingual+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Discrete and Soft Prompting for Multilingual Models**](https://arxiv.org/abs/2109.03630) , <br> by *Mengjie Zhao and Hinrich Schütze* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3370-L3380) <br>```EMNLP 2021
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhao2021discrete```
- [![](https://img.shields.io/badge/CoRR-2020-green)](https://arxiv.org/abs/2012.15682)<a href="https://scholar.google.com.hk/scholar?q=A+Closer+Look+at+Few-Shot+Crosslingual+Transfer:+Variance,+Benchmarks+and+Baselines"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**A Closer Look at Few-Shot Crosslingual Transfer: Variance, Benchmarks
and Baselines**](https://arxiv.org/abs/2012.15682) , <br> by *Mengjie Zhao and
Yi Zhu and
Ehsan Shareghi and
Roi Reichart and
Anna Korhonen and
Hinrich Sch{\"{u}}tze* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3096-L3109) <br>```ACL 2021
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2012-15682```
## Hao Zhu

- [![](https://img.shields.io/badge/ICML-2021-green)](http://proceedings.mlr.press/v139/zhu21d.html)<a href="https://scholar.google.com.hk/scholar?q=Few-shot+Language+Coordination+by+Modeling+Theory+of+Mind"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-shot Language Coordination by Modeling Theory of Mind**](http://proceedings.mlr.press/v139/zhu21d.html) , <br> by *Zhu, Hao, Neubig, Graham and Bisk, Yonatan* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L984-L991) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v139-zhu21d```
- [![](https://img.shields.io/badge/EMNLP-2019-green)](https://doi.org/10.18653/v1/D19-1649)<a href="https://scholar.google.com.hk/scholar?q=FewRel+2.0:+Towards+More+Challenging+Few-Shot+Relation+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**FewRel 2.0: Towards More Challenging Few-Shot Relation Classification**](https://doi.org/10.18653/v1/D19-1649) , <br> by *Tianyu Gao and
Xu Han and
Hao Zhu and
Zhiyuan Liu and
Peng Li and
Maosong Sun and
Jie Zhou* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L723-L736) <br>```Fewrel 2.0 dataset
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```GaoHZLLSZ19```
- [![](https://img.shields.io/badge/EMNLP-2018-green)](https://doi.org/10.18653/v1/d18-1514)<a href="https://scholar.google.com.hk/scholar?q=FewRel:+A+Large-Scale+Supervised+Few-shot+Relation+Classification+Dataset+with+State-of-the-Art+Evaluation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**FewRel: A Large-Scale Supervised Few-shot Relation Classification
Dataset with State-of-the-Art Evaluation**](https://doi.org/10.18653/v1/d18-1514) , <br> by *Xu Han and
Hao Zhu and
Pengfei Yu and
Ziyun Wang and
Yuan Yao and
Zhiyuan Liu and
Maosong Sun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L706-L720) <br>```FewRel dataset
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```HanZYWYLS18```
## Ahmed Hassan Awadallah

- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/3644a684f98ea8fe223c713b77189a77-Abstract-round2.html)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Learning+Evaluation+in+Natural+Language+Understanding"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Learning Evaluation in Natural Language Understanding**](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/3644a684f98ea8fe223c713b77189a77-Abstract-round2.html) , <br> by *Subhabrata Mukherjee and
Xiaodong Liu and
Guoqing Zheng and
Saghar Hosseini and
Saghar Hosseini and
Hao Cheng and
Ge Yang and
Christopher Meek and
Ahmed Hassan Awadallah and
Jianfeng Gao* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L464-L479) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```MukherjeeLZHH0Y21```
- [![](https://img.shields.io/badge/KDD-2021-green)](https://doi.org/10.1145/3447548.3467235)<a href="https://scholar.google.com.hk/scholar?q=Meta+Self-Training+for+Few-Shot+Neural+Sequence+Labeling"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta Self-Training for Few-Shot Neural Sequence Labeling**](https://doi.org/10.1145/3447548.3467235) , <br> by *Wang, Yaqing, Mukherjee, Subhabrata, Chu, Haoda, Tu, Yuancheng, Wu, Ming, Gao, Jing and Awadallah, Ahmed Hassan* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L792-L799) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```3447548.3467235```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/f23d125da1e29e34c552f448610ff25f-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Uncertainty-aware+Self-training+for+Few-shot+Text+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Uncertainty-aware Self-training for Few-shot Text Classification**](https://proceedings.neurips.cc/paper/2020/hash/f23d125da1e29e34c552f448610ff25f-Abstract.html) , <br> by *Subhabrata Mukherjee and
Ahmed Hassan Awadallah* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2015-L2022) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```MukherjeeA20```
## Arzoo Katiyar

- [![](https://img.shields.io/badge/ACL-2022-green)](https://aclanthology.org/2022.acl-long.439)<a href="https://scholar.google.com.hk/scholar?q=CONTaiNER:+Few-Shot+Named+Entity+Recognition+via+Contrastive+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**CONTaiNER: Few-Shot Named Entity Recognition via Contrastive Learning**](https://aclanthology.org/2022.acl-long.439) , <br> by *Das, Sarkar Snigdha Sarathi  and
Katiyar, Arzoo  and
Passonneau, Rebecca  and
Zhang, Rui* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L319-L330) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```das-etal-2022-container```
- [![](https://img.shields.io/badge/ICLR-2021-green)](https://openreview.net/forum?id=cO1IH43yUF)<a href="https://scholar.google.com.hk/scholar?q=Revisiting+Few-sample+\BERT\+Fine-tuning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Revisiting Few-sample \BERT\ Fine-tuning**](https://openreview.net/forum?id=cO1IH43yUF) , <br> by *Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q Weinberger and Yoav Artzi* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1386-L1392) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhang2021revisiting```
- [![](https://img.shields.io/badge/EMNLP-2020-green)](https://www.aclweb.org/anthology/2020.emnlp-main.516)<a href="https://scholar.google.com.hk/scholar?q=Simple+and+Effective+Few-Shot+Named+Entity+Recognition+with+Structured+Nearest+Neighbor+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Simple and Effective Few-Shot Named Entity Recognition with Structured Nearest Neighbor Learning**](https://www.aclweb.org/anthology/2020.emnlp-main.516) , <br> by *Yang, Yi  and
Katiyar, Arzoo* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2695-L2703) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```yang-katiyar-2020-simple```
## S. M. Ali Eslami

- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/01b7575c38dac42f3cfb7d500438b875-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Multimodal+Few-Shot+Learning+with+Frozen+Language+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Multimodal Few-Shot Learning with Frozen Language Models**](https://proceedings.neurips.cc/paper/2021/hash/01b7575c38dac42f3cfb7d500438b875-Abstract.html) , <br> by *Maria Tsimpoukelli and
Jacob Menick and
Serkan Cabi and
S. M. Ali Eslami and
Oriol Vinyals and
Felix Hill* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L248-L260) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```TsimpoukelliMCE21```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2106.13884)<a href="https://scholar.google.com.hk/scholar?q=Multimodal+Few-Shot+Learning+with+Frozen+Language+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Multimodal Few-Shot Learning with Frozen Language Models**](https://arxiv.org/abs/2106.13884) , <br> by *Maria Tsimpoukelli and
Jacob Menick and
Serkan Cabi and
S. M. Ali Eslami and
Oriol Vinyals and
Felix Hill* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3154-L3166) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2106-13884```
- [![](https://img.shields.io/badge/ICLR-2018-green)](https://openreview.net/forum?id=r1wEFyWCW)<a href="https://scholar.google.com.hk/scholar?q=Few-shot+Autoregressive+Density+Estimation:+Towards+Learning+to+Learn+Distributions"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions**](https://openreview.net/forum?id=r1wEFyWCW) , <br> by *Scott Reed, Yutian Chen, Thomas Paine, Aäron van den Oord, S. M. Ali Eslami, Danilo Rezende, Oriol Vinyals and Nando de Freitas* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1858-L1864) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```reed2018fewshot```
## Xiantong Zhen

- [![](https://img.shields.io/badge/International_Conference_on_Learning_Representations-2022-green)](https://openreview.net/forum?id=i3RI65sR7N)<a href="https://scholar.google.com.hk/scholar?q=Hierarchical+Variational+Memory+for+Few-shot+Learning+Across+Domains"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Hierarchical Variational Memory for Few-shot Learning Across Domains**](https://openreview.net/forum?id=i3RI65sR7N) , <br> by *Yingjun Du, Xiantong Zhen, Ling Shao and Cees G. M. Snoek* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L83-L89) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```du2022hierarchical```
- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/6e2713a6efee97bacb63e52c54f0ada0-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Learning+to+Learn+Dense+Gaussian+Processes+for+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning to Learn Dense Gaussian Processes for Few-Shot Learning**](https://proceedings.neurips.cc/paper/2021/hash/6e2713a6efee97bacb63e52c54f0ada0-Abstract.html) , <br> by *Ze Wang and
Zichen Miao and
Xiantong Zhen and
Qiang Qiu* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L208-L219) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```WangMZQ21```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/67d16d00201083a2b118dd5128dd6f59-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Learning+to+Learn+Variational+Semantic+Memory"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning to Learn Variational Semantic Memory**](https://proceedings.neurips.cc/paper/2020/hash/67d16d00201083a2b118dd5128dd6f59-Abstract.html) , <br> by *Xiantong Zhen and
Ying{-}Jun Du and
Huan Xiong and
Qiang Qiu and
Cees Snoek and
Ling Shao* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2192-L2203) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ZhenDXQS020```
## Pieter Abbeel

- [![](https://img.shields.io/badge/International_Conference_on_Learning_Representations-2022-green)](https://openreview.net/forum?id=xKZ4K0lTj_)<a href="https://scholar.google.com.hk/scholar?q=Hierarchical+Few-Shot+Imitation+with+Skill+Transition+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Hierarchical Few-Shot Imitation with Skill Transition Models**](https://openreview.net/forum?id=xKZ4K0lTj_) , <br> by *Kourosh Hakhamaneshi, Ruihan Zhao, Albert Zhan, Pieter Abbeel and Michael Laskin* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L65-L71) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```hakhamaneshi2022hierarchical```
- [![](https://img.shields.io/badge/ICLR-2018-green)](https://openreview.net/forum?id=SyX0IeWAW)<a href="https://scholar.google.com.hk/scholar?q=META+LEARNING+SHARED+HIERARCHIES"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**META LEARNING SHARED HIERARCHIES**](https://openreview.net/forum?id=SyX0IeWAW) , <br> by *Kevin Frans, Jonathan Ho, Xi Chen, Pieter Abbeel and John Schulman* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1894-L1900) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```frans2018meta```
- [![](https://img.shields.io/badge/ICML-2017-green)](http://proceedings.mlr.press/v70/finn17a.html)<a href="https://scholar.google.com.hk/scholar?q=Model-Agnostic+Meta-Learning+for+Fast+Adaptation+of+Deep+Networks"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks**](http://proceedings.mlr.press/v70/finn17a.html) , <br> by *Chelsea Finn, Pieter Abbeel and Sergey Levine* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1654-L1661) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v70-finn17a```
## Yu-Xiong Wang

- [![](https://img.shields.io/badge/International_Conference_on_Learning_Representations-2022-green)](https://openreview.net/forum?id=DNRADop4ksB)<a href="https://scholar.google.com.hk/scholar?q=On+the+Importance+of+Firth+Bias+Reduction+in+Few-Shot+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**On the Importance of Firth Bias Reduction in Few-Shot Classification**](https://openreview.net/forum?id=DNRADop4ksB) , <br> by *Saba Ghaffari, Ehsan Saleh, David Forsyth and Yu-Xiong Wang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L38-L44) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ghaffari2022on```
- [![](https://img.shields.io/badge/ICCV-2021-green)](https://openaccess.thecvf.com/content/ICCV2021/html/Das_On_the_Importance_of_Distractors_for_Few-Shot_Classification_ICCV_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=On+the+Importance+of+Distractors+for+Few-Shot+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**On the Importance of Distractors for Few-Shot Classification**](https://openaccess.thecvf.com/content/ICCV2021/html/Das_On_the_Importance_of_Distractors_for_Few-Shot_Classification_ICCV_2021_paper.html) , <br> by *Das, Rajshekhar, Wang, Yu-Xiong and Moura, Jos\'e M. F.* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L525-L533) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Das_2021_ICCV```
- [![](https://img.shields.io/badge/CVPR-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Hallucination_Improves_Few-Shot_Object_Detection_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Hallucination+Improves+Few-Shot+Object+Detection"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Hallucination Improves Few-Shot Object Detection**](https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Hallucination_Improves_Few-Shot_Object_Detection_CVPR_2021_paper.html) , <br> by *Zhang, Weilin and Wang, Yu-Xiong* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1142-L1150) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Zhang_2021_CVPR```
## Fei Huang

- [![](https://img.shields.io/badge/International_Conference_on_Learning_Representations-2022-green)](https://openreview.net/forum?id=ek9a0qIafW)<a href="https://scholar.google.com.hk/scholar?q=Differentiable+Prompt+Makes+Pre-trained+Language+Models+Better+Few-shot+Learners"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners**](https://openreview.net/forum?id=ek9a0qIafW) , <br> by *Ningyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang and Huajun Chen* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2-L8) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhang2022differentiable```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2104.07650)<a href="https://scholar.google.com.hk/scholar?q=AdaPrompt:+Adaptive+Prompt-based+Finetuning+for+Relation+Extraction"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**AdaPrompt: Adaptive Prompt-based Finetuning for Relation Extraction**](https://arxiv.org/abs/2104.07650) , <br> by *Xiang Chen and
Xin Xie and
Ningyu Zhang and
Jiahuan Yan and
Shumin Deng and
Chuanqi Tan and
Fei Huang and
Luo Si and
Huajun Chen* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2857-L2872) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2104-07650```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2108.13161)<a href="https://scholar.google.com.hk/scholar?q=Differentiable+Prompt+Makes+Pre-trained+Language+Models+Better+Few-shot+Learners"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners**](https://arxiv.org/abs/2108.13161) , <br> by *Ningyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang and Huajun Chen* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3204-L3211) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhang2021differentiable```
## Chuanqi Tan

- [![](https://img.shields.io/badge/International_Conference_on_Learning_Representations-2022-green)](https://openreview.net/forum?id=ek9a0qIafW)<a href="https://scholar.google.com.hk/scholar?q=Differentiable+Prompt+Makes+Pre-trained+Language+Models+Better+Few-shot+Learners"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners**](https://openreview.net/forum?id=ek9a0qIafW) , <br> by *Ningyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang and Huajun Chen* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2-L8) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhang2022differentiable```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2104.07650)<a href="https://scholar.google.com.hk/scholar?q=AdaPrompt:+Adaptive+Prompt-based+Finetuning+for+Relation+Extraction"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**AdaPrompt: Adaptive Prompt-based Finetuning for Relation Extraction**](https://arxiv.org/abs/2104.07650) , <br> by *Xiang Chen and
Xin Xie and
Ningyu Zhang and
Jiahuan Yan and
Shumin Deng and
Chuanqi Tan and
Fei Huang and
Luo Si and
Huajun Chen* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2857-L2872) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2104-07650```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2108.13161)<a href="https://scholar.google.com.hk/scholar?q=Differentiable+Prompt+Makes+Pre-trained+Language+Models+Better+Few-shot+Learners"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners**](https://arxiv.org/abs/2108.13161) , <br> by *Ningyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang and Huajun Chen* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3204-L3211) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhang2021differentiable```
## Mohit Bansal

- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2106.07499)<a href="https://scholar.google.com.hk/scholar?q=An+Empirical+Survey+of+Data+Augmentation+for+Limited+Data+Learning+in+NLP"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**An Empirical Survey of Data Augmentation for Limited Data Learning in NLP**](https://arxiv.org/abs/2106.07499) , <br> by *Jiaao Chen, Derek Tam, Colin Raffel, Mohit Bansal and Diyi Yang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3038-L3045) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```chen2021empirical```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2103.11955)<a href="https://scholar.google.com.hk/scholar?q=Improving+and+Simplifying+Pattern+Exploiting+Training"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Improving and Simplifying Pattern Exploiting Training**](https://arxiv.org/abs/2103.11955) , <br> by *Derek Tam and
Rakesh R. Menon and
Mohit Bansal and
Shashank Srivastava and
Colin Raffel* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3309-L3324) <br>```EMNLP 2021, proposing ADAPET which promisingly improves the data efficiency of PET. ADAPET does not leverage unlabelled data for training, and introduces label-conditioned loss for the denser supervision.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2103-11955```
## Colin Raffel

- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2106.07499)<a href="https://scholar.google.com.hk/scholar?q=An+Empirical+Survey+of+Data+Augmentation+for+Limited+Data+Learning+in+NLP"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**An Empirical Survey of Data Augmentation for Limited Data Learning in NLP**](https://arxiv.org/abs/2106.07499) , <br> by *Jiaao Chen, Derek Tam, Colin Raffel, Mohit Bansal and Diyi Yang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3038-L3045) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```chen2021empirical```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2103.11955)<a href="https://scholar.google.com.hk/scholar?q=Improving+and+Simplifying+Pattern+Exploiting+Training"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Improving and Simplifying Pattern Exploiting Training**](https://arxiv.org/abs/2103.11955) , <br> by *Derek Tam and
Rakesh R. Menon and
Mohit Bansal and
Shashank Srivastava and
Colin Raffel* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3309-L3324) <br>```EMNLP 2021, proposing ADAPET which promisingly improves the data efficiency of PET. ADAPET does not leverage unlabelled data for training, and introduces label-conditioned loss for the denser supervision.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2103-11955```
## Derek Tam

- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2106.07499)<a href="https://scholar.google.com.hk/scholar?q=An+Empirical+Survey+of+Data+Augmentation+for+Limited+Data+Learning+in+NLP"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**An Empirical Survey of Data Augmentation for Limited Data Learning in NLP**](https://arxiv.org/abs/2106.07499) , <br> by *Jiaao Chen, Derek Tam, Colin Raffel, Mohit Bansal and Diyi Yang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3038-L3045) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```chen2021empirical```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2103.11955)<a href="https://scholar.google.com.hk/scholar?q=Improving+and+Simplifying+Pattern+Exploiting+Training"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Improving and Simplifying Pattern Exploiting Training**](https://arxiv.org/abs/2103.11955) , <br> by *Derek Tam and
Rakesh R. Menon and
Mohit Bansal and
Shashank Srivastava and
Colin Raffel* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3309-L3324) <br>```EMNLP 2021, proposing ADAPET which promisingly improves the data efficiency of PET. ADAPET does not leverage unlabelled data for training, and introduces label-conditioned loss for the denser supervision.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2103-11955```
## Sebastian Riedel

- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2104.08786)<a href="https://scholar.google.com.hk/scholar?q=Fantastically+Ordered+Prompts+and+Where+to+Find+Them:+Overcoming+Few-Shot+Prompt+Order+Sensitivity"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot
Prompt Order Sensitivity**](https://arxiv.org/abs/2104.08786) , <br> by *Yao Lu and
Max Bartolo and
Alastair Moore and
Sebastian Riedel and
Pontus Stenetorp* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3006-L3018) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2104-08786```
- [![](https://img.shields.io/badge/CoRR-2020-green)](https://arxiv.org/abs/2106.13353)<a href="https://scholar.google.com.hk/scholar?q=Cutting+Down+on+Prompts+and+Parameters:+Simple+Few-Shot+Learning+with+Language+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models**](https://arxiv.org/abs/2106.13353) , <br> by *Robert L. Logan IV au2, Ivana Balažević, Eric Wallace, Fabio Petroni, Sameer Singh and Sebastian Riedel* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3145-L3152) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```logan2021cutting```
## Hinrich Sch{\"{u}}tze

- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2104.07540)<a href="https://scholar.google.com.hk/scholar?q=Generating+Datasets+with+Pretrained+Language+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Generating Datasets with Pretrained Language Models**](https://arxiv.org/abs/2104.07540) , <br> by *Timo Schick and
Hinrich Sch{\"{u}}tze* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2970-L2978) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2104-07540```
- [![](https://img.shields.io/badge/CoRR-2020-green)](https://arxiv.org/abs/2012.15682)<a href="https://scholar.google.com.hk/scholar?q=A+Closer+Look+at+Few-Shot+Crosslingual+Transfer:+Variance,+Benchmarks+and+Baselines"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**A Closer Look at Few-Shot Crosslingual Transfer: Variance, Benchmarks
and Baselines**](https://arxiv.org/abs/2012.15682) , <br> by *Mengjie Zhao and
Yi Zhu and
Ehsan Shareghi and
Roi Reichart and
Anna Korhonen and
Hinrich Sch{\"{u}}tze* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3096-L3109) <br>```ACL 2021
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2012-15682```
## Hannaneh Hajishirzi

- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2104.08773)<a href="https://scholar.google.com.hk/scholar?q=Natural+Instructions:+Benchmarking+Generalization+to+New+Tasks+from+Natural+Language+Instructions"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Natural Instructions: Benchmarking Generalization to New Tasks from
Natural Language Instructions**](https://arxiv.org/abs/2104.08773) , <br> by *Swaroop Mishra and
Daniel Khashabi and
Chitta Baral and
Hannaneh Hajishirzi* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2910-L2921) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2104-08773```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2108.04106)<a href="https://scholar.google.com.hk/scholar?q=Noisy+Channel+Language+Model+Prompting+for+Few-Shot+Text+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Noisy Channel Language Model Prompting for Few-Shot Text Classification**](https://arxiv.org/abs/2108.04106) , <br> by *Sewon Min and
Mike Lewis and
Hannaneh Hajishirzi and
Luke Zettlemoyer* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3192-L3202) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2108-04106```
## Luheng He

- [![](https://img.shields.io/badge/NAACL_HLT-2021-green)](https://www.aclweb.org/anthology/2021.naacl-main.59)<a href="https://scholar.google.com.hk/scholar?q=Few-shot+Intent+Classification+and+Slot+Filling+with+Retrieved+Examples"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-shot Intent Classification and Slot Filling with Retrieved Examples**](https://www.aclweb.org/anthology/2021.naacl-main.59) , <br> by *Yu, Dian  and
He, Luheng  and
Zhang, Yuan  and
Du, Xinya  and
Pasupat, Panupong  and
Li, Qi* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2771-L2783) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```yu-etal-2021-shot```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2102.01335)<a href="https://scholar.google.com.hk/scholar?q=Neural+Data+Augmentation+via+Example+Extrapolation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Neural Data Augmentation via Example Extrapolation**](https://arxiv.org/abs/2102.01335) , <br> by *Kenton Lee and
Kelvin Guu and
Luheng He and
Tim Dozat and
Hyung Won Chung* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2980-L2991) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2102-01335```
## Michael Zeng

- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2108.13487)<a href="https://scholar.google.com.hk/scholar?q=Want+To+Reduce+Labeling+Cost?+GPT-3+Can+Help"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Want To Reduce Labeling Cost? GPT-3 Can Help**](https://arxiv.org/abs/2108.13487) , <br> by *Shuohang Wang, Yang Liu, Yichong Xu, Chenguang Zhu and Michael Zeng* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3342-L3352) <br>```EMNLP Findings 2021, adopting GPT-3 for label generation.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```wang2021want```
- [![](https://img.shields.io/badge/EMNLP_Findings-2020-green)](https://www.aclweb.org/anthology/2020.findings-emnlp.17)<a href="https://scholar.google.com.hk/scholar?q=Few-shot+Natural+Language+Generation+for+Task-Oriented+Dialog"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-shot Natural Language Generation for Task-Oriented Dialog**](https://www.aclweb.org/anthology/2020.findings-emnlp.17) , <br> by *Peng, Baolin  and
Zhu, Chenguang  and
Li, Chunyuan  and
Li, Xiujun  and
Li, Jinchao  and
Zeng, Michael  and
Gao, Jianfeng* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2732-L2745) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```peng-etal-2020-shot```
## Chenguang Zhu

- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2108.13487)<a href="https://scholar.google.com.hk/scholar?q=Want+To+Reduce+Labeling+Cost?+GPT-3+Can+Help"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Want To Reduce Labeling Cost? GPT-3 Can Help**](https://arxiv.org/abs/2108.13487) , <br> by *Shuohang Wang, Yang Liu, Yichong Xu, Chenguang Zhu and Michael Zeng* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3342-L3352) <br>```EMNLP Findings 2021, adopting GPT-3 for label generation.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```wang2021want```
- [![](https://img.shields.io/badge/EMNLP_Findings-2020-green)](https://www.aclweb.org/anthology/2020.findings-emnlp.17)<a href="https://scholar.google.com.hk/scholar?q=Few-shot+Natural+Language+Generation+for+Task-Oriented+Dialog"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-shot Natural Language Generation for Task-Oriented Dialog**](https://www.aclweb.org/anthology/2020.findings-emnlp.17) , <br> by *Peng, Baolin  and
Zhu, Chenguang  and
Li, Chunyuan  and
Li, Xiujun  and
Li, Jinchao  and
Zeng, Michael  and
Gao, Jianfeng* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2732-L2745) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```peng-etal-2020-shot```
## Wenpeng Yin

- [![](https://img.shields.io/badge/NAACL_HLT-2021-green)](https://www.aclweb.org/anthology/2021.naacl-main.106)<a href="https://scholar.google.com.hk/scholar?q=Incremental+Few-shot+Text+Classification+with+Multi-round+New+Classes:+Formulation,+Dataset+and+System"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Incremental Few-shot Text Classification with Multi-round New Classes: Formulation, Dataset and System**](https://www.aclweb.org/anthology/2021.naacl-main.106) , <br> by *Xia, Congying  and
Yin, Wenpeng  and
Feng, Yihao  and
Yu, Philip* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2786-L2796) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```xia-etal-2021-incremental```
- [![](https://img.shields.io/badge/EMNLP-2020-green)](https://www.aclweb.org/anthology/2020.emnlp-main.660)<a href="https://scholar.google.com.hk/scholar?q=Universal+Natural+Language+Processing+with+Limited+Annotations:+Try+Few-shot+Textual+Entailment+as+a+Start"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Universal Natural Language Processing with Limited Annotations: Try Few-shot Textual Entailment as a Start**](https://www.aclweb.org/anthology/2020.emnlp-main.660) , <br> by *Yin, Wenpeng  and
Rajani, Nazneen Fatema  and
Radev, Dragomir  and
Socher, Richard  and
Xiong, Caiming* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2719-L2730) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```yin-etal-2020-universal```
## Jianguo Zhang

- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2109.06349)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Intent+Detection+via+Contrastive+Pre-Training+and+Fine-Tuning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Intent Detection via Contrastive Pre-Training and Fine-Tuning**](https://arxiv.org/abs/2109.06349) , <br> by *Jianguo Zhang, Trung Bui, Seunghyun Yoon, Xiang Chen, Zhiwei Liu, Congying Xia, Quan Hung Tran, Walter Chang and Philip Yu* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3279-L3286) <br>```EMNLP 2021
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhang2021fewshot```
- [![](https://img.shields.io/badge/EMNLP-2020-green)](https://www.aclweb.org/anthology/2020.emnlp-main.411)<a href="https://scholar.google.com.hk/scholar?q=Discriminative+Nearest+Neighbor+Few-Shot+Intent+Detection+by+Transferring+Natural+Language+Inference"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Discriminative Nearest Neighbor Few-Shot Intent Detection by Transferring Natural Language Inference**](https://www.aclweb.org/anthology/2020.emnlp-main.411) , <br> by *Zhang, Jianguo  and
Hashimoto, Kazuma  and
Liu, Wenhao  and
Wu, Chien-Sheng  and
Wan, Yao  and
Yu, Philip  and
Socher, Richard  and
Xiong, Caiming* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2666-L2680) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhang-etal-2020-discriminative```
## Tingwen Liu

- [![](https://img.shields.io/badge/EMNLP-2020-green)](https://www.aclweb.org/anthology/2020.emnlp-main.131)<a href="https://scholar.google.com.hk/scholar?q=Adaptive+Attentional+Network+for+Few-Shot+Knowledge+Graph+Completion"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Adaptive Attentional Network for Few-Shot Knowledge Graph Completion**](https://www.aclweb.org/anthology/2020.emnlp-main.131) , <br> by *Sheng, Jiawei  and
Guo, Shu  and
Chen, Zhenyu  and
Yue, Juwei  and
Wang, Lihong  and
Liu, Tingwen  and
Xu, Hongbo* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2625-L2638) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```sheng-etal-2020-adaptive```
- [![](https://img.shields.io/badge/CoRR-2020-green)](https://arxiv.org/abs/2012.02353)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Event+Detection+with+Prototypical+Amortized+Conditional+Random+Field"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Event Detection with Prototypical Amortized Conditional Random
Field**](https://arxiv.org/abs/2012.02353) , <br> by *Xin Cong and
Shiyao Cui and
Bowen Yu and
Tingwen Liu and
Yubin Wang and
Bin Wang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3080-L3093) <br>```ACL 2021
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2012-02353```
## Percy Liang

- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2101.00190)<a href="https://scholar.google.com.hk/scholar?q=Prefix-Tuning:+Optimizing+Continuous+Prompts+for+Generation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Prefix-Tuning: Optimizing Continuous Prompts for Generation**](https://arxiv.org/abs/2101.00190) , <br> by *Xiang Lisa Li and
Percy Liang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2890-L2898) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2101-00190```
- [![](https://img.shields.io/badge/ACL-2020-green)](https://www.aclweb.org/anthology/2020.acl-main.436)<a href="https://scholar.google.com.hk/scholar?q=Shaping+Visual+Representations+with+Language+for+Few-Shot+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Shaping Visual Representations with Language for Few-Shot Classification**](https://www.aclweb.org/anthology/2020.acl-main.436) , <br> by *Mu, Jesse  and
Liang, Percy  and
Goodman, Noah* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2541-L2550) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```mu-etal-2020-shaping```
## Ting Liu

- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2106.07343)<a href="https://scholar.google.com.hk/scholar?q=Learning+to+Bridge+Metric+Spaces:+Few-shot+Joint+Learning+of+Intent+Detection+and+Slot+Filling"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning to Bridge Metric Spaces: Few-shot Joint Learning of Intent Detection and Slot Filling**](https://arxiv.org/abs/2106.07343) , <br> by *Yutai Hou, Yongkui Lai, Cheng Chen, Wanxiang Che and Ting Liu* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3070-L3077) <br>```ACL Findings 2021 preprint
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```hou2021learning```
- [![](https://img.shields.io/badge/ACL-2020-green)](https://www.aclweb.org/anthology/2020.acl-main.128)<a href="https://scholar.google.com.hk/scholar?q=Few-shot+Slot+Tagging+with+Collapsed+Dependency+Transfer+and+Label-enhanced+Task-adaptive+Projection+Network"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-shot Slot Tagging with Collapsed Dependency Transfer and Label-enhanced Task-adaptive Projection Network**](https://www.aclweb.org/anthology/2020.acl-main.128) , <br> by *Hou, Yutai  and
Che, Wanxiang  and
Lai, Yongkui  and
Zhou, Zhihan  and
Liu, Yijia  and
Liu, Han  and
Liu, Ting* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2526-L2539) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```hou-etal-2020-shot```
## Yongkui Lai

- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2106.07343)<a href="https://scholar.google.com.hk/scholar?q=Learning+to+Bridge+Metric+Spaces:+Few-shot+Joint+Learning+of+Intent+Detection+and+Slot+Filling"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning to Bridge Metric Spaces: Few-shot Joint Learning of Intent Detection and Slot Filling**](https://arxiv.org/abs/2106.07343) , <br> by *Yutai Hou, Yongkui Lai, Cheng Chen, Wanxiang Che and Ting Liu* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3070-L3077) <br>```ACL Findings 2021 preprint
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```hou2021learning```
- [![](https://img.shields.io/badge/ACL-2020-green)](https://www.aclweb.org/anthology/2020.acl-main.128)<a href="https://scholar.google.com.hk/scholar?q=Few-shot+Slot+Tagging+with+Collapsed+Dependency+Transfer+and+Label-enhanced+Task-adaptive+Projection+Network"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-shot Slot Tagging with Collapsed Dependency Transfer and Label-enhanced Task-adaptive Projection Network**](https://www.aclweb.org/anthology/2020.acl-main.128) , <br> by *Hou, Yutai  and
Che, Wanxiang  and
Lai, Yongkui  and
Zhou, Zhihan  and
Liu, Yijia  and
Liu, Han  and
Liu, Ting* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2526-L2539) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```hou-etal-2020-shot```
## Wanxiang Che

- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2106.07343)<a href="https://scholar.google.com.hk/scholar?q=Learning+to+Bridge+Metric+Spaces:+Few-shot+Joint+Learning+of+Intent+Detection+and+Slot+Filling"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning to Bridge Metric Spaces: Few-shot Joint Learning of Intent Detection and Slot Filling**](https://arxiv.org/abs/2106.07343) , <br> by *Yutai Hou, Yongkui Lai, Cheng Chen, Wanxiang Che and Ting Liu* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3070-L3077) <br>```ACL Findings 2021 preprint
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```hou2021learning```
- [![](https://img.shields.io/badge/ACL-2020-green)](https://www.aclweb.org/anthology/2020.acl-main.128)<a href="https://scholar.google.com.hk/scholar?q=Few-shot+Slot+Tagging+with+Collapsed+Dependency+Transfer+and+Label-enhanced+Task-adaptive+Projection+Network"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-shot Slot Tagging with Collapsed Dependency Transfer and Label-enhanced Task-adaptive Projection Network**](https://www.aclweb.org/anthology/2020.acl-main.128) , <br> by *Hou, Yutai  and
Che, Wanxiang  and
Lai, Yongkui  and
Zhou, Zhihan  and
Liu, Yijia  and
Liu, Han  and
Liu, Ting* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2526-L2539) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```hou-etal-2020-shot```
## Yutai Hou

- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2106.07343)<a href="https://scholar.google.com.hk/scholar?q=Learning+to+Bridge+Metric+Spaces:+Few-shot+Joint+Learning+of+Intent+Detection+and+Slot+Filling"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning to Bridge Metric Spaces: Few-shot Joint Learning of Intent Detection and Slot Filling**](https://arxiv.org/abs/2106.07343) , <br> by *Yutai Hou, Yongkui Lai, Cheng Chen, Wanxiang Che and Ting Liu* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3070-L3077) <br>```ACL Findings 2021 preprint
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```hou2021learning```
- [![](https://img.shields.io/badge/ACL-2020-green)](https://www.aclweb.org/anthology/2020.acl-main.128)<a href="https://scholar.google.com.hk/scholar?q=Few-shot+Slot+Tagging+with+Collapsed+Dependency+Transfer+and+Label-enhanced+Task-adaptive+Projection+Network"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-shot Slot Tagging with Collapsed Dependency Transfer and Label-enhanced Task-adaptive Projection Network**](https://www.aclweb.org/anthology/2020.acl-main.128) , <br> by *Hou, Yutai  and
Che, Wanxiang  and
Lai, Yongkui  and
Zhou, Zhihan  and
Liu, Yijia  and
Liu, Han  and
Liu, Ting* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2526-L2539) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```hou-etal-2020-shot```
## Matthew Henderson

- [![](https://img.shields.io/badge/NAACL_HLT-2021-green)](https://www.aclweb.org/anthology/2021.naacl-main.264)<a href="https://scholar.google.com.hk/scholar?q=ConVEx:+Data-Efficient+and+Few-Shot+Slot+Labeling"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**ConVEx: Data-Efficient and Few-Shot Slot Labeling**](https://www.aclweb.org/anthology/2021.naacl-main.264) , <br> by *Henderson, Matthew  and
Vuli{\'c}, Ivan* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2828-L2836) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```henderson-vulic-2021-convex```
- [![](https://img.shields.io/badge/ACL-2020-green)](https://www.aclweb.org/anthology/2020.acl-main.11)<a href="https://scholar.google.com.hk/scholar?q=Span-ConveRT:+Few-shot+Span+Extraction+for+Dialog+with+Pretrained+Conversational+Representations"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Span-ConveRT: Few-shot Span Extraction for Dialog with Pretrained Conversational Representations**](https://www.aclweb.org/anthology/2020.acl-main.11) , <br> by *Coope, Samuel  and
Farghly, Tyler  and
Gerz, Daniela  and
Vuli{\'c}, Ivan  and
Henderson, Matthew* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2487-L2498) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```coope-etal-2020-span```
## Zhengbao Jiang

- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2107.13586)<a href="https://scholar.google.com.hk/scholar?q=Pre-train,+Prompt,+and+Predict:+A+Systematic+Survey+of+Prompting+Methods+in+Natural+Language+Processing"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing**](https://arxiv.org/abs/2107.13586) , <br> by *Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi and Graham Neubig* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3112-L3120) <br>```Prompt-based learning -- survey paper
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```liu2021pretrain```
- [![](https://img.shields.io/badge/Trans._Assoc._Comput._Linguistics-2020-green)](https://transacl.org/ojs/index.php/tacl/article/view/1983)<a href="https://scholar.google.com.hk/scholar?q=How+Can+We+Know+What+Language+Models+Know"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**How Can We Know What Language Models Know**](https://transacl.org/ojs/index.php/tacl/article/view/1983) , <br> by *Zhengbao Jiang and
Frank F. Xu and
Jun Araki and
Graham Neubig* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2461-L2472) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```JiangXAN20```
## Jason Eisner

- [![](https://img.shields.io/badge/NAACL_HLT-2021-green)](https://www.aclweb.org/anthology/2021.naacl-main.410)<a href="https://scholar.google.com.hk/scholar?q=Learning+How+to+Ask:+Querying+LMs+with+Mixtures+of+Soft+Prompts"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning How to Ask: Querying LMs with Mixtures of Soft Prompts**](https://www.aclweb.org/anthology/2021.naacl-main.410) , <br> by *Qin, Guanghui  and
Eisner, Jason* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2420-L2428) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```qin-eisner-2021-learning```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2104.08768)<a href="https://scholar.google.com.hk/scholar?q=Constrained+Language+Models+Yield+Few-Shot+Semantic+Parsers"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Constrained Language Models Yield Few-Shot Semantic Parsers**](https://arxiv.org/abs/2104.08768) , <br> by *Richard Shin and
Christopher H. Lin and
Sam Thomson and
Charles Chen and
Subhro Roy and
Emmanouil Antonios Platanios and
Adam Pauls and
Dan Klein and
Jason Eisner and
Benjamin Van Durme* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3020-L3036) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2104-08768```
## Boris N. Oreshkin

- [![](https://img.shields.io/badge/NeurIPS-2019-green)](https://proceedings.neurips.cc/paper/2019/hash/d790c9e6c0b5e02c87b375e782ac01bc-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Adaptive+Cross-Modal+Few-shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Adaptive Cross-Modal Few-shot Learning**](https://proceedings.neurips.cc/paper/2019/hash/d790c9e6c0b5e02c87b375e782ac01bc-Abstract.html) , <br> by *Chen Xing and
Negar Rostamzadeh and
Boris N. Oreshkin and
Pedro O. Pinheiro* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2218-L2228) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```XingROP19```
- [![](https://img.shields.io/badge/NeurIPS-2018-green)](https://proceedings.neurips.cc/paper/2018/hash/66808e327dc79d135ba18e051673d906-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=TADAM:+Task+dependent+adaptive+metric+for+improved+few-shot+learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**TADAM: Task dependent adaptive metric for improved few-shot learning**](https://proceedings.neurips.cc/paper/2018/hash/66808e327dc79d135ba18e051673d906-Abstract.html) , <br> by *Boris N. Oreshkin and
Pau Rodr{\'{\i}}guez L{\'{o}}pez and
Alexandre Lacoste* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2303-L2312) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```OreshkinLL18```
## Giulia Denevi

- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/0a716fe8c7745e51a3185fc8be6ca23a-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=The+Advantage+of+Conditional+Meta-Learning+for+Biased+Regularization+and+Fine+Tuning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**The Advantage of Conditional Meta-Learning for Biased Regularization
and Fine Tuning**](https://proceedings.neurips.cc/paper/2020/hash/0a716fe8c7745e51a3185fc8be6ca23a-Abstract.html) , <br> by *Giulia Denevi and
Massimiliano Pontil and
Carlo Ciliberto* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2038-L2047) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```DeneviPC20```
- [![](https://img.shields.io/badge/NeurIPS-2018-green)](https://proceedings.neurips.cc/paper/2018/hash/b9a25e422ba96f7572089a00b838c3f8-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Learning+To+Learn+Around+A+Common+Mean"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning To Learn Around A Common Mean**](https://proceedings.neurips.cc/paper/2018/hash/b9a25e422ba96f7572089a00b838c3f8-Abstract.html) , <br> by *Giulia Denevi and
Carlo Ciliberto and
Dimitris Stamos and
Massimiliano Pontil* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2314-L2324) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```DeneviCSP18```
## Qianru Sun

- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/1cc8a8ea51cd0adddf5dab504a285915-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Interventional+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Interventional Few-Shot Learning**](https://proceedings.neurips.cc/paper/2020/hash/1cc8a8ea51cd0adddf5dab504a285915-Abstract.html) , <br> by *Zhongqi Yue and
Hanwang Zhang and
Qianru Sun and
Xian{-}Sheng Hua* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1935-L1944) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```YueZS020```
- [![](https://img.shields.io/badge/NeurIPS-2019-green)](https://proceedings.neurips.cc/paper/2019/hash/bf25356fd2a6e038f1a3a59c26687e80-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Learning+to+Self-Train+for+Semi-Supervised+Few-Shot+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning to Self-Train for Semi-Supervised Few-Shot Classification**](https://proceedings.neurips.cc/paper/2019/hash/bf25356fd2a6e038f1a3a59c26687e80-Abstract.html) , <br> by *Xinzhe Li and
Qianru Sun and
Yaoyao Liu and
Qin Zhou and
Shibao Zheng and
Tat{-}Seng Chua and
Bernt Schiele* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2241-L2254) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```LiSLZZCS19```
## Hanwang Zhang

- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/1cc8a8ea51cd0adddf5dab504a285915-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Interventional+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Interventional Few-Shot Learning**](https://proceedings.neurips.cc/paper/2020/hash/1cc8a8ea51cd0adddf5dab504a285915-Abstract.html) , <br> by *Zhongqi Yue and
Hanwang Zhang and
Qianru Sun and
Xian{-}Sheng Hua* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1935-L1944) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```YueZS020```
- [![](https://img.shields.io/badge/NeurIPS-2018-green)](https://proceedings.neurips.cc/paper/2018/hash/81448138f5f163ccdba4acc69819f280-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Low-shot+Learning+via+Covariance-Preserving+Adversarial+Augmentation+Networks"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Low-shot Learning via Covariance-Preserving Adversarial Augmentation
Networks**](https://proceedings.neurips.cc/paper/2018/hash/81448138f5f163ccdba4acc69819f280-Abstract.html) , <br> by *Hang Gao and
Zheng Shou and
Alireza Zareian and
Hanwang Zhang and
Shih{-}Fu Chang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2326-L2338) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```GaoSZZC18```
## Jake Snell

- [![](https://img.shields.io/badge/ICLR-2018-green)](https://openreview.net/forum?id=HJcSzz-CZ)<a href="https://scholar.google.com.hk/scholar?q=Meta-Learning+for+Semi-Supervised+Few-Shot+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta-Learning for Semi-Supervised Few-Shot Classification**](https://openreview.net/forum?id=HJcSzz-CZ) , <br> by *Mengye Ren, Sachin Ravi, Eleni Triantafillou, Jake Snell, Kevin Swersky, Josh B. Tenenbaum, Hugo Larochelle and Richard S. Zemel* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1876-L1882) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ren2018metalearning```
- [![](https://img.shields.io/badge/NeurIPS-2017-green)](https://proceedings.neurips.cc/paper/2017/hash/cb8da6767461f2812ae4290eac7cbc42-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Prototypical+Networks+for+Few-shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Prototypical Networks for Few-shot Learning**](https://proceedings.neurips.cc/paper/2017/hash/cb8da6767461f2812ae4290eac7cbc42-Abstract.html) , <br> by *Jake Snell and
Kevin Swersky and
Richard S. Zemel* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2351-L2360) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```SnellSZ17```
## Sachin Ravi

- [![](https://img.shields.io/badge/ICLR-2018-green)](https://openreview.net/forum?id=HJcSzz-CZ)<a href="https://scholar.google.com.hk/scholar?q=Meta-Learning+for+Semi-Supervised+Few-Shot+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta-Learning for Semi-Supervised Few-Shot Classification**](https://openreview.net/forum?id=HJcSzz-CZ) , <br> by *Mengye Ren, Sachin Ravi, Eleni Triantafillou, Jake Snell, Kevin Swersky, Josh B. Tenenbaum, Hugo Larochelle and Richard S. Zemel* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1876-L1882) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ren2018metalearning```
- [![](https://img.shields.io/badge/ICLR-2017-green)](https://openreview.net/forum?id=rJY0-Kcll)<a href="https://scholar.google.com.hk/scholar?q=Optimization+as+a+Model+for+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Optimization as a Model for Few-Shot Learning**](https://openreview.net/forum?id=rJY0-Kcll) , <br> by *Sachin Ravi and
Hugo Larochelle* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1902-L1909) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```RaviL17```
## Jia-Bin Huang

- [![](https://img.shields.io/badge/ICLR-2020-green)](https://openreview.net/forum?id=SJl5Np4tPr)<a href="https://scholar.google.com.hk/scholar?q=Cross-Domain+Few-Shot+Classification+via+Learned+Feature-Wise+Transformation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Cross-Domain Few-Shot Classification via Learned Feature-Wise Transformation**](https://openreview.net/forum?id=SJl5Np4tPr) , <br> by *Hung-Yu Tseng, Hsin-Ying Lee, Jia-Bin Huang and Ming-Hsuan Yang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1795-L1801) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Tseng2020Cross-Domain```
- [![](https://img.shields.io/badge/ICLR-2019-green)](https://openreview.net/forum?id=HkxLXnAcFQ)<a href="https://scholar.google.com.hk/scholar?q=A+Closer+Look+at+Few-shot+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**A Closer Look at Few-shot Classification**](https://openreview.net/forum?id=HkxLXnAcFQ) , <br> by *Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang and Jia-Bin Huang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1813-L1819) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```chen2018a```
## Minseop Park

- [![](https://img.shields.io/badge/ICLR-2020-green)](https://openreview.net/forum?id=rkeZIJBYvr)<a href="https://scholar.google.com.hk/scholar?q=Learning+to+Balance:+Bayesian+Meta-Learning+for+Imbalanced+and+Out-of-distribution+Tasks"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning to Balance: Bayesian Meta-Learning for Imbalanced and Out-of-distribution Tasks**](https://openreview.net/forum?id=rkeZIJBYvr) , <br> by *Hae Beom Lee, Hayeon Lee, Donghyun Na, Saehoon Kim, Minseop Park, Eunho Yang and Sung Ju Hwang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1740-L1746) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Lee2020Learning```
- [![](https://img.shields.io/badge/ICLR-2019-green)](https://openreview.net/forum?id=SyVuRiC5K7)<a href="https://scholar.google.com.hk/scholar?q=LEARNING+TO+PROPAGATE+LABELS:+TRANSDUCTIVE+PROPAGATION+NETWORK+FOR+FEW-SHOT+LEARNING"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING**](https://openreview.net/forum?id=SyVuRiC5K7) , <br> by *Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, Eunho Yang, Sungju Hwang and Yi Yang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1822-L1828) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```liu2018learning```
## Saehoon Kim

- [![](https://img.shields.io/badge/ICLR-2020-green)](https://openreview.net/forum?id=rkeZIJBYvr)<a href="https://scholar.google.com.hk/scholar?q=Learning+to+Balance:+Bayesian+Meta-Learning+for+Imbalanced+and+Out-of-distribution+Tasks"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning to Balance: Bayesian Meta-Learning for Imbalanced and Out-of-distribution Tasks**](https://openreview.net/forum?id=rkeZIJBYvr) , <br> by *Hae Beom Lee, Hayeon Lee, Donghyun Na, Saehoon Kim, Minseop Park, Eunho Yang and Sung Ju Hwang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1740-L1746) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Lee2020Learning```
- [![](https://img.shields.io/badge/ICLR-2019-green)](https://openreview.net/forum?id=SyVuRiC5K7)<a href="https://scholar.google.com.hk/scholar?q=LEARNING+TO+PROPAGATE+LABELS:+TRANSDUCTIVE+PROPAGATION+NETWORK+FOR+FEW-SHOT+LEARNING"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING**](https://openreview.net/forum?id=SyVuRiC5K7) , <br> by *Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, Eunho Yang, Sungju Hwang and Yi Yang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1822-L1828) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```liu2018learning```
## Tsendsuren Munkhdalai

- [![](https://img.shields.io/badge/EMNLP-2020-green)](https://www.aclweb.org/anthology/2020.emnlp-main.38)<a href="https://scholar.google.com.hk/scholar?q=Self-Supervised+Meta-Learning+for+Few-Shot+Natural+Language+Classification+Tasks"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Self-Supervised Meta-Learning for Few-Shot Natural Language Classification Tasks**](https://www.aclweb.org/anthology/2020.emnlp-main.38) , <br> by *Bansal, Trapit  and
Jha, Rishikesh  and
Munkhdalai, Tsendsuren  and
McCallum, Andrew* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2613-L2623) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```bansal-etal-2020-self```
- [![](https://img.shields.io/badge/ICML-2017-green)](http://proceedings.mlr.press/v70/munkhdalai17a.html)<a href="https://scholar.google.com.hk/scholar?q=Meta+Networks"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta Networks**](http://proceedings.mlr.press/v70/munkhdalai17a.html) , <br> by *Tsendsuren Munkhdalai and Hong Yu* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1663-L1670) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v70-munkhdalai17a```
## Junzhou Huang

- [![](https://img.shields.io/badge/ICLR-2020-green)](https://openreview.net/forum?id=r1eowANFvr)<a href="https://scholar.google.com.hk/scholar?q=Towards+Fast+Adaptation+of+Neural+Architectures+with+Meta+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Towards Fast Adaptation of Neural Architectures with Meta Learning**](https://openreview.net/forum?id=r1eowANFvr) , <br> by *Dongze Lian, Yin Zheng, Yintao Xu, Yanxiong Lu, Leyu Lin, Peilin Zhao, Junzhou Huang and Shenghua Gao* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1702-L1708) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Lian2020Towards```
- [![](https://img.shields.io/badge/ICML-2019-green)](
http://proceedings.mlr.press/v97/yao19b.html)<a href="https://scholar.google.com.hk/scholar?q=Hierarchically+Structured+Meta-learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Hierarchically Structured Meta-learning**](
http://proceedings.mlr.press/v97/yao19b.html) , <br> by *Yao, Huaxiu, Wei, Ying, Huang, Junzhou and Li, Zhenhui* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1591-L1600) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v97-yao19b```
## Jaekyun Moon

- [![](https://img.shields.io/badge/ICML-2020-green)](http://proceedings.mlr.press/v119/yoon20b.html)<a href="https://scholar.google.com.hk/scholar?q=XtarNet:+Learning+to+Extract+Task-Adaptive+Representation+for+Incremental+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**XtarNet: Learning to Extract Task-Adaptive Representation for Incremental Few-Shot Learning**](http://proceedings.mlr.press/v119/yoon20b.html) , <br> by *Yoon, Sung Whan, Kim, Do-Yeon, Seo, Jun and Moon, Jaekyun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1553-L1560) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v119-yoon20b```
- [![](https://img.shields.io/badge/ICML-2019-green)](http://proceedings.mlr.press/v97/yoon19a.html)<a href="https://scholar.google.com.hk/scholar?q=TapNet:+Neural+Network+Augmented+with+Task-Adaptive+Projection+for+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**TapNet: Neural Network Augmented with Task-Adaptive Projection for Few-Shot Learning**](http://proceedings.mlr.press/v97/yoon19a.html) , <br> by *Yoon, Sung Whan, Seo, Jun and Moon, Jaekyun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1582-L1589) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v97-yoon19a```
## Jun Seo

- [![](https://img.shields.io/badge/ICML-2020-green)](http://proceedings.mlr.press/v119/yoon20b.html)<a href="https://scholar.google.com.hk/scholar?q=XtarNet:+Learning+to+Extract+Task-Adaptive+Representation+for+Incremental+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**XtarNet: Learning to Extract Task-Adaptive Representation for Incremental Few-Shot Learning**](http://proceedings.mlr.press/v119/yoon20b.html) , <br> by *Yoon, Sung Whan, Kim, Do-Yeon, Seo, Jun and Moon, Jaekyun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1553-L1560) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v119-yoon20b```
- [![](https://img.shields.io/badge/ICML-2019-green)](http://proceedings.mlr.press/v97/yoon19a.html)<a href="https://scholar.google.com.hk/scholar?q=TapNet:+Neural+Network+Augmented+with+Task-Adaptive+Projection+for+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**TapNet: Neural Network Augmented with Task-Adaptive Projection for Few-Shot Learning**](http://proceedings.mlr.press/v97/yoon19a.html) , <br> by *Yoon, Sung Whan, Seo, Jun and Moon, Jaekyun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1582-L1589) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v97-yoon19a```
## Sung Whan Yoon

- [![](https://img.shields.io/badge/ICML-2020-green)](http://proceedings.mlr.press/v119/yoon20b.html)<a href="https://scholar.google.com.hk/scholar?q=XtarNet:+Learning+to+Extract+Task-Adaptive+Representation+for+Incremental+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**XtarNet: Learning to Extract Task-Adaptive Representation for Incremental Few-Shot Learning**](http://proceedings.mlr.press/v119/yoon20b.html) , <br> by *Yoon, Sung Whan, Kim, Do-Yeon, Seo, Jun and Moon, Jaekyun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1553-L1560) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v119-yoon20b```
- [![](https://img.shields.io/badge/ICML-2019-green)](http://proceedings.mlr.press/v97/yoon19a.html)<a href="https://scholar.google.com.hk/scholar?q=TapNet:+Neural+Network+Augmented+with+Task-Adaptive+Projection+for+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**TapNet: Neural Network Augmented with Task-Adaptive Projection for Few-Shot Learning**](http://proceedings.mlr.press/v97/yoon19a.html) , <br> by *Yoon, Sung Whan, Seo, Jun and Moon, Jaekyun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1582-L1589) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v97-yoon19a```
## Liam Fowl

- [![](https://img.shields.io/badge/ICML-2020-green)](http://proceedings.mlr.press/v119/goldblum20a.html)<a href="https://scholar.google.com.hk/scholar?q=Unraveling+Meta-Learning:+Understanding+Feature+Representations+for+Few-Shot+Tasks"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Unraveling Meta-Learning: Understanding Feature Representations for Few-Shot Tasks**](http://proceedings.mlr.press/v119/goldblum20a.html) , <br> by *Goldblum, Micah, Reich, Steven, Fowl, Liam, Ni, Renkun, Cherepanova, Valeriia and Goldstein, Tom* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1494-L1501) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v119-goldblum20a```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/cfee398643cbc3dc5eefc89334cacdc1-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Adversarially+Robust+Few-Shot+Learning:+A+Meta-Learning+Approach"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Adversarially Robust Few-Shot Learning: A Meta-Learning Approach**](https://proceedings.neurips.cc/paper/2020/hash/cfee398643cbc3dc5eefc89334cacdc1-Abstract.html) , <br> by *Micah Goldblum and
Liam Fowl and
Tom Goldstein* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2005-L2013) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```GoldblumFG20```
## Richard Turner

- [![](https://img.shields.io/badge/ICML-2020-green)](http://proceedings.mlr.press/v119/bronskill20a.html)<a href="https://scholar.google.com.hk/scholar?q=TaskNorm:+Rethinking+Batch+Normalization+for+Meta-Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**TaskNorm: Rethinking Batch Normalization for Meta-Learning**](http://proceedings.mlr.press/v119/bronskill20a.html) , <br> by *Bronskill, John, Gordon, Jonathan, Requeima, James, Nowozin, Sebastian and Turner, Richard* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1484-L1491) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v119-bronskill20a```
- [![](https://img.shields.io/badge/ICLR-2019-green)](https://openreview.net/forum?id=HkxStoC5F7)<a href="https://scholar.google.com.hk/scholar?q=Meta-Learning+Probabilistic+Inference+for+Prediction"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta-Learning Probabilistic Inference for Prediction**](https://openreview.net/forum?id=HkxStoC5F7) , <br> by *Jonathan Gordon, John Bronskill, Matthias Bauer, Sebastian Nowozin and Richard Turner* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1840-L1846) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```gordon2018metalearning```
## Sebastian Nowozin

- [![](https://img.shields.io/badge/ICML-2020-green)](http://proceedings.mlr.press/v119/bronskill20a.html)<a href="https://scholar.google.com.hk/scholar?q=TaskNorm:+Rethinking+Batch+Normalization+for+Meta-Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**TaskNorm: Rethinking Batch Normalization for Meta-Learning**](http://proceedings.mlr.press/v119/bronskill20a.html) , <br> by *Bronskill, John, Gordon, Jonathan, Requeima, James, Nowozin, Sebastian and Turner, Richard* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1484-L1491) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v119-bronskill20a```
- [![](https://img.shields.io/badge/ICLR-2019-green)](https://openreview.net/forum?id=HkxStoC5F7)<a href="https://scholar.google.com.hk/scholar?q=Meta-Learning+Probabilistic+Inference+for+Prediction"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta-Learning Probabilistic Inference for Prediction**](https://openreview.net/forum?id=HkxStoC5F7) , <br> by *Jonathan Gordon, John Bronskill, Matthias Bauer, Sebastian Nowozin and Richard Turner* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1840-L1846) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```gordon2018metalearning```
## Jonathan Gordon

- [![](https://img.shields.io/badge/ICML-2020-green)](http://proceedings.mlr.press/v119/bronskill20a.html)<a href="https://scholar.google.com.hk/scholar?q=TaskNorm:+Rethinking+Batch+Normalization+for+Meta-Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**TaskNorm: Rethinking Batch Normalization for Meta-Learning**](http://proceedings.mlr.press/v119/bronskill20a.html) , <br> by *Bronskill, John, Gordon, Jonathan, Requeima, James, Nowozin, Sebastian and Turner, Richard* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1484-L1491) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v119-bronskill20a```
- [![](https://img.shields.io/badge/ICLR-2019-green)](https://openreview.net/forum?id=HkxStoC5F7)<a href="https://scholar.google.com.hk/scholar?q=Meta-Learning+Probabilistic+Inference+for+Prediction"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta-Learning Probabilistic Inference for Prediction**](https://openreview.net/forum?id=HkxStoC5F7) , <br> by *Jonathan Gordon, John Bronskill, Matthias Bauer, Sebastian Nowozin and Richard Turner* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1840-L1846) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```gordon2018metalearning```
## John Bronskill

- [![](https://img.shields.io/badge/ICML-2020-green)](http://proceedings.mlr.press/v119/bronskill20a.html)<a href="https://scholar.google.com.hk/scholar?q=TaskNorm:+Rethinking+Batch+Normalization+for+Meta-Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**TaskNorm: Rethinking Batch Normalization for Meta-Learning**](http://proceedings.mlr.press/v119/bronskill20a.html) , <br> by *Bronskill, John, Gordon, Jonathan, Requeima, James, Nowozin, Sebastian and Turner, Richard* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1484-L1491) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v119-bronskill20a```
- [![](https://img.shields.io/badge/ICLR-2019-green)](https://openreview.net/forum?id=HkxStoC5F7)<a href="https://scholar.google.com.hk/scholar?q=Meta-Learning+Probabilistic+Inference+for+Prediction"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta-Learning Probabilistic Inference for Prediction**](https://openreview.net/forum?id=HkxStoC5F7) , <br> by *Jonathan Gordon, John Bronskill, Matthias Bauer, Sebastian Nowozin and Richard Turner* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1840-L1846) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```gordon2018metalearning```
## Jason D. Lee

- [![](https://img.shields.io/badge/ICLR-2021-green)](https://openreview.net/forum?id=pW2Q2xLwIMD)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Learning+via+Learning+the+Representation,+Provably"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Learning via Learning the Representation, Provably**](https://openreview.net/forum?id=pW2Q2xLwIMD) , <br> by *Simon Shaolei Du, Wei Hu, Sham M. Kakade, Jason D. Lee and Qi Lei* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1368-L1374) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```du2021fewshot```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/84c578f202616448a2f80e6f56d5f16d-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Convergence+of+Meta-Learning+with+Task-Specific+Adaptation+over+Partial+Parameters"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Convergence of Meta-Learning with Task-Specific Adaptation over Partial
Parameters**](https://proceedings.neurips.cc/paper/2020/hash/84c578f202616448a2f80e6f56d5f16d-Abstract.html) , <br> by *Kaiyi Ji and
Jason D. Lee and
Yingbin Liang and
H. Vincent Poor* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2126-L2136) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```JiLLP20```
## Lu Liu

- [![](https://img.shields.io/badge/ICLR-2021-green)](https://openreview.net/forum?id=JWOiYxMG92s)<a href="https://scholar.google.com.hk/scholar?q=Free+Lunch+for+Few-shot+Learning:+Distribution+Calibration"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Free Lunch for Few-shot Learning: Distribution Calibration**](https://openreview.net/forum?id=JWOiYxMG92s) , <br> by *Shuo Yang, Lu Liu and Min Xu* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1341-L1347) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```yang2021free```
- [![](https://img.shields.io/badge/ICLR-2021-green)](https://openreview.net/forum?id=04cII6MumYV)<a href="https://scholar.google.com.hk/scholar?q=A+Universal+Representation+Transformer+Layer+for+Few-Shot+Image+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**A Universal Representation Transformer Layer for Few-Shot Image Classification**](https://openreview.net/forum?id=04cII6MumYV) , <br> by *Lu Liu, William L. Hamilton, Guodong Long, Jing Jiang and Hugo Larochelle* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1377-L1383) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```liu2021a```
## Mubarak Shah

- [![](https://img.shields.io/badge/CVPR-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Rizve_Exploring_Complementary_Strengths_of_Invariant_and_Equivariant_Representations_for_Few-Shot_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Exploring+Complementary+Strengths+of+Invariant+and+Equivariant+Representations+for+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Exploring Complementary Strengths of Invariant and Equivariant Representations for Few-Shot Learning**](https://openaccess.thecvf.com/content/CVPR2021/html/Rizve_Exploring_Complementary_Strengths_of_Invariant_and_Equivariant_Representations_for_Few-Shot_CVPR_2021_paper.html) , <br> by *Rizve, Mamshad Nayeem, Khan, Salman, Khan, Fahad Shahbaz and Shah, Mubarak* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1300-L1308) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Rizve_2021_CVPR```
- [![](https://img.shields.io/badge/NeurIPS-2019-green)](https://proceedings.neurips.cc/paper/2019/hash/fd0a5a5e367a0955d81278062ef37429-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Unsupervised+Meta-Learning+for+Few-Shot+Image+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Unsupervised Meta-Learning for Few-Shot Image Classification**](https://proceedings.neurips.cc/paper/2019/hash/fd0a5a5e367a0955d81278062ef37429-Abstract.html) , <br> by *Siavash Khodadadeh and
Ladislau B{\"{o}}l{\"{o}}ni and
Mubarak Shah* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2230-L2239) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```KhodadadehBS19```
## Feiyue Huang

- [![](https://img.shields.io/badge/CVPR-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Xu_Learning_Dynamic_Alignment_via_Meta-Filter_for_Few-Shot_Learning_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Learning+Dynamic+Alignment+via+Meta-Filter+for+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning Dynamic Alignment via Meta-Filter for Few-Shot Learning**](https://openaccess.thecvf.com/content/CVPR2021/html/Xu_Learning_Dynamic_Alignment_via_Meta-Filter_for_Few-Shot_Learning_CVPR_2021_paper.html) , <br> by *Xu, Chengming, Fu, Yanwei, Liu, Chen, Wang, Chengjie, Li, Jilin, Huang, Feiyue, Zhang, Li and Xue, Xiangyang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1290-L1298) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Xu_2021_CVPR```
- [![](https://img.shields.io/badge/ICML-2019-green)](http://proceedings.mlr.press/v97/li19c.html)<a href="https://scholar.google.com.hk/scholar?q=LGM-Net:+Learning+to+Generate+Matching+Networks+for+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**LGM-Net: Learning to Generate Matching Networks for Few-Shot Learning**](http://proceedings.mlr.press/v97/li19c.html) , <br> by *Li, Huaiyu, Dong, Weiming, Mei, Xing, Ma, Chongyang, Huang, Feiyue and Hu, Bao-Gang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1573-L1580) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v97-li19c```
## Yanwei Fu

- [![](https://img.shields.io/badge/CVPR-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Xu_Learning_Dynamic_Alignment_via_Meta-Filter_for_Few-Shot_Learning_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Learning+Dynamic+Alignment+via+Meta-Filter+for+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning Dynamic Alignment via Meta-Filter for Few-Shot Learning**](https://openaccess.thecvf.com/content/CVPR2021/html/Xu_Learning_Dynamic_Alignment_via_Meta-Filter_for_Few-Shot_Learning_CVPR_2021_paper.html) , <br> by *Xu, Chengming, Fu, Yanwei, Liu, Chen, Wang, Chengjie, Li, Jilin, Huang, Feiyue, Zhang, Li and Xue, Xiangyang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1290-L1298) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Xu_2021_CVPR```
- [![](https://img.shields.io/badge/ICML-2018-green)](http://proceedings.mlr.press/v80/zhao18c.html)<a href="https://scholar.google.com.hk/scholar?q=MSplit+LBI:+Realizing+Feature+Selection+and+Dense+Estimation+Simultaneously+in+Few-shot+and+Zero-shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**MSplit LBI: Realizing Feature Selection and Dense Estimation Simultaneously in Few-shot and Zero-shot Learning**](http://proceedings.mlr.press/v80/zhao18c.html) , <br> by *Zhao, Bo, Sun, Xinwei, Fu, Yanwei, Yao, Yuan and Wang, Yizhou* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1614-L1621) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v80-zhao18c```
## Lars Petersson

- [![](https://img.shields.io/badge/CVPR-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Hong_Reinforced_Attention_for_Few-Shot_Learning_and_Beyond_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Reinforced+Attention+for+Few-Shot+Learning+and+Beyond"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Reinforced Attention for Few-Shot Learning and Beyond**](https://openaccess.thecvf.com/content/CVPR2021/html/Hong_Reinforced_Attention_for_Few-Shot_Learning_and_Beyond_CVPR_2021_paper.html) , <br> by *Hong, Jie, Fang, Pengfei, Li, Weihao, Zhang, Tong, Simon, Christian, Harandi, Mehrtash and Petersson, Lars* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1260-L1268) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Hong_2021_CVPR```
- [![](https://img.shields.io/badge/the_IEEE/CVF_Conference_on_Computer_Vision_and_Pattern_Recognition_(CVPR)-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Cheraghian_Semantic-Aware_Knowledge_Distillation_for_Few-Shot_Class-Incremental_Learning_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Semantic-Aware+Knowledge+Distillation+for+Few-Shot+Class-Incremental+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Semantic-Aware Knowledge Distillation for Few-Shot Class-Incremental Learning**](https://openaccess.thecvf.com/content/CVPR2021/html/Cheraghian_Semantic-Aware_Knowledge_Distillation_for_Few-Shot_Class-Incremental_Learning_CVPR_2021_paper.html) , <br> by *Cheraghian, Ali, Rahman, Shafin, Fang, Pengfei, Roy, Soumava Kumar, Petersson, Lars and Harandi, Mehrtash* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1330-L1338) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Cheraghian_2021_CVPR```
## Mehrtash Harandi

- [![](https://img.shields.io/badge/CVPR-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Hong_Reinforced_Attention_for_Few-Shot_Learning_and_Beyond_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Reinforced+Attention+for+Few-Shot+Learning+and+Beyond"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Reinforced Attention for Few-Shot Learning and Beyond**](https://openaccess.thecvf.com/content/CVPR2021/html/Hong_Reinforced_Attention_for_Few-Shot_Learning_and_Beyond_CVPR_2021_paper.html) , <br> by *Hong, Jie, Fang, Pengfei, Li, Weihao, Zhang, Tong, Simon, Christian, Harandi, Mehrtash and Petersson, Lars* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1260-L1268) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Hong_2021_CVPR```
- [![](https://img.shields.io/badge/the_IEEE/CVF_Conference_on_Computer_Vision_and_Pattern_Recognition_(CVPR)-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Cheraghian_Semantic-Aware_Knowledge_Distillation_for_Few-Shot_Class-Incremental_Learning_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Semantic-Aware+Knowledge+Distillation+for+Few-Shot+Class-Incremental+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Semantic-Aware Knowledge Distillation for Few-Shot Class-Incremental Learning**](https://openaccess.thecvf.com/content/CVPR2021/html/Cheraghian_Semantic-Aware_Knowledge_Distillation_for_Few-Shot_Class-Incremental_Learning_CVPR_2021_paper.html) , <br> by *Cheraghian, Ali, Rahman, Shafin, Fang, Pengfei, Roy, Soumava Kumar, Petersson, Lars and Harandi, Mehrtash* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1330-L1338) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Cheraghian_2021_CVPR```
## Pengfei Fang

- [![](https://img.shields.io/badge/CVPR-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Hong_Reinforced_Attention_for_Few-Shot_Learning_and_Beyond_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Reinforced+Attention+for+Few-Shot+Learning+and+Beyond"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Reinforced Attention for Few-Shot Learning and Beyond**](https://openaccess.thecvf.com/content/CVPR2021/html/Hong_Reinforced_Attention_for_Few-Shot_Learning_and_Beyond_CVPR_2021_paper.html) , <br> by *Hong, Jie, Fang, Pengfei, Li, Weihao, Zhang, Tong, Simon, Christian, Harandi, Mehrtash and Petersson, Lars* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1260-L1268) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Hong_2021_CVPR```
- [![](https://img.shields.io/badge/the_IEEE/CVF_Conference_on_Computer_Vision_and_Pattern_Recognition_(CVPR)-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Cheraghian_Semantic-Aware_Knowledge_Distillation_for_Few-Shot_Class-Incremental_Learning_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Semantic-Aware+Knowledge+Distillation+for+Few-Shot+Class-Incremental+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Semantic-Aware Knowledge Distillation for Few-Shot Class-Incremental Learning**](https://openaccess.thecvf.com/content/CVPR2021/html/Cheraghian_Semantic-Aware_Knowledge_Distillation_for_Few-Shot_Class-Incremental_Learning_CVPR_2021_paper.html) , <br> by *Cheraghian, Ali, Rahman, Shafin, Fang, Pengfei, Roy, Soumava Kumar, Petersson, Lars and Harandi, Mehrtash* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1330-L1338) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Cheraghian_2021_CVPR```
## Bharath Hariharan

- [![](https://img.shields.io/badge/CVPR-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Wertheimer_Few-Shot_Classification_With_Feature_Map_Reconstruction_Networks_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Classification+With+Feature+Map+Reconstruction+Networks"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Classification With Feature Map Reconstruction Networks**](https://openaccess.thecvf.com/content/CVPR2021/html/Wertheimer_Few-Shot_Classification_With_Feature_Map_Reconstruction_Networks_CVPR_2021_paper.html) , <br> by *Wertheimer, Davis, Tang, Luming and Hariharan, Bharath* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1240-L1248) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Wertheimer_2021_CVPR```
- [![](https://img.shields.io/badge/ICLR-2021-green)](https://openreview.net/forum?id=O3Y56aqpChA)<a href="https://scholar.google.com.hk/scholar?q=Self-training+For+Few-shot+Transfer+Across+Extreme+Task+Differences"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Self-training For Few-shot Transfer Across Extreme Task Differences**](https://openreview.net/forum?id=O3Y56aqpChA) , <br> by *Cheng Perng Phoo and Bharath Hariharan* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1350-L1356) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```phoo2021selftraining```
## Jose Dolz

- [![](https://img.shields.io/badge/CVPR-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Boudiaf_Few-Shot_Segmentation_Without_Meta-Learning_A_Good_Transductive_Inference_Is_All_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Segmentation+Without+Meta-Learning:+A+Good+Transductive+Inference+Is+All+You+Need?"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Segmentation Without Meta-Learning: A Good Transductive Inference Is All You Need?**](https://openaccess.thecvf.com/content/CVPR2021/html/Boudiaf_Few-Shot_Segmentation_Without_Meta-Learning_A_Good_Transductive_Inference_Is_All_CVPR_2021_paper.html) , <br> by *Boudiaf, Malik, Kervadec, Hoel, Masud, Ziko Imtiaz, Piantanida, Pablo, Ben Ayed, Ismail and Dolz, Jose* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1210-L1218) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Boudiaf_2021_CVPR```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/196f5641aa9dc87067da4ff90fd81e7b-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Information+Maximization+for+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Information Maximization for Few-Shot Learning**](https://proceedings.neurips.cc/paper/2020/hash/196f5641aa9dc87067da4ff90fd81e7b-Abstract.html) , <br> by *Malik Boudiaf and
Imtiaz Masud Ziko and
J{\'{e}}r{\^{o}}me Rony and
Jose Dolz and
Pablo Piantanida and
Ismail Ben Ayed* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1922-L1933) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```BoudiafZRDPA20```
## Ismail Ben Ayed

- [![](https://img.shields.io/badge/CVPR-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Boudiaf_Few-Shot_Segmentation_Without_Meta-Learning_A_Good_Transductive_Inference_Is_All_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Segmentation+Without+Meta-Learning:+A+Good+Transductive+Inference+Is+All+You+Need?"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Segmentation Without Meta-Learning: A Good Transductive Inference Is All You Need?**](https://openaccess.thecvf.com/content/CVPR2021/html/Boudiaf_Few-Shot_Segmentation_Without_Meta-Learning_A_Good_Transductive_Inference_Is_All_CVPR_2021_paper.html) , <br> by *Boudiaf, Malik, Kervadec, Hoel, Masud, Ziko Imtiaz, Piantanida, Pablo, Ben Ayed, Ismail and Dolz, Jose* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1210-L1218) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Boudiaf_2021_CVPR```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/196f5641aa9dc87067da4ff90fd81e7b-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Information+Maximization+for+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Information Maximization for Few-Shot Learning**](https://proceedings.neurips.cc/paper/2020/hash/196f5641aa9dc87067da4ff90fd81e7b-Abstract.html) , <br> by *Malik Boudiaf and
Imtiaz Masud Ziko and
J{\'{e}}r{\^{o}}me Rony and
Jose Dolz and
Pablo Piantanida and
Ismail Ben Ayed* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1922-L1933) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```BoudiafZRDPA20```
## Pablo Piantanida

- [![](https://img.shields.io/badge/CVPR-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Boudiaf_Few-Shot_Segmentation_Without_Meta-Learning_A_Good_Transductive_Inference_Is_All_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Segmentation+Without+Meta-Learning:+A+Good+Transductive+Inference+Is+All+You+Need?"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Segmentation Without Meta-Learning: A Good Transductive Inference Is All You Need?**](https://openaccess.thecvf.com/content/CVPR2021/html/Boudiaf_Few-Shot_Segmentation_Without_Meta-Learning_A_Good_Transductive_Inference_Is_All_CVPR_2021_paper.html) , <br> by *Boudiaf, Malik, Kervadec, Hoel, Masud, Ziko Imtiaz, Piantanida, Pablo, Ben Ayed, Ismail and Dolz, Jose* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1210-L1218) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Boudiaf_2021_CVPR```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/196f5641aa9dc87067da4ff90fd81e7b-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Information+Maximization+for+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Information Maximization for Few-Shot Learning**](https://proceedings.neurips.cc/paper/2020/hash/196f5641aa9dc87067da4ff90fd81e7b-Abstract.html) , <br> by *Malik Boudiaf and
Imtiaz Masud Ziko and
J{\'{e}}r{\^{o}}me Rony and
Jose Dolz and
Pablo Piantanida and
Ismail Ben Ayed* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1922-L1933) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```BoudiafZRDPA20```
## Malik Boudiaf

- [![](https://img.shields.io/badge/CVPR-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Boudiaf_Few-Shot_Segmentation_Without_Meta-Learning_A_Good_Transductive_Inference_Is_All_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Segmentation+Without+Meta-Learning:+A+Good+Transductive+Inference+Is+All+You+Need?"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Segmentation Without Meta-Learning: A Good Transductive Inference Is All You Need?**](https://openaccess.thecvf.com/content/CVPR2021/html/Boudiaf_Few-Shot_Segmentation_Without_Meta-Learning_A_Good_Transductive_Inference_Is_All_CVPR_2021_paper.html) , <br> by *Boudiaf, Malik, Kervadec, Hoel, Masud, Ziko Imtiaz, Piantanida, Pablo, Ben Ayed, Ismail and Dolz, Jose* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1210-L1218) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Boudiaf_2021_CVPR```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/196f5641aa9dc87067da4ff90fd81e7b-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Information+Maximization+for+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Information Maximization for Few-Shot Learning**](https://proceedings.neurips.cc/paper/2020/hash/196f5641aa9dc87067da4ff90fd81e7b-Abstract.html) , <br> by *Malik Boudiaf and
Imtiaz Masud Ziko and
J{\'{e}}r{\^{o}}me Rony and
Jose Dolz and
Pablo Piantanida and
Ismail Ben Ayed* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1922-L1933) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```BoudiafZRDPA20```
## Jian Sun

- [![](https://img.shields.io/badge/CVPR-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Fan_Generalized_Few-Shot_Object_Detection_Without_Forgetting_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Generalized+Few-Shot+Object+Detection+Without+Forgetting"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Generalized Few-Shot Object Detection Without Forgetting**](https://openaccess.thecvf.com/content/CVPR2021/html/Fan_Generalized_Few-Shot_Object_Detection_Without_Forgetting_CVPR_2021_paper.html) , <br> by *Fan, Zhibo, Ma, Yuchen, Li, Zeming and Sun, Jian* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1152-L1159) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Fan_2021_CVPR```
- [![](https://img.shields.io/badge/ACL-2020-green)](https://www.aclweb.org/anthology/2020.acl-main.102)<a href="https://scholar.google.com.hk/scholar?q=Dynamic+Memory+Induction+Networks+for+Few-Shot+Text+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Dynamic Memory Induction Networks for Few-Shot Text Classification**](https://www.aclweb.org/anthology/2020.acl-main.102) , <br> by *Geng, Ruiying  and
Li, Binhua  and
Li, Yongbin  and
Sun, Jian  and
Zhu, Xiaodan* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2513-L2524) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```geng-etal-2020-dynamic```
## Trevor Darrell

- [![](https://img.shields.io/badge/CVPR-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Yue_Prototypical_Cross-Domain_Self-Supervised_Learning_for_Few-Shot_Unsupervised_Domain_Adaptation_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Prototypical+Cross-Domain+Self-Supervised+Learning+for+Few-Shot+Unsupervised+Domain+Adaptation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Prototypical Cross-Domain Self-Supervised Learning for Few-Shot Unsupervised Domain Adaptation**](https://openaccess.thecvf.com/content/CVPR2021/html/Yue_Prototypical_Cross-Domain_Self-Supervised_Learning_for_Few-Shot_Unsupervised_Domain_Adaptation_CVPR_2021_paper.html) , <br> by *Yue, Xiangyu, Zheng, Zangwei, Zhang, Shanghang, Gao, Yang, Darrell, Trevor, Keutzer, Kurt and Vincentelli, Alberto Sangiovanni* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1133-L1140) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Yue_2021_CVPR```
- [![](https://img.shields.io/badge/ICML-2020-green)](http://proceedings.mlr.press/v119/wang20j.html)<a href="https://scholar.google.com.hk/scholar?q=Frustratingly+Simple+Few-Shot+Object+Detection"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Frustratingly Simple Few-Shot Object Detection**](http://proceedings.mlr.press/v119/wang20j.html) , <br> by *Wang, Xin, Huang, Thomas, Gonzalez, Joseph, Darrell, Trevor and Yu, Fisher* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1543-L1550) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v119-wang20j```
## Wei Hu

- [![](https://img.shields.io/badge/ICML-2021-green)](http://proceedings.mlr.press/v139/saunshi21a.html)<a href="https://scholar.google.com.hk/scholar?q=A+Representation+Learning+Perspective+on+the+Importance+of+Train-Validation+Splitting+in+Meta-Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**A Representation Learning Perspective on the Importance of Train-Validation Splitting in Meta-Learning**](http://proceedings.mlr.press/v139/saunshi21a.html) , <br> by *Saunshi, Nikunj, Gupta, Arushi and Hu, Wei* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1095-L1102) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v139-saunshi21a```
- [![](https://img.shields.io/badge/ICLR-2021-green)](https://openreview.net/forum?id=pW2Q2xLwIMD)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Learning+via+Learning+the+Representation,+Provably"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Learning via Learning the Representation, Provably**](https://openreview.net/forum?id=pW2Q2xLwIMD) , <br> by *Simon Shaolei Du, Wei Hu, Sham M. Kakade, Jason D. Lee and Qi Lei* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1368-L1374) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```du2021fewshot```
## Renkun Ni

- [![](https://img.shields.io/badge/ICML-2021-green)](http://proceedings.mlr.press/v139/ni21a.html)<a href="https://scholar.google.com.hk/scholar?q=Data+Augmentation+for+Meta-Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Data Augmentation for Meta-Learning**](http://proceedings.mlr.press/v139/ni21a.html) , <br> by *Ni, Renkun, Goldblum, Micah, Sharaf, Amr, Kong, Kezhi and Goldstein, Tom* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1065-L1072) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v139-ni21a```
- [![](https://img.shields.io/badge/ICML-2020-green)](http://proceedings.mlr.press/v119/goldblum20a.html)<a href="https://scholar.google.com.hk/scholar?q=Unraveling+Meta-Learning:+Understanding+Feature+Representations+for+Few-Shot+Tasks"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Unraveling Meta-Learning: Understanding Feature Representations for Few-Shot Tasks**](http://proceedings.mlr.press/v119/goldblum20a.html) , <br> by *Goldblum, Micah, Reich, Steven, Fowl, Liam, Ni, Renkun, Cherepanova, Valeriia and Goldstein, Tom* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1494-L1501) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v119-goldblum20a```
## Tuo Zhao

- [![](https://img.shields.io/badge/ICML-2021-green)](http://proceedings.mlr.press/v139/bai21a.html)<a href="https://scholar.google.com.hk/scholar?q=How+Important+is+the+Train-Validation+Split+in+Meta-Learning?"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**How Important is the Train-Validation Split in Meta-Learning?**](http://proceedings.mlr.press/v139/bai21a.html) , <br> by *Bai, Yu, Chen, Minshuo, Zhou, Pan, Zhao, Tuo, Lee, Jason, Kakade, Sham, Wang, Huan and Xiong, Caiming* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1035-L1042) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v139-bai21a```
- [![](https://img.shields.io/badge/NeurIPS-2019-green)](https://proceedings.neurips.cc/paper/2019/hash/6fe43269967adbb64ec6149852b5cc3e-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Meta+Learning+with+Relational+Information+for+Short+Sequences"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta Learning with Relational Information for Short Sequences**](https://proceedings.neurips.cc/paper/2019/hash/6fe43269967adbb64ec6149852b5cc3e-Abstract.html) , <br> by *Yujia Xie and
Haoming Jiang and
Feng Liu and
Tuo Zhao and
Hongyuan Zha* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2290-L2301) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```XieJLZZ19```
## Vincent Dumoulin

- [![](https://img.shields.io/badge/ICML-2021-green)](http://proceedings.mlr.press/v139/triantafillou21a.html)<a href="https://scholar.google.com.hk/scholar?q=Learning+a+Universal+Template+for+Few-shot+Dataset+Generalization"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning a Universal Template for Few-shot Dataset Generalization**](http://proceedings.mlr.press/v139/triantafillou21a.html) , <br> by *Triantafillou, Eleni, Larochelle, Hugo, Zemel, Richard and Dumoulin, Vincent* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1015-L1022) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v139-triantafillou21a```
- [![](https://img.shields.io/badge/ICLR-2020-green)](https://openreview.net/forum?id=rkgAGAVKPr)<a href="https://scholar.google.com.hk/scholar?q=Meta-Dataset:+A+Dataset+of+Datasets+for+Learning+to+Learn+from+Few+Examples"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples**](https://openreview.net/forum?id=rkgAGAVKPr) , <br> by *Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol and Hugo Larochelle* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1692-L1698) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Triantafillou2020Meta-Dataset```
## Hang Gao

- [![](https://img.shields.io/badge/ACL-2021-green)](https://aclanthology.org/2021.acl-long.495)<a href="https://scholar.google.com.hk/scholar?q=Multi-Label+Few-Shot+Learning+for+Aspect+Category+Detection"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Multi-Label Few-Shot Learning for Aspect Category Detection**](https://aclanthology.org/2021.acl-long.495) , <br> by *Hu, Mengting  and
Zhao, Shiwan  and
Guo, Honglei  and
Xue, Chao  and
Gao, Hang  and
Gao, Tiegang  and
Cheng, Renhong  and
Su, Zhong* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L907-L921) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```hu-etal-2021-multi-label```
- [![](https://img.shields.io/badge/NeurIPS-2018-green)](https://proceedings.neurips.cc/paper/2018/hash/81448138f5f163ccdba4acc69819f280-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Low-shot+Learning+via+Covariance-Preserving+Adversarial+Augmentation+Networks"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Low-shot Learning via Covariance-Preserving Adversarial Augmentation
Networks**](https://proceedings.neurips.cc/paper/2018/hash/81448138f5f163ccdba4acc69819f280-Abstract.html) , <br> by *Hang Gao and
Zheng Shou and
Alireza Zareian and
Hanwang Zhang and
Shih{-}Fu Chang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2326-L2338) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```GaoSZZC18```
## Danqi Chen

- [![](https://img.shields.io/badge/ACL-2021-green)](https://aclanthology.org/2021.acl-long.295)<a href="https://scholar.google.com.hk/scholar?q=Making+Pre-trained+Language+Models+Better+Few-shot+Learners"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Making Pre-trained Language Models Better Few-shot Learners**](https://aclanthology.org/2021.acl-long.295) , <br> by *Gao, Tianyu  and
Fisch, Adam  and
Chen, Danqi* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L872-L881) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```gao-etal-2021-making```
- [![](https://img.shields.io/badge/NAACL_HLT-2021-green)](https://www.aclweb.org/anthology/2021.naacl-main.398)<a href="https://scholar.google.com.hk/scholar?q=Factual+Probing+Is+[MASK]:+Learning+vs.+Learning+to+Recall"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Factual Probing Is [MASK]: Learning vs. Learning to Recall**](https://www.aclweb.org/anthology/2021.naacl-main.398) , <br> by *Zhong, Zexuan  and
Friedman, Dan  and
Chen, Danqi* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2440-L2449) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhong-etal-2021-factual```
## Adam Fisch

- [![](https://img.shields.io/badge/ACL-2021-green)](https://aclanthology.org/2021.acl-long.295)<a href="https://scholar.google.com.hk/scholar?q=Making+Pre-trained+Language+Models+Better+Few-shot+Learners"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Making Pre-trained Language Models Better Few-shot Learners**](https://aclanthology.org/2021.acl-long.295) , <br> by *Gao, Tianyu  and
Fisch, Adam  and
Chen, Danqi* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L872-L881) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```gao-etal-2021-making```
- [![](https://img.shields.io/badge/ICML-2021-green)](http://proceedings.mlr.press/v139/fisch21a.html)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Conformal+Prediction+with+Auxiliary+Tasks"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Conformal Prediction with Auxiliary Tasks**](http://proceedings.mlr.press/v139/fisch21a.html) , <br> by *Fisch, Adam, Schuster, Tal, Jaakkola, Tommi and Barzilay, Dr.Regina* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1045-L1052) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v139-fisch21a```
## Pengjun Xie

- [![](https://img.shields.io/badge/ACL-2021-green)](https://aclanthology.org/2021.acl-long.248)<a href="https://scholar.google.com.hk/scholar?q=Few-NERD:+A+Few-shot+Named+Entity+Recognition+Dataset"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-NERD: A Few-shot Named Entity Recognition Dataset**](https://aclanthology.org/2021.acl-long.248) , <br> by *Ding, Ning  and
Xu, Guangwei  and
Chen, Yulin  and
Wang, Xiaobin  and
Han, Xu  and
Xie, Pengjun  and
Zheng, Haitao  and
Liu, Zhiyuan* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L856-L870) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ding-etal-2021-nerd```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2108.10604)<a href="https://scholar.google.com.hk/scholar?q=Prompt-Learning+for+Fine-Grained+Entity+Typing"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Prompt-Learning for Fine-Grained Entity Typing**](https://arxiv.org/abs/2108.10604) , <br> by *Ning Ding and
Yulin Chen and
Xu Han and
Guangwei Xu and
Pengjun Xie and
Hai{-}Tao Zheng and
Zhiyuan Liu and
Juanzi Li and
Hong{-}Gee Kim* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3231-L3246) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2108-10604```
## Yulin Chen

- [![](https://img.shields.io/badge/ACL-2021-green)](https://aclanthology.org/2021.acl-long.248)<a href="https://scholar.google.com.hk/scholar?q=Few-NERD:+A+Few-shot+Named+Entity+Recognition+Dataset"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-NERD: A Few-shot Named Entity Recognition Dataset**](https://aclanthology.org/2021.acl-long.248) , <br> by *Ding, Ning  and
Xu, Guangwei  and
Chen, Yulin  and
Wang, Xiaobin  and
Han, Xu  and
Xie, Pengjun  and
Zheng, Haitao  and
Liu, Zhiyuan* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L856-L870) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ding-etal-2021-nerd```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2108.10604)<a href="https://scholar.google.com.hk/scholar?q=Prompt-Learning+for+Fine-Grained+Entity+Typing"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Prompt-Learning for Fine-Grained Entity Typing**](https://arxiv.org/abs/2108.10604) , <br> by *Ning Ding and
Yulin Chen and
Xu Han and
Guangwei Xu and
Pengjun Xie and
Hai{-}Tao Zheng and
Zhiyuan Liu and
Juanzi Li and
Hong{-}Gee Kim* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3231-L3246) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2108-10604```
## Guangwei Xu

- [![](https://img.shields.io/badge/ACL-2021-green)](https://aclanthology.org/2021.acl-long.248)<a href="https://scholar.google.com.hk/scholar?q=Few-NERD:+A+Few-shot+Named+Entity+Recognition+Dataset"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-NERD: A Few-shot Named Entity Recognition Dataset**](https://aclanthology.org/2021.acl-long.248) , <br> by *Ding, Ning  and
Xu, Guangwei  and
Chen, Yulin  and
Wang, Xiaobin  and
Han, Xu  and
Xie, Pengjun  and
Zheng, Haitao  and
Liu, Zhiyuan* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L856-L870) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ding-etal-2021-nerd```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2108.10604)<a href="https://scholar.google.com.hk/scholar?q=Prompt-Learning+for+Fine-Grained+Entity+Typing"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Prompt-Learning for Fine-Grained Entity Typing**](https://arxiv.org/abs/2108.10604) , <br> by *Ning Ding and
Yulin Chen and
Xu Han and
Guangwei Xu and
Pengjun Xie and
Hai{-}Tao Zheng and
Zhiyuan Liu and
Juanzi Li and
Hong{-}Gee Kim* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3231-L3246) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2108-10604```
## Anna Korhonen

- [![](https://img.shields.io/badge/ACL-2021-green)](https://aclanthology.org/2021.acl-long.447)<a href="https://scholar.google.com.hk/scholar?q=A+Closer+Look+at+Few-Shot+Crosslingual+Transfer:+The+Choice+of+Shots+Matters"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**A Closer Look at Few-Shot Crosslingual Transfer: The Choice of Shots Matters**](https://aclanthology.org/2021.acl-long.447) , <br> by *Zhao, Mengjie  and
Zhu, Yi  and
Shareghi, Ehsan  and
Vuli{\'c}, Ivan  and
Reichart, Roi  and
Korhonen, Anna  and
Sch{\"u}tze, Hinrich* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L828-L841) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhao-etal-2021-closer```
- [![](https://img.shields.io/badge/CoRR-2020-green)](https://arxiv.org/abs/2012.15682)<a href="https://scholar.google.com.hk/scholar?q=A+Closer+Look+at+Few-Shot+Crosslingual+Transfer:+Variance,+Benchmarks+and+Baselines"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**A Closer Look at Few-Shot Crosslingual Transfer: Variance, Benchmarks
and Baselines**](https://arxiv.org/abs/2012.15682) , <br> by *Mengjie Zhao and
Yi Zhu and
Ehsan Shareghi and
Roi Reichart and
Anna Korhonen and
Hinrich Sch{\"{u}}tze* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3096-L3109) <br>```ACL 2021
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2012-15682```
## Roi Reichart

- [![](https://img.shields.io/badge/ACL-2021-green)](https://aclanthology.org/2021.acl-long.447)<a href="https://scholar.google.com.hk/scholar?q=A+Closer+Look+at+Few-Shot+Crosslingual+Transfer:+The+Choice+of+Shots+Matters"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**A Closer Look at Few-Shot Crosslingual Transfer: The Choice of Shots Matters**](https://aclanthology.org/2021.acl-long.447) , <br> by *Zhao, Mengjie  and
Zhu, Yi  and
Shareghi, Ehsan  and
Vuli{\'c}, Ivan  and
Reichart, Roi  and
Korhonen, Anna  and
Sch{\"u}tze, Hinrich* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L828-L841) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhao-etal-2021-closer```
- [![](https://img.shields.io/badge/CoRR-2020-green)](https://arxiv.org/abs/2012.15682)<a href="https://scholar.google.com.hk/scholar?q=A+Closer+Look+at+Few-Shot+Crosslingual+Transfer:+Variance,+Benchmarks+and+Baselines"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**A Closer Look at Few-Shot Crosslingual Transfer: Variance, Benchmarks
and Baselines**](https://arxiv.org/abs/2012.15682) , <br> by *Mengjie Zhao and
Yi Zhu and
Ehsan Shareghi and
Roi Reichart and
Anna Korhonen and
Hinrich Sch{\"{u}}tze* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3096-L3109) <br>```ACL 2021
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2012-15682```
## Ehsan Shareghi

- [![](https://img.shields.io/badge/ACL-2021-green)](https://aclanthology.org/2021.acl-long.447)<a href="https://scholar.google.com.hk/scholar?q=A+Closer+Look+at+Few-Shot+Crosslingual+Transfer:+The+Choice+of+Shots+Matters"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**A Closer Look at Few-Shot Crosslingual Transfer: The Choice of Shots Matters**](https://aclanthology.org/2021.acl-long.447) , <br> by *Zhao, Mengjie  and
Zhu, Yi  and
Shareghi, Ehsan  and
Vuli{\'c}, Ivan  and
Reichart, Roi  and
Korhonen, Anna  and
Sch{\"u}tze, Hinrich* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L828-L841) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhao-etal-2021-closer```
- [![](https://img.shields.io/badge/CoRR-2020-green)](https://arxiv.org/abs/2012.15682)<a href="https://scholar.google.com.hk/scholar?q=A+Closer+Look+at+Few-Shot+Crosslingual+Transfer:+Variance,+Benchmarks+and+Baselines"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**A Closer Look at Few-Shot Crosslingual Transfer: Variance, Benchmarks
and Baselines**](https://arxiv.org/abs/2012.15682) , <br> by *Mengjie Zhao and
Yi Zhu and
Ehsan Shareghi and
Roi Reichart and
Anna Korhonen and
Hinrich Sch{\"{u}}tze* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3096-L3109) <br>```ACL 2021
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2012-15682```
## Yi Zhu

- [![](https://img.shields.io/badge/ACL-2021-green)](https://aclanthology.org/2021.acl-long.447)<a href="https://scholar.google.com.hk/scholar?q=A+Closer+Look+at+Few-Shot+Crosslingual+Transfer:+The+Choice+of+Shots+Matters"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**A Closer Look at Few-Shot Crosslingual Transfer: The Choice of Shots Matters**](https://aclanthology.org/2021.acl-long.447) , <br> by *Zhao, Mengjie  and
Zhu, Yi  and
Shareghi, Ehsan  and
Vuli{\'c}, Ivan  and
Reichart, Roi  and
Korhonen, Anna  and
Sch{\"u}tze, Hinrich* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L828-L841) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhao-etal-2021-closer```
- [![](https://img.shields.io/badge/CoRR-2020-green)](https://arxiv.org/abs/2012.15682)<a href="https://scholar.google.com.hk/scholar?q=A+Closer+Look+at+Few-Shot+Crosslingual+Transfer:+Variance,+Benchmarks+and+Baselines"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**A Closer Look at Few-Shot Crosslingual Transfer: Variance, Benchmarks
and Baselines**](https://arxiv.org/abs/2012.15682) , <br> by *Mengjie Zhao and
Yi Zhu and
Ehsan Shareghi and
Roi Reichart and
Anna Korhonen and
Hinrich Sch{\"{u}}tze* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3096-L3109) <br>```ACL 2021
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2012-15682```
## Leyu Lin

- [![](https://img.shields.io/badge/AAAI-2020-green)](https://aaai.org/ojs/index.php/AAAI/article/view/6281)<a href="https://scholar.google.com.hk/scholar?q=Neural+Snowball+for+Few-Shot+Relation+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Neural Snowball for Few-Shot Relation Learning**](https://aaai.org/ojs/index.php/AAAI/article/view/6281) , <br> by *Tianyu Gao and
Xu Han and
Ruobing Xie and
Zhiyuan Liu and
Fen Lin and
Leyu Lin and
Maosong Sun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L765-L778) <br>```Neural Snowball
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```GaoHX0LLS20```
- [![](https://img.shields.io/badge/ICLR-2020-green)](https://openreview.net/forum?id=r1eowANFvr)<a href="https://scholar.google.com.hk/scholar?q=Towards+Fast+Adaptation+of+Neural+Architectures+with+Meta+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Towards Fast Adaptation of Neural Architectures with Meta Learning**](https://openreview.net/forum?id=r1eowANFvr) , <br> by *Dongze Lian, Yin Zheng, Yintao Xu, Yanxiong Lu, Leyu Lin, Peilin Zhao, Junzhou Huang and Shenghua Gao* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1702-L1708) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Lian2020Towards```
## Yuan Yao

- [![](https://img.shields.io/badge/EMNLP-2018-green)](https://doi.org/10.18653/v1/d18-1514)<a href="https://scholar.google.com.hk/scholar?q=FewRel:+A+Large-Scale+Supervised+Few-shot+Relation+Classification+Dataset+with+State-of-the-Art+Evaluation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**FewRel: A Large-Scale Supervised Few-shot Relation Classification
Dataset with State-of-the-Art Evaluation**](https://doi.org/10.18653/v1/d18-1514) , <br> by *Xu Han and
Hao Zhu and
Pengfei Yu and
Ziyun Wang and
Yuan Yao and
Zhiyuan Liu and
Maosong Sun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L706-L720) <br>```FewRel dataset
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```HanZYWYLS18```
- [![](https://img.shields.io/badge/ICML-2018-green)](http://proceedings.mlr.press/v80/zhao18c.html)<a href="https://scholar.google.com.hk/scholar?q=MSplit+LBI:+Realizing+Feature+Selection+and+Dense+Estimation+Simultaneously+in+Few-shot+and+Zero-shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**MSplit LBI: Realizing Feature Selection and Dense Estimation Simultaneously in Few-shot and Zero-shot Learning**](http://proceedings.mlr.press/v80/zhao18c.html) , <br> by *Zhao, Bo, Sun, Xinwei, Fu, Yanwei, Yao, Yuan and Wang, Yizhou* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1614-L1621) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```pmlr-v80-zhao18c```
## Yaqing Wang

- [![](https://img.shields.io/badge/KDD-2021-green)](https://doi.org/10.1145/3447548.3467235)<a href="https://scholar.google.com.hk/scholar?q=Meta+Self-Training+for+Few-Shot+Neural+Sequence+Labeling"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta Self-Training for Few-Shot Neural Sequence Labeling**](https://doi.org/10.1145/3447548.3467235) , <br> by *Wang, Yaqing, Mukherjee, Subhabrata, Chu, Haoda, Tu, Yuancheng, Wu, Ming, Gao, Jing and Awadallah, Ahmed Hassan* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L792-L799) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```3447548.3467235```
- [![](https://img.shields.io/badge/ACM_Comput._Surv.-2020-green)](https://doi.org/10.1145/3386252)<a href="https://scholar.google.com.hk/scholar?q=Generalizing+from+a+Few+Examples:+A+Survey+on+Few-shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Generalizing from a Few Examples: A Survey on Few-shot Learning**](https://doi.org/10.1145/3386252) , <br> by *Yaqing Wang and
Quanming Yao and
James T. Kwok and
Lionel M. Ni* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L650-L660) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```WangYKN20```
## Liwei Wang

- [![](https://img.shields.io/badge/CVPR-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Hu_Dense_Relation_Distillation_With_Context-Aware_Aggregation_for_Few-Shot_Object_Detection_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Dense+Relation+Distillation+With+Context-Aware+Aggregation+for+Few-Shot+Object+Detection"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Dense Relation Distillation With Context-Aware Aggregation for Few-Shot Object Detection**](https://openaccess.thecvf.com/content/CVPR2021/html/Hu_Dense_Relation_Distillation_With_Context-Aware_Aggregation_for_Few-Shot_Object_Detection_CVPR_2021_paper.html) , <br> by *Hu, Hanzhe, Bai, Shuai, Li, Aoxue, Cui, Jinshi and Wang, Liwei* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1270-L1278) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Hu_2021_CVPR```
- [![](https://img.shields.io/badge/CVPR-2020-green)](https://doi.org/10.1109/CVPR42600.2020.01259)<a href="https://scholar.google.com.hk/scholar?q=Boosting+Few-Shot+Learning+With+Adaptive+Margin+Loss"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Boosting Few-Shot Learning With Adaptive Margin Loss**](https://doi.org/10.1109/CVPR42600.2020.01259) , <br> by *Aoxue Li and
Weiran Huang and
Xu Lan and
Jiashi Feng and
Zhenguo Li and
Liwei Wang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L636-L648) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Li0LFLW20```
## Aoxue Li

- [![](https://img.shields.io/badge/CVPR-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Hu_Dense_Relation_Distillation_With_Context-Aware_Aggregation_for_Few-Shot_Object_Detection_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Dense+Relation+Distillation+With+Context-Aware+Aggregation+for+Few-Shot+Object+Detection"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Dense Relation Distillation With Context-Aware Aggregation for Few-Shot Object Detection**](https://openaccess.thecvf.com/content/CVPR2021/html/Hu_Dense_Relation_Distillation_With_Context-Aware_Aggregation_for_Few-Shot_Object_Detection_CVPR_2021_paper.html) , <br> by *Hu, Hanzhe, Bai, Shuai, Li, Aoxue, Cui, Jinshi and Wang, Liwei* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1270-L1278) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Hu_2021_CVPR```
- [![](https://img.shields.io/badge/CVPR-2020-green)](https://doi.org/10.1109/CVPR42600.2020.01259)<a href="https://scholar.google.com.hk/scholar?q=Boosting+Few-Shot+Learning+With+Adaptive+Margin+Loss"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Boosting Few-Shot Learning With Adaptive Margin Loss**](https://doi.org/10.1109/CVPR42600.2020.01259) , <br> by *Aoxue Li and
Weiran Huang and
Xu Lan and
Jiashi Feng and
Zhenguo Li and
Liwei Wang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L636-L648) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Li0LFLW20```
## Lu Zhang

- [![](https://img.shields.io/badge/IJCAI-2021-green)](https://doi.org/10.24963/ijcai.2021/295)<a href="https://scholar.google.com.hk/scholar?q=Conditional+Self-Supervised+Learning+for+Few-Shot+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Conditional Self-Supervised Learning for Few-Shot Classification**](https://doi.org/10.24963/ijcai.2021/295) , <br> by *An, Yuexuan, Xue, Hui, Zhao, Xingyu and Zhang, Lu* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L597-L604) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ijcai2021-295```
- [![](https://img.shields.io/badge/CVPR-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Accurate_Few-Shot_Object_Detection_With_Support-Query_Mutual_Guidance_and_Hybrid_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=Accurate+Few-Shot+Object+Detection+With+Support-Query+Mutual+Guidance+and+Hybrid+Loss"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Accurate Few-Shot Object Detection With Support-Query Mutual Guidance and Hybrid Loss**](https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Accurate_Few-Shot_Object_Detection_With_Support-Query_Mutual_Guidance_and_Hybrid_CVPR_2021_paper.html) , <br> by *Zhang, Lu, Zhou, Shuigeng, Guan, Jihong and Zhang, Ji* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1142-L1150) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Zhang_2021_CVPR```
## Gholamreza Haffari

- [![](https://img.shields.io/badge/ACL_Findings-2021-green)](https://doi.org/10.18653/v1/2021.findings-acl.214)<a href="https://scholar.google.com.hk/scholar?q=Adaptive+Knowledge-Enhanced+Bayesian+Meta-Learning+for+Few-shot+Event+Detection"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Adaptive Knowledge-Enhanced Bayesian Meta-Learning for Few-shot Event
Detection**](https://doi.org/10.18653/v1/2021.findings-acl.214) , <br> by *Shirong Shen and
Tongtong Wu and
Guilin Qi and
Yuan{-}Fang Li and
Gholamreza Haffari and
Sheng Bi* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L549-L562) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ShenWQLHB21```
- [![](https://img.shields.io/badge/EMNLP-2020-green)](https://www.aclweb.org/anthology/2020.emnlp-main.469)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Complex+Knowledge+Base+Question+Answering+via+Meta+Reinforcement+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Complex Knowledge Base Question Answering via Meta Reinforcement Learning**](https://www.aclweb.org/anthology/2020.emnlp-main.469) , <br> by *Hua, Yuncheng  and
Li, Yuan-Fang  and
Haffari, Gholamreza  and
Qi, Guilin  and
Wu, Tongtong* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2682-L2693) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```hua-etal-2020-shot```
## Guilin Qi

- [![](https://img.shields.io/badge/ACL_Findings-2021-green)](https://doi.org/10.18653/v1/2021.findings-acl.214)<a href="https://scholar.google.com.hk/scholar?q=Adaptive+Knowledge-Enhanced+Bayesian+Meta-Learning+for+Few-shot+Event+Detection"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Adaptive Knowledge-Enhanced Bayesian Meta-Learning for Few-shot Event
Detection**](https://doi.org/10.18653/v1/2021.findings-acl.214) , <br> by *Shirong Shen and
Tongtong Wu and
Guilin Qi and
Yuan{-}Fang Li and
Gholamreza Haffari and
Sheng Bi* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L549-L562) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ShenWQLHB21```
- [![](https://img.shields.io/badge/EMNLP-2020-green)](https://www.aclweb.org/anthology/2020.emnlp-main.469)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Complex+Knowledge+Base+Question+Answering+via+Meta+Reinforcement+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Complex Knowledge Base Question Answering via Meta Reinforcement Learning**](https://www.aclweb.org/anthology/2020.emnlp-main.469) , <br> by *Hua, Yuncheng  and
Li, Yuan-Fang  and
Haffari, Gholamreza  and
Qi, Guilin  and
Wu, Tongtong* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2682-L2693) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```hua-etal-2020-shot```
## Tongtong Wu

- [![](https://img.shields.io/badge/ACL_Findings-2021-green)](https://doi.org/10.18653/v1/2021.findings-acl.214)<a href="https://scholar.google.com.hk/scholar?q=Adaptive+Knowledge-Enhanced+Bayesian+Meta-Learning+for+Few-shot+Event+Detection"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Adaptive Knowledge-Enhanced Bayesian Meta-Learning for Few-shot Event
Detection**](https://doi.org/10.18653/v1/2021.findings-acl.214) , <br> by *Shirong Shen and
Tongtong Wu and
Guilin Qi and
Yuan{-}Fang Li and
Gholamreza Haffari and
Sheng Bi* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L549-L562) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ShenWQLHB21```
- [![](https://img.shields.io/badge/EMNLP-2020-green)](https://www.aclweb.org/anthology/2020.emnlp-main.469)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Complex+Knowledge+Base+Question+Answering+via+Meta+Reinforcement+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Complex Knowledge Base Question Answering via Meta Reinforcement Learning**](https://www.aclweb.org/anthology/2020.emnlp-main.469) , <br> by *Hua, Yuncheng  and
Li, Yuan-Fang  and
Haffari, Gholamreza  and
Qi, Guilin  and
Wu, Tongtong* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2682-L2693) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```hua-etal-2020-shot```
## Jianfeng Gao

- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/3644a684f98ea8fe223c713b77189a77-Abstract-round2.html)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Learning+Evaluation+in+Natural+Language+Understanding"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Learning Evaluation in Natural Language Understanding**](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/3644a684f98ea8fe223c713b77189a77-Abstract-round2.html) , <br> by *Subhabrata Mukherjee and
Xiaodong Liu and
Guoqing Zheng and
Saghar Hosseini and
Saghar Hosseini and
Hao Cheng and
Ge Yang and
Christopher Meek and
Ahmed Hassan Awadallah and
Jianfeng Gao* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L464-L479) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```MukherjeeLZHH0Y21```
- [![](https://img.shields.io/badge/EMNLP_Findings-2020-green)](https://www.aclweb.org/anthology/2020.findings-emnlp.17)<a href="https://scholar.google.com.hk/scholar?q=Few-shot+Natural+Language+Generation+for+Task-Oriented+Dialog"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-shot Natural Language Generation for Task-Oriented Dialog**](https://www.aclweb.org/anthology/2020.findings-emnlp.17) , <br> by *Peng, Baolin  and
Zhu, Chenguang  and
Li, Chunyuan  and
Li, Xiujun  and
Li, Jinchao  and
Zeng, Michael  and
Gao, Jianfeng* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2732-L2745) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```peng-etal-2020-shot```
## Saghar Hosseini

- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/3644a684f98ea8fe223c713b77189a77-Abstract-round2.html)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Learning+Evaluation+in+Natural+Language+Understanding"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Learning Evaluation in Natural Language Understanding**](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/3644a684f98ea8fe223c713b77189a77-Abstract-round2.html) , <br> by *Subhabrata Mukherjee and
Xiaodong Liu and
Guoqing Zheng and
Saghar Hosseini and
Saghar Hosseini and
Hao Cheng and
Ge Yang and
Christopher Meek and
Ahmed Hassan Awadallah and
Jianfeng Gao* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L464-L479) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```MukherjeeLZHH0Y21```
## Minlie Huang

- [![](https://img.shields.io/badge/ACL-2022-green)](https://aclanthology.org/2022.acl-long.576)<a href="https://scholar.google.com.hk/scholar?q=PPT:+Pre-trained+Prompt+Tuning+for+Few-shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**PPT: Pre-trained Prompt Tuning for Few-shot Learning**](https://aclanthology.org/2022.acl-long.576) , <br> by *Gu, Yuxian  and
Han, Xu  and
Liu, Zhiyuan  and
Huang, Minlie* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L438-L449) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```gu-etal-2022-ppt```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2109.04332)<a href="https://scholar.google.com.hk/scholar?q=PPT:+Pre-trained+Prompt+Tuning+for+Few-shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**PPT: Pre-trained Prompt Tuning for Few-shot Learning**](https://arxiv.org/abs/2109.04332) , <br> by *Yuxian Gu, Xu Han, Zhiyuan Liu and Minlie Huang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3222-L3229) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```gu2021ppt```
## Yuxian Gu

- [![](https://img.shields.io/badge/ACL-2022-green)](https://aclanthology.org/2022.acl-long.576)<a href="https://scholar.google.com.hk/scholar?q=PPT:+Pre-trained+Prompt+Tuning+for+Few-shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**PPT: Pre-trained Prompt Tuning for Few-shot Learning**](https://aclanthology.org/2022.acl-long.576) , <br> by *Gu, Yuxian  and
Han, Xu  and
Liu, Zhiyuan  and
Huang, Minlie* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L438-L449) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```gu-etal-2022-ppt```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2109.04332)<a href="https://scholar.google.com.hk/scholar?q=PPT:+Pre-trained+Prompt+Tuning+for+Few-shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**PPT: Pre-trained Prompt Tuning for Few-shot Learning**](https://arxiv.org/abs/2109.04332) , <br> by *Yuxian Gu, Xu Han, Zhiyuan Liu and Minlie Huang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3222-L3229) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```gu2021ppt```
## Shengding Hu

- [![](https://img.shields.io/badge/ACL-2022-green)](https://aclanthology.org/2022.acl-long.483)<a href="https://scholar.google.com.hk/scholar?q=Prototypical+Verbalizer+for+Prompt-based+Few-shot+Tuning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Prototypical Verbalizer for Prompt-based Few-shot Tuning**](https://aclanthology.org/2022.acl-long.483) , <br> by *Cui, Ganqu  and
Hu, Shengding  and
Ding, Ning  and
Huang, Longtao  and
Liu, Zhiyuan* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L399-L411) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```cui-etal-2022-prototypical```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2108.02035)<a href="https://scholar.google.com.hk/scholar?q=Knowledgeable+Prompt-tuning:+Incorporating+Knowledge+into+Prompt+Verbalizer+for+Text+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer
for Text Classification**](https://arxiv.org/abs/2108.02035) , <br> by *Shengding Hu and
Ning Ding and
Huadong Wang and
Zhiyuan Liu and
Juanzi Li and
Maosong Sun* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3177-L3190) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2108-02035```
## Zhilin Yang

- [![](https://img.shields.io/badge/ACL-2022-green)](https://aclanthology.org/2022.acl-long.592)<a href="https://scholar.google.com.hk/scholar?q=FlipDA:+Effective+and+Robust+Data+Augmentation+for+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**FlipDA: Effective and Robust Data Augmentation for Few-Shot Learning**](https://aclanthology.org/2022.acl-long.592) , <br> by *Zhou, Jing  and
Zheng, Yanan  and
Tang, Jie  and
Jian, Li  and
Yang, Zhilin* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L385-L397) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhou-etal-2022-flipda```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2103.10385)<a href="https://scholar.google.com.hk/scholar?q=GPT+Understands,+Too"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**GPT Understands, Too**](https://arxiv.org/abs/2103.10385) , <br> by *Xiao Liu and
Yanan Zheng and
Zhengxiao Du and
Ming Ding and
Yujie Qian and
Zhilin Yang and
Jie Tang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2874-L2887) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2103-10385```
## Jie Tang

- [![](https://img.shields.io/badge/ACL-2022-green)](https://aclanthology.org/2022.acl-long.592)<a href="https://scholar.google.com.hk/scholar?q=FlipDA:+Effective+and+Robust+Data+Augmentation+for+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**FlipDA: Effective and Robust Data Augmentation for Few-Shot Learning**](https://aclanthology.org/2022.acl-long.592) , <br> by *Zhou, Jing  and
Zheng, Yanan  and
Tang, Jie  and
Jian, Li  and
Yang, Zhilin* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L385-L397) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhou-etal-2022-flipda```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2103.10385)<a href="https://scholar.google.com.hk/scholar?q=GPT+Understands,+Too"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**GPT Understands, Too**](https://arxiv.org/abs/2103.10385) , <br> by *Xiao Liu and
Yanan Zheng and
Zhengxiao Du and
Ming Ding and
Yujie Qian and
Zhilin Yang and
Jie Tang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2874-L2887) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2103-10385```
## Yanan Zheng

- [![](https://img.shields.io/badge/ACL-2022-green)](https://aclanthology.org/2022.acl-long.592)<a href="https://scholar.google.com.hk/scholar?q=FlipDA:+Effective+and+Robust+Data+Augmentation+for+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**FlipDA: Effective and Robust Data Augmentation for Few-Shot Learning**](https://aclanthology.org/2022.acl-long.592) , <br> by *Zhou, Jing  and
Zheng, Yanan  and
Tang, Jie  and
Jian, Li  and
Yang, Zhilin* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L385-L397) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhou-etal-2022-flipda```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2103.10385)<a href="https://scholar.google.com.hk/scholar?q=GPT+Understands,+Too"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**GPT Understands, Too**](https://arxiv.org/abs/2103.10385) , <br> by *Xiao Liu and
Yanan Zheng and
Zhengxiao Du and
Ming Ding and
Yujie Qian and
Zhilin Yang and
Jie Tang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2874-L2887) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2103-10385```
## Yu Cheng

- [![](https://img.shields.io/badge/ACL-2022-green)](https://aclanthology.org/2022.acl-long.197)<a href="https://scholar.google.com.hk/scholar?q=A+Good+Prompt+Is+Worth+Millions+of+Parameters:+Low-resource+Prompt-based+Learning+for+Vision-Language+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models**](https://aclanthology.org/2022.acl-long.197) , <br> by *Jin, Woojeong  and
Cheng, Yu  and
Shen, Yelong  and
Chen, Weizhu  and
Ren, Xiang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L359-L371) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```jin-etal-2022-good```
- [![](https://img.shields.io/badge/NAACL_HLT-2021-green)](https://www.aclweb.org/anthology/2021.naacl-main.434)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Text+Classification+with+Triplet+Networks,+Data+Augmentation,+and+Curriculum+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Text Classification with Triplet Networks, Data Augmentation, and Curriculum Learning**](https://www.aclweb.org/anthology/2021.naacl-main.434) , <br> by *Wei, Jason  and
Huang, Chengyu  and
Vosoughi, Soroush  and
Cheng, Yu  and
Xu, Shiqi* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2838-L2849) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```wei-etal-2021-shot```
## Ruiyi Zhang

- [![](https://img.shields.io/badge/ACL-2022-green)](https://aclanthology.org/2022.acl-long.43)<a href="https://scholar.google.com.hk/scholar?q=Few-Shot+Class-Incremental+Learning+for+Named+Entity+Recognition"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Few-Shot Class-Incremental Learning for Named Entity Recognition**](https://aclanthology.org/2022.acl-long.43) , <br> by *Wang, Rui  and
Yu, Tong  and
Zhao, Handong  and
Kim, Sungchul  and
Mitra, Subrata  and
Zhang, Ruiyi  and
Henao, Ricardo* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L332-L346) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```wang-etal-2022-shot```
- [![](https://img.shields.io/badge/ICLR-2020-green)](https://openreview.net/forum?id=Bkxv90EKPB)<a href="https://scholar.google.com.hk/scholar?q=Bayesian+Meta+Sampling+for+Fast+Uncertainty+Adaptation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Bayesian Meta Sampling for Fast Uncertainty Adaptation**](https://openreview.net/forum?id=Bkxv90EKPB) , <br> by *Zhenyi Wang, Yang Zhao, Ping Yu, Ruiyi Zhang and Changyou Chen* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1712-L1718) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Wang2020Bayesian```
## Luke Zettlemoyer

- [![](https://img.shields.io/badge/ACL-2022-green)](https://aclanthology.org/2022.acl-long.254)<a href="https://scholar.google.com.hk/scholar?q=Prompt-free+and+Efficient+Few-shot+Learning+with+Language+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Prompt-free and Efficient Few-shot Learning with Language Models**](https://aclanthology.org/2022.acl-long.254) , <br> by *Karimi Mahabadi, Rabeeh  and
Zettlemoyer, Luke  and
Henderson, James  and
Mathias, Lambert  and
Saeidi, Marzieh  and
Stoyanov, Veselin  and
Yazdani, Majid* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L303-L317) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```karimi-mahabadi-etal-2022-prompt```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2108.04106)<a href="https://scholar.google.com.hk/scholar?q=Noisy+Channel+Language+Model+Prompting+for+Few-Shot+Text+Classification"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Noisy Channel Language Model Prompting for Few-Shot Text Classification**](https://arxiv.org/abs/2108.04106) , <br> by *Sewon Min and
Mike Lewis and
Hannaneh Hajishirzi and
Luke Zettlemoyer* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3192-L3202) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2108-04106```
## Xiao Ming Wu

- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/357cfba15668cc2e1e73111e09d54383-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Overcoming+Catastrophic+Forgetting+in+Incremental+Few-Shot+Learning+by+Finding+Flat+Minima"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Overcoming Catastrophic Forgetting in Incremental Few-Shot Learning
by Finding Flat Minima**](https://proceedings.neurips.cc/paper/2021/hash/357cfba15668cc2e1e73111e09d54383-Abstract.html) , <br> by *Guangyuan Shi and
Jiaxin Chen and
Wenlong Zhang and
Li{-}Ming Zhan and
Xiao{-}Ming Wu* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L287-L299) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ShiCZZW21```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/0415740eaa4d9decbc8da001d3fd805f-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=A+Closer+Look+at+the+Training+Strategy+for+Modern+Meta-Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**A Closer Look at the Training Strategy for Modern Meta-Learning**](https://proceedings.neurips.cc/paper/2020/hash/0415740eaa4d9decbc8da001d3fd805f-Abstract.html) , <br> by *Jiaxin Chen and
Xiao{-}Ming Wu and
Yanke Li and
Qimai Li and
Li{-}Ming Zhan and
Fu{-}Lai Chung* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2025-L2036) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ChenWLLZC20```
## Li Ming Zhan

- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/357cfba15668cc2e1e73111e09d54383-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Overcoming+Catastrophic+Forgetting+in+Incremental+Few-Shot+Learning+by+Finding+Flat+Minima"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Overcoming Catastrophic Forgetting in Incremental Few-Shot Learning
by Finding Flat Minima**](https://proceedings.neurips.cc/paper/2021/hash/357cfba15668cc2e1e73111e09d54383-Abstract.html) , <br> by *Guangyuan Shi and
Jiaxin Chen and
Wenlong Zhang and
Li{-}Ming Zhan and
Xiao{-}Ming Wu* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L287-L299) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ShiCZZW21```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/0415740eaa4d9decbc8da001d3fd805f-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=A+Closer+Look+at+the+Training+Strategy+for+Modern+Meta-Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**A Closer Look at the Training Strategy for Modern Meta-Learning**](https://proceedings.neurips.cc/paper/2020/hash/0415740eaa4d9decbc8da001d3fd805f-Abstract.html) , <br> by *Jiaxin Chen and
Xiao{-}Ming Wu and
Yanke Li and
Qimai Li and
Li{-}Ming Zhan and
Fu{-}Lai Chung* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2025-L2036) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ChenWLLZC20```
## Jiaxin Chen

- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/357cfba15668cc2e1e73111e09d54383-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Overcoming+Catastrophic+Forgetting+in+Incremental+Few-Shot+Learning+by+Finding+Flat+Minima"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Overcoming Catastrophic Forgetting in Incremental Few-Shot Learning
by Finding Flat Minima**](https://proceedings.neurips.cc/paper/2021/hash/357cfba15668cc2e1e73111e09d54383-Abstract.html) , <br> by *Guangyuan Shi and
Jiaxin Chen and
Wenlong Zhang and
Li{-}Ming Zhan and
Xiao{-}Ming Wu* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L287-L299) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ShiCZZW21```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/0415740eaa4d9decbc8da001d3fd805f-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=A+Closer+Look+at+the+Training+Strategy+for+Modern+Meta-Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**A Closer Look at the Training Strategy for Modern Meta-Learning**](https://proceedings.neurips.cc/paper/2020/hash/0415740eaa4d9decbc8da001d3fd805f-Abstract.html) , <br> by *Jiaxin Chen and
Xiao{-}Ming Wu and
Yanke Li and
Qimai Li and
Li{-}Ming Zhan and
Fu{-}Lai Chung* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2025-L2036) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ChenWLLZC20```
## Khoi Nguyen

- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/c91591a8d461c2869b9f535ded3e213e-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=POODLE:+Improving+Few-shot+Learning+via+Penalizing+Out-of-Distribution+Samples"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**POODLE: Improving Few-shot Learning via Penalizing Out-of-Distribution
Samples**](https://proceedings.neurips.cc/paper/2021/hash/c91591a8d461c2869b9f535ded3e213e-Abstract.html) , <br> by *Duong H. Le and
Khoi Duc Nguyen and
Khoi Nguyen and
Quoc{-}Huy Tran and
Rang Nguyen and
Binh{-}Son Hua* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L272-L285) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```LeNNTNH21```
- [![](https://img.shields.io/badge/CVPR-2021-green)](https://openaccess.thecvf.com/content/CVPR2021/html/Nguyen_FAPIS_A_Few-Shot_Anchor-Free_Part-Based_Instance_Segmenter_CVPR_2021_paper.html)<a href="https://scholar.google.com.hk/scholar?q=FAPIS:+A+Few-Shot+Anchor-Free+Part-Based+Instance+Segmenter"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**FAPIS: A Few-Shot Anchor-Free Part-Based Instance Segmenter**](https://openaccess.thecvf.com/content/CVPR2021/html/Nguyen_FAPIS_A_Few-Shot_Anchor-Free_Part-Based_Instance_Segmenter_CVPR_2021_paper.html) , <br> by *Nguyen, Khoi and Todorovic, Sinisa* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L1250-L1258) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Nguyen_2021_CVPR```
## Felix Hill

- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/01b7575c38dac42f3cfb7d500438b875-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Multimodal+Few-Shot+Learning+with+Frozen+Language+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Multimodal Few-Shot Learning with Frozen Language Models**](https://proceedings.neurips.cc/paper/2021/hash/01b7575c38dac42f3cfb7d500438b875-Abstract.html) , <br> by *Maria Tsimpoukelli and
Jacob Menick and
Serkan Cabi and
S. M. Ali Eslami and
Oriol Vinyals and
Felix Hill* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L248-L260) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```TsimpoukelliMCE21```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2106.13884)<a href="https://scholar.google.com.hk/scholar?q=Multimodal+Few-Shot+Learning+with+Frozen+Language+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Multimodal Few-Shot Learning with Frozen Language Models**](https://arxiv.org/abs/2106.13884) , <br> by *Maria Tsimpoukelli and
Jacob Menick and
Serkan Cabi and
S. M. Ali Eslami and
Oriol Vinyals and
Felix Hill* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3154-L3166) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2106-13884```
## Serkan Cabi

- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/01b7575c38dac42f3cfb7d500438b875-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Multimodal+Few-Shot+Learning+with+Frozen+Language+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Multimodal Few-Shot Learning with Frozen Language Models**](https://proceedings.neurips.cc/paper/2021/hash/01b7575c38dac42f3cfb7d500438b875-Abstract.html) , <br> by *Maria Tsimpoukelli and
Jacob Menick and
Serkan Cabi and
S. M. Ali Eslami and
Oriol Vinyals and
Felix Hill* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L248-L260) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```TsimpoukelliMCE21```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2106.13884)<a href="https://scholar.google.com.hk/scholar?q=Multimodal+Few-Shot+Learning+with+Frozen+Language+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Multimodal Few-Shot Learning with Frozen Language Models**](https://arxiv.org/abs/2106.13884) , <br> by *Maria Tsimpoukelli and
Jacob Menick and
Serkan Cabi and
S. M. Ali Eslami and
Oriol Vinyals and
Felix Hill* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3154-L3166) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2106-13884```
## Jacob Menick

- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/01b7575c38dac42f3cfb7d500438b875-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Multimodal+Few-Shot+Learning+with+Frozen+Language+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Multimodal Few-Shot Learning with Frozen Language Models**](https://proceedings.neurips.cc/paper/2021/hash/01b7575c38dac42f3cfb7d500438b875-Abstract.html) , <br> by *Maria Tsimpoukelli and
Jacob Menick and
Serkan Cabi and
S. M. Ali Eslami and
Oriol Vinyals and
Felix Hill* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L248-L260) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```TsimpoukelliMCE21```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2106.13884)<a href="https://scholar.google.com.hk/scholar?q=Multimodal+Few-Shot+Learning+with+Frozen+Language+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Multimodal Few-Shot Learning with Frozen Language Models**](https://arxiv.org/abs/2106.13884) , <br> by *Maria Tsimpoukelli and
Jacob Menick and
Serkan Cabi and
S. M. Ali Eslami and
Oriol Vinyals and
Felix Hill* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3154-L3166) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2106-13884```
## Maria Tsimpoukelli

- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/01b7575c38dac42f3cfb7d500438b875-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Multimodal+Few-Shot+Learning+with+Frozen+Language+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Multimodal Few-Shot Learning with Frozen Language Models**](https://proceedings.neurips.cc/paper/2021/hash/01b7575c38dac42f3cfb7d500438b875-Abstract.html) , <br> by *Maria Tsimpoukelli and
Jacob Menick and
Serkan Cabi and
S. M. Ali Eslami and
Oriol Vinyals and
Felix Hill* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L248-L260) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```TsimpoukelliMCE21```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2106.13884)<a href="https://scholar.google.com.hk/scholar?q=Multimodal+Few-Shot+Learning+with+Frozen+Language+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Multimodal Few-Shot Learning with Frozen Language Models**](https://arxiv.org/abs/2106.13884) , <br> by *Maria Tsimpoukelli and
Jacob Menick and
Serkan Cabi and
S. M. Ali Eslami and
Oriol Vinyals and
Felix Hill* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3154-L3166) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```abs-2106-13884```
## Iz Beltagy

- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/8493eeaccb772c0878f99d60a0bd2bb3-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=FLEX:+Unifying+Evaluation+for+Few-Shot+NLP"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**FLEX: Unifying Evaluation for Few-Shot NLP**](https://proceedings.neurips.cc/paper/2021/hash/8493eeaccb772c0878f99d60a0bd2bb3-Abstract.html) , <br> by *Jonathan Bragg and
Arman Cohan and
Kyle Lo and
Iz Beltagy* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L236-L246) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```BraggCLB21```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2107.07170)<a href="https://scholar.google.com.hk/scholar?q=FLEX:+Unifying+Evaluation+for+Few-Shot+NLP"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**FLEX: Unifying Evaluation for Few-Shot NLP**](https://arxiv.org/abs/2107.07170) , <br> by *Jonathan Bragg, Arman Cohan, Kyle Lo and Iz Beltagy* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3168-L3175) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```bragg2021flex```
## Kyle Lo

- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/8493eeaccb772c0878f99d60a0bd2bb3-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=FLEX:+Unifying+Evaluation+for+Few-Shot+NLP"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**FLEX: Unifying Evaluation for Few-Shot NLP**](https://proceedings.neurips.cc/paper/2021/hash/8493eeaccb772c0878f99d60a0bd2bb3-Abstract.html) , <br> by *Jonathan Bragg and
Arman Cohan and
Kyle Lo and
Iz Beltagy* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L236-L246) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```BraggCLB21```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2107.07170)<a href="https://scholar.google.com.hk/scholar?q=FLEX:+Unifying+Evaluation+for+Few-Shot+NLP"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**FLEX: Unifying Evaluation for Few-Shot NLP**](https://arxiv.org/abs/2107.07170) , <br> by *Jonathan Bragg, Arman Cohan, Kyle Lo and Iz Beltagy* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3168-L3175) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```bragg2021flex```
## Arman Cohan

- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/8493eeaccb772c0878f99d60a0bd2bb3-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=FLEX:+Unifying+Evaluation+for+Few-Shot+NLP"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**FLEX: Unifying Evaluation for Few-Shot NLP**](https://proceedings.neurips.cc/paper/2021/hash/8493eeaccb772c0878f99d60a0bd2bb3-Abstract.html) , <br> by *Jonathan Bragg and
Arman Cohan and
Kyle Lo and
Iz Beltagy* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L236-L246) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```BraggCLB21```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2107.07170)<a href="https://scholar.google.com.hk/scholar?q=FLEX:+Unifying+Evaluation+for+Few-Shot+NLP"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**FLEX: Unifying Evaluation for Few-Shot NLP**](https://arxiv.org/abs/2107.07170) , <br> by *Jonathan Bragg, Arman Cohan, Kyle Lo and Iz Beltagy* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3168-L3175) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```bragg2021flex```
## Jonathan Bragg

- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/8493eeaccb772c0878f99d60a0bd2bb3-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=FLEX:+Unifying+Evaluation+for+Few-Shot+NLP"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**FLEX: Unifying Evaluation for Few-Shot NLP**](https://proceedings.neurips.cc/paper/2021/hash/8493eeaccb772c0878f99d60a0bd2bb3-Abstract.html) , <br> by *Jonathan Bragg and
Arman Cohan and
Kyle Lo and
Iz Beltagy* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L236-L246) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```BraggCLB21```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2107.07170)<a href="https://scholar.google.com.hk/scholar?q=FLEX:+Unifying+Evaluation+for+Few-Shot+NLP"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**FLEX: Unifying Evaluation for Few-Shot NLP**](https://arxiv.org/abs/2107.07170) , <br> by *Jonathan Bragg, Arman Cohan, Kyle Lo and Iz Beltagy* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3168-L3175) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```bragg2021flex```
## Qiang Qiu

- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/6e2713a6efee97bacb63e52c54f0ada0-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Learning+to+Learn+Dense+Gaussian+Processes+for+Few-Shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning to Learn Dense Gaussian Processes for Few-Shot Learning**](https://proceedings.neurips.cc/paper/2021/hash/6e2713a6efee97bacb63e52c54f0ada0-Abstract.html) , <br> by *Ze Wang and
Zichen Miao and
Xiantong Zhen and
Qiang Qiu* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L208-L219) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```WangMZQ21```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/67d16d00201083a2b118dd5128dd6f59-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Learning+to+Learn+Variational+Semantic+Memory"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning to Learn Variational Semantic Memory**](https://proceedings.neurips.cc/paper/2020/hash/67d16d00201083a2b118dd5128dd6f59-Abstract.html) , <br> by *Xiantong Zhen and
Ying{-}Jun Du and
Huan Xiong and
Qiang Qiu and
Cees Snoek and
Ling Shao* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2192-L2203) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ZhenDXQS020```
## Ruohan Wang

- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/e3b6fb0fd4df098162eede3313c54a8d-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=The+Role+of+Global+Labels+in+Few-Shot+Classification+and+How+to+Infer+Them"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**The Role of Global Labels in Few-Shot Classification and How to Infer
Them**](https://proceedings.neurips.cc/paper/2021/hash/e3b6fb0fd4df098162eede3313c54a8d-Abstract.html) , <br> by *Ruohan Wang and
Massimiliano Pontil and
Carlo Ciliberto* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L181-L191) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```WangPC21```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/1b69ebedb522700034547abc5652ffac-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Structured+Prediction+for+Conditional+Meta-Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Structured Prediction for Conditional Meta-Learning**](https://proceedings.neurips.cc/paper/2020/hash/1b69ebedb522700034547abc5652ffac-Abstract.html) , <br> by *Ruohan Wang and
Yiannis Demiris and
Carlo Ciliberto* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2049-L2057) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```WangDC20```
## James T. Kwok

- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/af5d5ef24881f3c3049a7b9bfe74d58b-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=TOHAN:+A+One-step+Approach+towards+Few-shot+Hypothesis+Adaptation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**TOHAN: A One-step Approach towards Few-shot Hypothesis Adaptation**](https://proceedings.neurips.cc/paper/2021/hash/af5d5ef24881f3c3049a7b9bfe74d58b-Abstract.html) , <br> by *Haoang Chi and
Feng Liu and
Wenjing Yang and
Long Lan and
Tongliang Liu and
Bo Han and
William K. Cheung and
James T. Kwok* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L165-L179) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ChiLYLLHCK21```
- [![](https://img.shields.io/badge/ACM_Comput._Surv.-2020-green)](https://doi.org/10.1145/3386252)<a href="https://scholar.google.com.hk/scholar?q=Generalizing+from+a+Few+Examples:+A+Survey+on+Few-shot+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Generalizing from a Few Examples: A Survey on Few-shot Learning**](https://doi.org/10.1145/3386252) , <br> by *Yaqing Wang and
Quanming Yao and
James T. Kwok and
Lionel M. Ni* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L650-L660) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```WangYKN20```
## Feng Liu

- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/af5d5ef24881f3c3049a7b9bfe74d58b-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=TOHAN:+A+One-step+Approach+towards+Few-shot+Hypothesis+Adaptation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**TOHAN: A One-step Approach towards Few-shot Hypothesis Adaptation**](https://proceedings.neurips.cc/paper/2021/hash/af5d5ef24881f3c3049a7b9bfe74d58b-Abstract.html) , <br> by *Haoang Chi and
Feng Liu and
Wenjing Yang and
Long Lan and
Tongliang Liu and
Bo Han and
William K. Cheung and
James T. Kwok* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L165-L179) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ChiLYLLHCK21```
- [![](https://img.shields.io/badge/NeurIPS-2019-green)](https://proceedings.neurips.cc/paper/2019/hash/6fe43269967adbb64ec6149852b5cc3e-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Meta+Learning+with+Relational+Information+for+Short+Sequences"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Meta Learning with Relational Information for Short Sequences**](https://proceedings.neurips.cc/paper/2019/hash/6fe43269967adbb64ec6149852b5cc3e-Abstract.html) , <br> by *Yujia Xie and
Haoming Jiang and
Feng Liu and
Tuo Zhao and
Hongyuan Zha* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2290-L2301) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```XieJLZZ19```
## Kyunghyun Cho

- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/5c04925674920eb58467fb52ce4ef728-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=True+Few-Shot+Learning+with+Language+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**True Few-Shot Learning with Language Models**](https://proceedings.neurips.cc/paper/2021/hash/5c04925674920eb58467fb52ce4ef728-Abstract.html) , <br> by *Ethan Perez and
Douwe Kiela and
Kyunghyun Cho* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L129-L138) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```PerezKC21```
- [![](https://img.shields.io/badge/CoRR-2020-green)](https://arxiv.org/abs/2105.11447)<a href="https://scholar.google.com.hk/scholar?q=True+Few-Shot+Learning+with+Language+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**True Few-Shot Learning with Language Models**](https://arxiv.org/abs/2105.11447) , <br> by *Ethan Perez, Douwe Kiela and Kyunghyun Cho* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3136-L3143) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```perez2021true```
## Douwe Kiela

- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/5c04925674920eb58467fb52ce4ef728-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=True+Few-Shot+Learning+with+Language+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**True Few-Shot Learning with Language Models**](https://proceedings.neurips.cc/paper/2021/hash/5c04925674920eb58467fb52ce4ef728-Abstract.html) , <br> by *Ethan Perez and
Douwe Kiela and
Kyunghyun Cho* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L129-L138) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```PerezKC21```
- [![](https://img.shields.io/badge/CoRR-2020-green)](https://arxiv.org/abs/2105.11447)<a href="https://scholar.google.com.hk/scholar?q=True+Few-Shot+Learning+with+Language+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**True Few-Shot Learning with Language Models**](https://arxiv.org/abs/2105.11447) , <br> by *Ethan Perez, Douwe Kiela and Kyunghyun Cho* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3136-L3143) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```perez2021true```
## Ethan Perez

- [![](https://img.shields.io/badge/NeurIPS-2021-green)](https://proceedings.neurips.cc/paper/2021/hash/5c04925674920eb58467fb52ce4ef728-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=True+Few-Shot+Learning+with+Language+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**True Few-Shot Learning with Language Models**](https://proceedings.neurips.cc/paper/2021/hash/5c04925674920eb58467fb52ce4ef728-Abstract.html) , <br> by *Ethan Perez and
Douwe Kiela and
Kyunghyun Cho* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L129-L138) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```PerezKC21```
- [![](https://img.shields.io/badge/CoRR-2020-green)](https://arxiv.org/abs/2105.11447)<a href="https://scholar.google.com.hk/scholar?q=True+Few-Shot+Learning+with+Language+Models"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**True Few-Shot Learning with Language Models**](https://arxiv.org/abs/2105.11447) , <br> by *Ethan Perez, Douwe Kiela and Kyunghyun Cho* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3136-L3143) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```perez2021true```
## Jiashi Feng

- [![](https://img.shields.io/badge/International_Conference_on_Learning_Representations-2022-green)](https://openreview.net/forum?id=_jMtny3sMKU)<a href="https://scholar.google.com.hk/scholar?q=Generalizing+Few-Shot+NAS+with+Gradient+Matching"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Generalizing Few-Shot NAS with Gradient Matching**](https://openreview.net/forum?id=_jMtny3sMKU) , <br> by *Shoukang Hu, Ruochen Wang, Lanqing HONG, Zhenguo Li, Cho-Jui Hsieh and Jiashi Feng* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L101-L107) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```hu2022generalizing```
- [![](https://img.shields.io/badge/CVPR-2020-green)](https://doi.org/10.1109/CVPR42600.2020.01259)<a href="https://scholar.google.com.hk/scholar?q=Boosting+Few-Shot+Learning+With+Adaptive+Margin+Loss"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Boosting Few-Shot Learning With Adaptive Margin Loss**](https://doi.org/10.1109/CVPR42600.2020.01259) , <br> by *Aoxue Li and
Weiran Huang and
Xu Lan and
Jiashi Feng and
Zhenguo Li and
Liwei Wang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L636-L648) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Li0LFLW20```
## Zhenguo Li

- [![](https://img.shields.io/badge/International_Conference_on_Learning_Representations-2022-green)](https://openreview.net/forum?id=_jMtny3sMKU)<a href="https://scholar.google.com.hk/scholar?q=Generalizing+Few-Shot+NAS+with+Gradient+Matching"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Generalizing Few-Shot NAS with Gradient Matching**](https://openreview.net/forum?id=_jMtny3sMKU) , <br> by *Shoukang Hu, Ruochen Wang, Lanqing HONG, Zhenguo Li, Cho-Jui Hsieh and Jiashi Feng* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L101-L107) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```hu2022generalizing```
- [![](https://img.shields.io/badge/CVPR-2020-green)](https://doi.org/10.1109/CVPR42600.2020.01259)<a href="https://scholar.google.com.hk/scholar?q=Boosting+Few-Shot+Learning+With+Adaptive+Margin+Loss"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Boosting Few-Shot Learning With Adaptive Margin Loss**](https://doi.org/10.1109/CVPR42600.2020.01259) , <br> by *Aoxue Li and
Weiran Huang and
Xu Lan and
Jiashi Feng and
Zhenguo Li and
Liwei Wang* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L636-L648) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Li0LFLW20```
## Ling Shao

- [![](https://img.shields.io/badge/International_Conference_on_Learning_Representations-2022-green)](https://openreview.net/forum?id=i3RI65sR7N)<a href="https://scholar.google.com.hk/scholar?q=Hierarchical+Variational+Memory+for+Few-shot+Learning+Across+Domains"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Hierarchical Variational Memory for Few-shot Learning Across Domains**](https://openreview.net/forum?id=i3RI65sR7N) , <br> by *Yingjun Du, Xiantong Zhen, Ling Shao and Cees G. M. Snoek* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L83-L89) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```du2022hierarchical```
- [![](https://img.shields.io/badge/NeurIPS-2020-green)](https://proceedings.neurips.cc/paper/2020/hash/67d16d00201083a2b118dd5128dd6f59-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=Learning+to+Learn+Variational+Semantic+Memory"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Learning to Learn Variational Semantic Memory**](https://proceedings.neurips.cc/paper/2020/hash/67d16d00201083a2b118dd5128dd6f59-Abstract.html) , <br> by *Xiantong Zhen and
Ying{-}Jun Du and
Huan Xiong and
Qiang Qiu and
Cees Snoek and
Ling Shao* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2192-L2203) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ZhenDXQS020```
## Shafiq Joty

- [![](https://img.shields.io/badge/International_Conference_on_Learning_Representations-2022-green)](https://openreview.net/forum?id=HCRVf71PMF)<a href="https://scholar.google.com.hk/scholar?q=LFPT5:+A+Unified+Framework+for+Lifelong+Few-shot+Language+Learning+Based+on+Prompt+Tuning+of+T5"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**LFPT5: A Unified Framework for Lifelong Few-shot Language Learning Based on Prompt Tuning of T5**](https://openreview.net/forum?id=HCRVf71PMF) , <br> by *Chengwei Qin and Shafiq Joty* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L56-L62) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```qin2022lfpt```
- [![](https://img.shields.io/badge/ACL-2022-green)](https://aclanthology.org/2022.acl-long.198)<a href="https://scholar.google.com.hk/scholar?q=Continual+Few-shot+Relation+Learning+via+Embedding+Space+Regularization+and+Data+Augmentation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Continual Few-shot Relation Learning via Embedding Space Regularization and Data Augmentation**](https://aclanthology.org/2022.acl-long.198) , <br> by *Qin, Chengwei  and
Joty, Shafiq* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L348-L357) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```qin-joty-2022-continual```
## Chengwei Qin

- [![](https://img.shields.io/badge/International_Conference_on_Learning_Representations-2022-green)](https://openreview.net/forum?id=HCRVf71PMF)<a href="https://scholar.google.com.hk/scholar?q=LFPT5:+A+Unified+Framework+for+Lifelong+Few-shot+Language+Learning+Based+on+Prompt+Tuning+of+T5"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**LFPT5: A Unified Framework for Lifelong Few-shot Language Learning Based on Prompt Tuning of T5**](https://openreview.net/forum?id=HCRVf71PMF) , <br> by *Chengwei Qin and Shafiq Joty* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L56-L62) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```qin2022lfpt```
- [![](https://img.shields.io/badge/ACL-2022-green)](https://aclanthology.org/2022.acl-long.198)<a href="https://scholar.google.com.hk/scholar?q=Continual+Few-shot+Relation+Learning+via+Embedding+Space+Regularization+and+Data+Augmentation"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Continual Few-shot Relation Learning via Embedding Space Regularization and Data Augmentation**](https://aclanthology.org/2022.acl-long.198) , <br> by *Qin, Chengwei  and
Joty, Shafiq* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L348-L357) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```qin-joty-2022-continual```
## Jacob Andreas

- [![](https://img.shields.io/badge/International_Conference_on_Learning_Representations-2022-green)](https://openreview.net/forum?id=boJy41J-tnQ)<a href="https://scholar.google.com.hk/scholar?q=Subspace+Regularizers+for+Few-Shot+Class+Incremental+Learning"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Subspace Regularizers for Few-Shot Class Incremental Learning**](https://openreview.net/forum?id=boJy41J-tnQ) , <br> by *Afra Feyza Aky{\"u}rek, Ekin Aky{\"u}rek, Derry Wijaya and Jacob Andreas* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L20-L26) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```aky{\"u}rek2022subspace```
- [![](https://img.shields.io/badge/ACL-2021-green)](https://aclanthology.org/2021.acl-long.382)<a href="https://scholar.google.com.hk/scholar?q=Lexicon+Learning+for+Few+Shot+Sequence+Modeling"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Lexicon Learning for Few Shot Sequence Modeling**](https://aclanthology.org/2021.acl-long.382) , <br> by *Akyurek, Ekin  and
Andreas, Jacob* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L923-L931) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```akyurek-andreas-2021-lexicon```
## Zhen Bi

- [![](https://img.shields.io/badge/International_Conference_on_Learning_Representations-2022-green)](https://openreview.net/forum?id=ek9a0qIafW)<a href="https://scholar.google.com.hk/scholar?q=Differentiable+Prompt+Makes+Pre-trained+Language+Models+Better+Few-shot+Learners"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners**](https://openreview.net/forum?id=ek9a0qIafW) , <br> by *Ningyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang and Huajun Chen* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2-L8) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhang2022differentiable```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2108.13161)<a href="https://scholar.google.com.hk/scholar?q=Differentiable+Prompt+Makes+Pre-trained+Language+Models+Better+Few-shot+Learners"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners**](https://arxiv.org/abs/2108.13161) , <br> by *Ningyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang and Huajun Chen* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3204-L3211) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhang2021differentiable```
## Luoqiu Li

- [![](https://img.shields.io/badge/International_Conference_on_Learning_Representations-2022-green)](https://openreview.net/forum?id=ek9a0qIafW)<a href="https://scholar.google.com.hk/scholar?q=Differentiable+Prompt+Makes+Pre-trained+Language+Models+Better+Few-shot+Learners"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners**](https://openreview.net/forum?id=ek9a0qIafW) , <br> by *Ningyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang and Huajun Chen* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L2-L8) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhang2022differentiable```
- [![](https://img.shields.io/badge/CoRR-2021-green)](https://arxiv.org/abs/2108.13161)<a href="https://scholar.google.com.hk/scholar?q=Differentiable+Prompt+Makes+Pre-trained+Language+Models+Better+Few-shot+Learners"><img src="https://img.shields.io/badge/-green.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners**](https://arxiv.org/abs/2108.13161) , <br> by *Ningyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang and Huajun Chen* [[bib]](https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/./bibtex.bib#L3204-L3211) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Few_Shot_Learning/blob/master/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```zhang2021differentiable```